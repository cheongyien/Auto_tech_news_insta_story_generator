header,content,url
Gray code,,https://en.wikipedia.org/wiki/Gray_code
Spaghetti sort,"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, introduced by Alexander Dewdney in his Scientific American column. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor.",https://en.wikipedia.org/wiki/Spaghetti_sort
nth root algorithm,"The principal nth root An of a positive real number A, is the positive real solution of the equation xn=A.",https://en.wikipedia.org/wiki/Nth_root_algorithm
Probability matching,"Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates.  Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of ""positive"" on 60% of instances, and a class label of ""negative"" on 40% of instances.  ",https://en.wikipedia.org/wiki/Probability_matching
Cuthill–McKee algorithm,"In numerical linear algebra, the Cuthill–McKee algorithm (CM), named for Elizabeth Cuthill  and James McKee, is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a   band matrix form with a small bandwidth. The reverse Cuthill–McKee algorithm  (RCM)  due to Alan George  is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in  than the CM ordering when Gaussian elimination is applied.",https://en.wikipedia.org/wiki/Cuthill%E2%80%93McKee_algorithm
Cipolla's algorithm,"In computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form",https://en.wikipedia.org/wiki/Cipolla%27s_algorithm
Stochastic gradient descent,"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in big data applications this reduces the computational burden, achieving faster iterations in trade for a slightly lower convergence rate.",https://en.wikipedia.org/wiki/Stochastic_gradient_descent
Rounding,,https://en.wikipedia.org/wiki/Rounding_functions
Quickselect,"In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list. It is related to the quicksort sorting algorithm. Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and its variants are the selection algorithms most often used in efficient real-world implementations. ",https://en.wikipedia.org/wiki/Quickselect
Quickprop,"Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group  of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be close to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied.",https://en.wikipedia.org/wiki/Quickprop
Automatic summarization,"Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. ",https://en.wikipedia.org/wiki/Automatic_summarization
Quasi-triangulation,"A quasi-triangulation is a subdivision of a geometric object into simplices, where vertices are not points but arbitrary sloped line segments. This division is not a triangulation in the geometric sense.  It is a topological triangulation, however. A quasi-triangulation may have some of the characteristics of a Delaunay triangulation.",https://en.wikipedia.org/wiki/Quasitriangulation
Wolfram Language,"The Wolfram Language is a general multi-paradigm computational language developed by Wolfram Research. It emphasizes symbolic computation, functional programming, and rule-based programming and can employ arbitrary structures and data.",https://en.wikipedia.org/wiki/Wolfram_Language
Salvatore J. Stolfo,"Salvatore J. Stolfo is a tenured professor of computer science at Columbia University in New York and a leading expert in computer security. He is known for his research in machine learning applied to computer security, intrusion detection systems, anomaly detection algorithms and systems, fraud detection, and parallel computing.",https://en.wikipedia.org/wiki/Salvatore_J._Stolfo
Sense Networks,"Sense Networks is a New York City based company with a focus on applications that analyze big data from mobile phones, carrier networks, and taxicabs, particularly by using machine learning technology to make sense of large amounts of location (latitude/longitude) data.",https://en.wikipedia.org/wiki/Sense_Networks
Independent component analysis,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.",https://en.wikipedia.org/wiki/Independent_component_analysis
Boltzmann machine,"A Boltzmann machine (also called stochastic Hopfield network with hidden units) is a type of stochastic recurrent neural network. It is a Markov random field It was translated from statistical physics for use in cognitive science. The Boltzmann machine is based on stochastic spin-glass model with an external field, i.e., a Sherrington–Kirkpatrick model that is a stochastic Ising Model and applied to machine learning.",https://en.wikipedia.org/wiki/Boltzmann_machine
Massive Online Analysis,"Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.",https://en.wikipedia.org/wiki/Massive_Online_Analysis
Customer relationship management,,https://en.wikipedia.org/wiki/Customer_relationship_management
Teaching dimension,"In computational learning theory, the teaching dimension of a concept class C is defined to be maxc∈C{wC(c)}, where wC(c) is the minimum size of a witness set for c in C.",https://en.wikipedia.org/wiki/Teaching_dimension
Selmer Bringsjord,"Prof. Selmer Bringsjord (born November 24, 1958) is the chair of the Department of Cognitive Science at Rensselaer Polytechnic Institute and a professor of Computer Science and Cognitive Science. He also holds an appointment in the Lally School of Management & Technology and teaches artificial Intelligence (AI), formal logic, human and machine reasoning, and philosophy of AI.",https://en.wikipedia.org/wiki/Selmer_Bringsjord
Sublinear function,"In linear algebra, a sublinear function (or functional, as is more often used in functional analysis) is a function f:V→F an ordered field (e.g. the real numbers R), which satisfies",https://en.wikipedia.org/wiki/Sublinear
Monte Carlo method,"Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution.",https://en.wikipedia.org/wiki/Monte_Carlo_method
Forward algorithm,"The forward algorithm, in the context of a hidden Markov model (HMM), is used to calculate a 'belief state': the probability of a state at a certain time, given the history of evidence.  The process is also known as filtering. The forward algorithm is closely related to, but distinct from, the Viterbi algorithm.",https://en.wikipedia.org/wiki/Forward_algorithm
Feature hashing,"In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. This trick is often attributed to Weinberger et al., but there exists a much earlier description of this method published by John Moody in 1989.",https://en.wikipedia.org/wiki/Feature_hashing
Range encoding,"Range encoding is an entropy coding method defined by G. Nigel N. Martin in a 1979 paper, which effectively rediscovered the FIFO arithmetic code first introduced by Richard Clark Pasco in 1976. Given a stream of symbols and their probabilities, a range coder produces a space-efficient stream of bits to represent these symbols and, given the stream and the probabilities, a range decoder reverses the process.",https://en.wikipedia.org/wiki/Range_encoding
Dijkstra's algorithm,,https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
Gauss–Legendre algorithm,"The Gauss–Legendre algorithm is an algorithm to compute the digits of π. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. However, the drawback is that it is computer memory-intensive and therefore sometimes Machin-like formulas are used instead.",https://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_algorithm
Cross-validation (statistics),"Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).",https://en.wikipedia.org/wiki/Cross-validation_(statistics)
Bogosort,"In computer science, bogosort (also known as permutation sort, stupid sort, slowsort, shotgun sort, or monkey sort) is a highly inefficient sorting algorithm based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted. It is not useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms.",https://en.wikipedia.org/wiki/Bogosort
Distributed R,"Distributed R is an open source, high-performance platform for the R language. It splits tasks between multiple processing nodes to reduce execution time and analyze large data sets. Distributed R enhances R by adding distributed data structures, parallelism primitives to run functions on distributed data, a task scheduler, and multiple data loaders. It is mostly used to implement distributed versions of machine learning tasks. Distributed R is written in C++ and R, and retains the familiar look and feel of R. As of February 2015, Hewlett-Packard (HP) provides enterprise support for Distributed R with proprietary additions such as a fast data loader from the Vertica database.",https://en.wikipedia.org/wiki/Distributed_R
Distance transform,"A distance transform, also known as distance map or distance field, is a derived representation of a digital image. The choice of the term depends on the point of view on the object in question: whether the initial image is transformed into another representation, or it is simply endowed with an additional map or field.",https://en.wikipedia.org/wiki/Euclidean_distance_map
Incremental encoding,"Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated. This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary.",https://en.wikipedia.org/wiki/Incremental_encoding
Reverse-delete algorithm,"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph.  It first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph.  The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.",https://en.wikipedia.org/wiki/Reverse-delete_algorithm
Dartmouth workshop,The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered to be the founding event of artificial intelligence as a field.,https://en.wikipedia.org/wiki/Dartmouth_workshop
Clique (graph theory),"In the mathematical area of graph theory, a clique (/ˈkliːk/ or /ˈklɪk/) is a subset of vertices of an undirected graph such that  every two distinct vertices in the clique are adjacent; that is, its induced subgraph is complete. Cliques are one of the basic concepts of graph theory and are used in many other mathematical problems and constructions on graphs. Cliques have also been studied in computer science: the task of finding whether there is a clique of a given size in a graph (the clique problem) is NP-complete, but despite this hardness result, many algorithms for finding cliques have been studied.",https://en.wikipedia.org/wiki/Maximal_clique
Byzantine fault,"A Byzantine fault (also interactive consistency, source congruency, error avalanche, Byzantine agreement problem, Byzantine generals problem, and Byzantine failure) is a condition of a computer system, particularly distributed computing systems, where components may fail and there is imperfect information on whether a component has failed. The term takes its name from an allegory, the ""Byzantine Generals Problem"", developed to describe a situation in which, in order to avoid catastrophic failure of the system, the system's actors must agree on a concerted strategy, but some of these actors are unreliable.",https://en.wikipedia.org/wiki/Byzantine_fault_tolerance
Shogun (toolbox),"Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.",https://en.wikipedia.org/wiki/Shogun_(toolbox)
Difference-map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.",https://en.wikipedia.org/wiki/Difference-map_algorithm
Cloud robotics,"Cloud robotics is a field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent ""brain"" in the cloud. The ""brain"" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support, etc.",https://en.wikipedia.org/wiki/RoboEarth
Pollard's p − 1 algorithm,"Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; it is the simplest example of an algebraic-group factorisation algorithm.",https://en.wikipedia.org/wiki/Pollard%27s_p_%E2%88%92_1_algorithm
Bron–Kerbosch algorithm,"In computer science, the Bron–Kerbosch algorithm is an enumeration algorithm for finding maximal cliques in an undirected graph. That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, and no listed subset can have any additional vertices added to it while preserving its complete connectivity. The Bron–Kerbosch algorithm was designed by Dutch scientists Coenraad Bron and Joep Kerbosch, who published its description in 1973. Although other algorithms for solving the clique problem have running times that are, in theory, better on inputs that have few maximal independent sets, the Bron–Kerbosch algorithm and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. It is well-known and widely used in application areas of graph algorithms such as computational chemistry.",https://en.wikipedia.org/wiki/Bron%E2%80%93Kerbosch_algorithm
Force-directed graph drawing,"Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically-pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.",https://en.wikipedia.org/wiki/Force-based_algorithms_(graph_drawing)
Inductive programming,"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.",https://en.wikipedia.org/wiki/Inductive_programming
Markovian discrimination,"Markovian discrimination in spam filtering is a method used in CRM114 and other spam filters to model the statistical behaviors of spam and nonspam more accurately than in simple Bayesian methods.  A simple Bayesian model of written text contains only the dictionary of legal words and their relative probabilities.  A Markovian model adds the relative transition probabilities that given one word, predict what the next word will be. It is based on the theory of Markov chains by Andrey Markov, hence the name.  In essence, a Bayesian filter works on single words alone, while a Markovian filter works on phrases or entire sentences.",https://en.wikipedia.org/wiki/Markovian_discrimination
Adaptive coding,"Adaptive coding refers to variants of entropy encoding methods of lossless data compression. They are particularly suited to streaming data, as they adapt to localized changes in the characteristics of the data, and don't require a first pass over the data to calculate a probability model. The cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized, and more computational power is needed to keep adapting the encoder/decoder state.",https://en.wikipedia.org/wiki/Adaptive_coding
Astrostatistics,"Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory.  Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference.",https://en.wikipedia.org/wiki/Astrostatistics
Multiplicative weight update method,"The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise.  It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.",https://en.wikipedia.org/wiki/Multiplicative_Weight_Update_Method
RuleML Symposium,"The annual International Web Rule Symposium (RuleML) is an international academic conferences on research, applications, languages and standards for rule technologies. Since 2017 it is organised as International Joint Conference on Rules and Reasoning (RuleML+RR). It is a conference in the field of rule-based programming and rule-based systems including production rules systems, logic programming rule engines, and business rules engines/business rules management systems; Semantic Web rule languages and rule standards (e.g., RuleML, LegalRuleML, Reaction RuleML, SWRL, RIF, Common Logic, PRR, Decision Model and Notation (DMN), SBVR); rule-based event processing languages (EPLs) and technologies; and research on inference rules, constraint handling rules, transformation rules, decision rules, production rules, and ECA rules. RuleML+RR is the leading conference to build bridges between academia and industry in the field of Web rules and its applications, especially as part of the semantic technology stack. RuleML+RR is commonly listed together with other Artificial Intelligence conferences worldwide.",https://en.wikipedia.org/wiki/RuleML_Symposium
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-01825-8
Stochastic tunneling,"In numerical analysis, stochastic tunneling (STUN) is an approach to global optimization based on the Monte Carlo method-sampling of the function to be objective minimized in which the function is nonlinearly transformed to allow for easier tunneling among regions containing function minima. Easier tunneling allows for faster exploration of sample space and faster convergence to a good solution.",https://en.wikipedia.org/wiki/Stochastic_tunneling
Joseph Nechvatal,"Joseph James Nechvatal (born January 15, 1951)  is an American post-conceptual digital artist and art theoretician who creates computer-assisted paintings and computer animations, often using custom-created computer viruses.",https://en.wikipedia.org/wiki/Joseph_Nechvatal
Toeplitz matrix,"In linear algebra, a Toeplitz matrix or diagonal-constant matrix, named after Otto Toeplitz, is a matrix in which each descending diagonal from left to right is constant.",https://en.wikipedia.org/wiki/Toeplitz_matrix
Additive smoothing,"In statistics, additive smoothing, also called Laplace smoothing (not to be confused with Laplacian smoothing as used in image processing), or Lidstone smoothing, is a technique used to smooth categorical data.",https://en.wikipedia.org/wiki/Additive_smoothing
Apprenticeship learning,"In artificial intelligence, apprenticeship learning (or learning from demonstration) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.",https://en.wikipedia.org/wiki/Apprenticeship_learning
Discriminative model,"Discriminative models, also referred to as conditional models, are a class of models used in statistical classification, especially in supervised machine learning. A discriminative classifier tries to model by just depending on the observed data while learning how to do the classification from the given statistics.",https://en.wikipedia.org/wiki/Discriminative_model
Varimax rotation,"In statistics, a varimax rotation is used to simplify the expression of a particular sub-space in terms of just a few major items each.  The actual coordinate system is unchanged, it is the orthogonal basis that is being rotated to align with those coordinates.  The sub-space found with principal component analysis or factor analysis is expressed as a dense basis with many non-zero weights which makes it hard to interpret.  Varimax is so called because it maximizes the sum of the variances of the squared loadings (squared correlations between variables and factors). Preserving orthogonality requires that it is a rotation that leaves the sub-space invariant. Intuitively, this is achieved if, (a) any given variable has a high loading on a single factor but near-zero loadings on the remaining factors and if (b) any given factor is constituted by only a few variables with very high loadings on this factor while the remaining variables have near-zero loadings on this factor.  If these conditions hold, the factor loading matrix is said to have ""simple structure,"" and varimax rotation brings the loading matrix closer to such simple structure (as much as the data allow).  From the perspective of individuals measured on the variables, varimax seeks a basis that most economically represents each individual—that is, each individual can be well described by a linear combination of only a few basis functions.",https://en.wikipedia.org/wiki/Varimax_rotation
Neural Engineering Object,"Neural Engineering Object (Nengo) is a graphical and scripting software for simulating large-scale neural systems. As Neural network software Nengo is a tool for modelling neural networks with applications in cognitive science, psychology, Artificial Intelligence and neuroscience.",https://en.wikipedia.org/wiki/Neural_Engineering_Object
Blowfish (cipher),"Blowfish is a symmetric-key block cipher, designed in 1993 by Bruce Schneier and included in many cipher suites and encryption products. Blowfish provides a good encryption rate in software and no effective cryptanalysis of it has been found to date. However, the Advanced Encryption Standard (AES) now receives more attention, and Schneier recommends Twofish for modern applications.",https://en.wikipedia.org/wiki/Blowfish_(cipher)
Gosper's algorithm,"In mathematics, Gosper's algorithm, due to Bill Gosper, is a procedure for finding sums of hypergeometric terms that are themselves hypergeometric terms. That is: suppose one has a(1) + ... + a(n) = S(n) − S(0), where S(n) is a hypergeometric term (i.e., S(n + 1)/S(n) is a rational function of n); then necessarily a(n) is itself a hypergeometric term, and given the formula for a(n) Gosper's algorithm finds that for S(n).",https://en.wikipedia.org/wiki/Gosper%27s_algorithm
Run-length encoding,"Run-length encoding (RLE) is a form of lossless data compression in which runs of data (sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run.  This is most useful on data that contains many such runs. Consider, for example, simple graphic images such as icons, line drawings, Conway’s Game of Life, and animations. It is not useful with files that don't have many runs as it could greatly increase the file size.",https://en.wikipedia.org/wiki/Run-length_encoding
Conjugate gradient method,"In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite. The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. Large sparse systems often arise when numerically solving partial differential equations or optimization problems.",https://en.wikipedia.org/wiki/Conjugate_gradient
Robert Tibshirani,"Robert Tibshirani FRS FRSC (born July 10, 1956) is a Professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a Professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.",https://en.wikipedia.org/wiki/Robert_Tibshirani
Developmental robotics,"Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence, developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.",https://en.wikipedia.org/wiki/Developmental_robotics
Ant colony optimization algorithms,"In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. The pheromone-based communication of biological ants is often the predominant paradigm used.   Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima.",https://en.wikipedia.org/wiki/Artificial_ants
Cone tracing,"Cone tracing and beam tracing are a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with thick rays.",https://en.wikipedia.org/wiki/Cone_tracing
Exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.",https://en.wikipedia.org/wiki/Truncated_binary_exponential_backoff
Evolutionary algorithm,"In artificial intelligence (AI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.",https://en.wikipedia.org/wiki/Evolutionary_algorithm
Nearest neighbour algorithm,"The nearest neighbour algorithm was one of the first algorithms used to solve the travelling salesman problem approximately. In that problem, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. The algorithm quickly yields a short tour, but usually not the optimal one.",https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.",https://en.wikipedia.org/wiki/Feature_(machine_learning)
Weka (machine learning),"Waikato Environment for Knowledge Analysis (Weka), developed at the University of Waikato, New Zealand. It is free software licensed under the GNU General Public License, and the companion software to the book ""Data Mining: Practical Machine Learning Tools and Techniques"".",https://en.wikipedia.org/wiki/Weka_(machine_learning)
Conjunctive normal form,"In Boolean logic, a formula is in conjunctive normal form (CNF) or clausal normal form if it is a conjunction of one or more clauses, where a clause is a disjunction of literals; otherwise put, it is an AND of ORs. As a canonical normal form, it is useful in automated theorem proving and circuit theory.",https://en.wikipedia.org/wiki/Conjunctive_normal_form
Search engine optimization,,https://en.wikipedia.org/wiki/Search_engine_optimization
Rete algorithm,"The Rete algorithm (/ˈriːtiː/ REE-tee, /ˈreɪtiː/ RAY-tee, rarely /ˈriːt/ REET, /rɛˈteɪ/ reh-TAY) is a pattern matching algorithm for implementing rule-based systems. The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base. It is used to determine which of the system's rules should fire based on its data store, its facts. The Rete algorithm was designed by Charles L. Forgy of Carnegie Mellon University, first published in a working paper in 1974, and later elaborated in his 1979 Ph.D. thesis and a 1982 paper .",https://en.wikipedia.org/wiki/Rete_algorithm
Help:IPA/French,"The charts below show the way in which the International Phonetic Alphabet (IPA) represents French language pronunciations in Wikipedia articles. For a guide to adding IPA characters to Wikipedia articles, see {{IPA-fr}}, {{IPAc-fr}} and Wikipedia:Manual of Style/Pronunciation § Entering IPA characters.",https://en.wikipedia.org/wiki/Help:IPA/French
Blum Blum Shub,"Blum Blum Shub (B.B.S.) is a pseudorandom number generator proposed in 1986 by Lenore Blum, Manuel Blum and Michael Shub that is derived from Michael O. Rabin's one-way function.",https://en.wikipedia.org/wiki/Blum_Blum_Shub
Artificial neural network,Topics,https://en.wikipedia.org/wiki/Artificial_neural_network
Melomics,"Melomics (derived from ""genomics of melodies"") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms.",https://en.wikipedia.org/wiki/Melomics
Medoid,"Medoids are representative objects of a data set or a cluster with a data set whose average dissimilarity to all the objects in the cluster is minimal. Medoids are similar in concept to means or centroids, but medoids are always restricted to be members of the data set. Medoids are most commonly used on data when a mean or centroid cannot be defined, such as graphs. They are also used in contexts where the centroid is not representative of the dataset like in images and 3-D trajectories and  gene expression  (where while the data is sparse the medoid need not be).  These are also of interest while wanting to find a representative using some distance other than squared euclidean distance (for instance in movie-ratings).",https://en.wikipedia.org/wiki/Medoid
Lax–Wendroff method,"The Lax–Wendroff method, named after Peter Lax and Burton Wendroff, is a numerical method for the solution of hyperbolic partial differential equations, based on finite differences. It is second-order accurate in both space and time. This method is an example of explicit time integration where the function that defines the governing equation is evaluated at the current time.",https://en.wikipedia.org/wiki/Lax%E2%80%93Wendroff_method
Diffie–Hellman key exchange,Diffie–Hellman key exchange[nb 1] is a method of securely exchanging cryptographic keys over a public channel and was one of the first public-key protocols as originally conceptualized by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography.,https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange
Exponential function,,https://en.wikipedia.org/wiki/Exponential_function
Ben Taskar,"Ben Taskar (March 3, 1977 – November 18, 2013) was a professor and researcher in the area of machine learning and applications to computational linguistics and computer vision.  He was a Magerman Term Associate Professor for Computer and Information Science at University of Pennsylvania. He co-directed PRiML: Penn Research in Machine Learning, a joint venture between the School of Engineering and Wharton. He was also a Distinguished Research Fellow at the Annenberg Center for Public Policy. At the University of Washington, he held the Boeing Professorship.",https://en.wikipedia.org/wiki/Ben_Taskar
Crossover (genetic algorithm),"In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population.",https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)
Linkurious,Linkurious is a French software company that provides social network analysis primarily through graph visualization.,https://en.wikipedia.org/wiki/Linkurious
Buzen's algorithm,"In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or  convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.",https://en.wikipedia.org/wiki/Buzen%27s_algorithm
Case-based reasoning,"Case-based reasoning (CBR), broadly construed, is the process of solving new problems based on the solutions of similar past problems. An auto mechanic who fixes an engine by recalling another car that exhibited similar symptoms is using case-based reasoning.  A lawyer who advocates a particular outcome in a trial based on legal precedents or a judge who creates case law is using case-based reasoning.  So, too, an engineer copying working elements of nature (practicing biomimicry), is treating nature as a database of solutions to problems. Case-based reasoning is a prominent type of analogy solution making.",https://en.wikipedia.org/wiki/Case-based_reasoning
Hierarchical hidden Markov model,The hierarchical hidden Markov model (HHMM) is a statistical model derived from the hidden Markov model (HMM). In an HHMM each state is considered to be a self-contained probabilistic model. More precisely each state of the HHMM is itself an HHMM. ,https://en.wikipedia.org/wiki/Hierarchical_hidden_Markov_model
Canopy clustering algorithm,"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.",https://en.wikipedia.org/wiki/Canopy_clustering_algorithm
De Boor's algorithm,"In the mathematical subfield of numerical analysis de Boor's algorithm is a polynomial-time and numerically stable algorithm for evaluating spline curves in B-spline form. It is a generalization of de Casteljau's algorithm for Bézier curves. The algorithm was devised by Carl R. de Boor.  Simplified, potentially faster variants of the de Boor algorithm have been created but they suffer from comparatively lower stability.",https://en.wikipedia.org/wiki/De_Boor_algorithm
Shortest remaining time,"Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling. In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.",https://en.wikipedia.org/wiki/Shortest_remaining_time
Global illumination,"Global illumination (shortened as GI), or indirect illumination is a group of algorithms used in 3D computer graphics that are meant to add more realistic lighting to 3D scenes. Such algorithms take into account not only the light that comes directly from a light source (direct illumination), but also subsequent cases in which light rays from the same source are reflected by other surfaces in the scene, whether reflective or not (indirect illumination).",https://en.wikipedia.org/wiki/Global_illumination
Sweep line algorithm,"In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. It is one of the key techniques in computational geometry.",https://en.wikipedia.org/wiki/Sweep_line_algorithm
Shortest Path Faster Algorithm,"The Shortest Path Faster Algorithm (SPFA) is an improvement of the Bellman–Ford algorithm which computes single-source shortest paths in a weighted directed graph. The algorithm is believed to work well on random sparse graphs and is particularly suitable for graphs that contain negative-weight edges. However, the worst-case complexity of SPFA is the same as that of Bellman–Ford, so for graphs with nonnegative edge weights Dijkstra's algorithm is preferred.  The SPFA algorithm was first published by Edward F. Moore in 1959, as a generalization of breadth first search; the same algorithm was rediscovered in 1994 by Fanding Duan.",https://en.wikipedia.org/wiki/Shortest_Path_Faster_Algorithm
Jabberwacky,"Jabberwacky is a chatterbot created by British programmer Rollo Carpenter. Its stated aim is to ""simulate natural human chat in an interesting, entertaining and humorous manner"". It is an early attempt at creating an artificial intelligence through human interaction.",https://en.wikipedia.org/wiki/Jabberwacky
Mathematical constant,"A mathematical constant is a number whose value is fixed by an unambiguous definition, often referred to by a symbol or by mathematicians' names to facilitate using it across multiple mathematical problems. Constants arise in many areas of mathematics, with constants such as e and π occurring in such diverse contexts as geometry, number theory, and calculus.",https://en.wikipedia.org/wiki/Mathematical_constant
Graph traversal,"In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph. Such traversals are classified by the order in which the vertices are visited. Tree traversal is a special case of graph traversal.",https://en.wikipedia.org/wiki/Graph_traversal
Dictionary coder,"A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. When the encoder finds such a match, it substitutes a reference to the string's position in the data structure.",https://en.wikipedia.org/wiki/Dictionary_coder
Path dependence,"Path dependence explains how the set of decisions people face for any given circumstance is limited by the decisions they have made in the past or by the events that they experienced, even though past circumstances may no longer be relevant.",https://en.wikipedia.org/wiki/Path_dependence
Decision tree model,,https://en.wikipedia.org/wiki/Decision_tree_model
SAS (software),"SAS (previously ""Statistical Analysis System"") is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics.",https://en.wikipedia.org/wiki/SAS_(software)
Insertion sort,"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort.",https://en.wikipedia.org/wiki/Insertion_sort
Relief (feature selection),"Relief is an algorithm developed by Kira and Rendell in 1992 that takes a filter-method approach to feature selection that is notably sensitive to feature interactions.  It was originally designed for application to binary classification problems with discrete or numerical features.  Relief calculates a feature score for each feature which can then be applied to rank and select top scoring features for feature selection.  Alternatively, these scores may be applied as feature weights to guide downstream modeling. Relief feature scoring is based on the identification of feature value differences between nearest neighbor instance pairs. If a feature value difference is observed in a neighboring instance pair with the same class (a 'hit'), the feature score decreases.  Alternatively, if a feature value difference is observed in a neighboring instance pair with different class values (a 'miss'), the feature score increases. The original Relief algorithm has since inspired a family of Relief-based feature selection algorithms (RBAs), including the ReliefF algorithm.  Beyond the original Relief algorithm, RBAs have been adapted to (1) perform more reliably in noisy problems, (2) generalize to multi-class problems  (3) generalize to numerical outcome (i.e. regression) problems, and (4) to make them robust to incomplete (i.e. missing) data.",https://en.wikipedia.org/wiki/Relief_(feature_selection)
Zeller's congruence,Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date. It can be considered to be based on the conversion between Julian day and the calendar date.,https://en.wikipedia.org/wiki/Zeller%27s_congruence
Path tracing,"Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically correct cameras, path tracing can produce still images that are indistinguishable from photographs.",https://en.wikipedia.org/wiki/Path_tracing
Operator-precedence parser,"In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN).",https://en.wikipedia.org/wiki/Operator-precedence_parser
Language Acquisition Device (computer),"The Language Acquisition Device is a computer program developed by Lobal Technologies, a computer company in the United Kingdom, and scientists from King's College. It emulates the functions of the brain's frontal lobes where humans process language and emotion.",https://en.wikipedia.org/wiki/Language_Acquisition_Device_(computer)
Elastic map,"Elastic maps provide a tool for nonlinear dimensionality reduction. By their construction, they are a system of elastic springs  embedded in the dataspace. This system approximates a low-dimensional manifold. The elastic coefficients of this system allow the switch from completely unstructured k-means clustering (zero elasticity) to the estimators located closely to linear PCA manifolds (for high bending and low stretching modules). With some intermediate values of the elasticity coefficients, this system effectively approximates non-linear principal manifolds. This approach is based on a mechanical analogy between principal manifolds, that are passing through ""the middle"" of the data distribution, and elastic membranes and plates. The method was developed by A.N. Gorban, A.Y. Zinovyev and A.A. Pitenko in 1996–1998.",https://en.wikipedia.org/wiki/Elastic_map
BCH code,"In coding theory, the BCH codes or Bose–Chaudhuri–Hocquenghem codes form a class of cyclic error-correcting codes that are constructed using polynomials over a finite field (also called Galois field). BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Bose and D. K. Ray-Chaudhuri. The name Bose–Chaudhuri–Hocquenghem (and the acronym BCH) arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).",https://en.wikipedia.org/wiki/BCH_Code
Quickhull,"Quickhull is a method of computing the convex hull of a finite set of points in n-dimensional space. It uses a divide and conquer approach similar to that of quicksort, from which its name derives. Its worst case complexity for 2-dimensional and 3-dimensional space is considered to be O(n∗log(r)), where n is the number of processed points. However, unlike quicksort, there is no obvious way to convert quickhull into a randomized algorithm. Thus, its average time complexity cannot be easily calculated.",https://en.wikipedia.org/wiki/Quickhull
Oren Etzioni,"Oren Etzioni (born 1964) is an American entrepreneur, professor of computer science, and CEO of the Allen Institute for Artificial Intelligence. He joined the University of Washington faculty in 1991, where he became the Washington Research Foundation Entrepreneurship Professor in the Department of Computer Science and Engineering. In May 2005, he founded and became the director of the university's Turing Center. The center investigated problems in data mining, natural language processing, the Semantic Web and other web search topics. Etzioni coined the term machine reading and helped to create the first commercial comparison shopping agent.",https://en.wikipedia.org/wiki/Oren_Etzioni
Secant method,,https://en.wikipedia.org/wiki/Secant_method
Prior knowledge for pattern recognition,"Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs (xi,yi),y_{i})} that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.",https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition
Lloyd's algorithm,"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces and partitions of these subsets into well-shaped and uniformly sized convex cells.  Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition and then re-partitions the input according to which of these centroids is closest. In this setting, the mean operation is an integral over a region of space, and the nearest centroid operation results in Voronoi diagrams.",https://en.wikipedia.org/wiki/Lloyd%27s_algorithm
Set partitioning in hierarchical trees,Set partitioning in hierarchical trees (SPIHT) is an image compression algorithm that exploits the inherent similarities across the subbands in a wavelet decomposition of an image. The algorithm was developed by Brazilian engineer Amir Said with William A. Pearlman in 1996.,https://en.wikipedia.org/wiki/Set_Partitioning_in_Hierarchical_Trees
Ward's method,"In statistics, Ward's method is a criterion applied in hierarchical cluster analysis. Ward's minimum variance method is a special case of the objective function approach originally presented by Joe H. Ward, Jr. Ward suggested a general agglomerative hierarchical clustering procedure, where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function. This objective function could be ""any function that reflects the investigator's purpose."" Many of the standard clustering procedures are contained in this very general class. To illustrate the procedure, Ward used the example where the objective function is the error sum of squares, and this example is known as Ward's method or more precisely Ward's minimum variance method.",https://en.wikipedia.org/wiki/Ward%27s_method
Burrows–Wheeler transform,"The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. This is useful for compression, since it tends to be easy to compress a string that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding.  More importantly, the transformation is reversible, without needing to store any additional data except the position of the first original character. The BWT is thus a ""free"" method of improving the efficiency of text compression algorithms, costing only some extra computation.",https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform
SHA-1,"In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function which takes an input and  produces a 160-bit (20-byte) hash value known as a message digest – typically rendered as a hexadecimal number, 40 digits long. It was designed by the United States National Security Agency, and is a U.S. Federal Information Processing Standard.",https://en.wikipedia.org/wiki/SHA-1
New York State Identification and Intelligence System,"The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services). It features an accuracy increase of 2.7% over the traditional Soundex algorithm.",https://en.wikipedia.org/wiki/New_York_State_Identification_and_Intelligence_System
Generative topographic map,"Generative topographic map (GTM) is a machine learning method that is a probabilistic counterpart of the self-organizing map (SOM), is probably convergent and does not require a shrinking neighborhood or a decreasing step size. It is a generative model: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the expectation-maximization (EM) algorithm.  GTM was introduced in 1996 in a paper by Christopher Bishop, Markus Svensen, and Christopher K. I. Williams.",https://en.wikipedia.org/wiki/Generative_topographic_map
Salsa20,"Salsa20 and the closely related ChaCha are stream ciphers developed by Daniel J. Bernstein. Because the two ciphers are very similar, this article will describe both together. Salsa20, the original cipher, was designed in 2005, then later submitted to eSTREAM by Bernstein. ChaCha is a modification of Salsa20 published in 2008. It uses a new round function that increases diffusion and increases performance on some architectures.",https://en.wikipedia.org/wiki/Salsa20
Statistical parsing,"Statistical parsing is a group of parsing methods within natural language processing.  The methods have in common that they associate grammar rules with a probability.  Grammar rules are traditionally viewed in computational linguistics as defining the valid sentences in a language.  Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence.  (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.)  Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate's probability, to derive the most probable parse of a sentence.  The Viterbi algorithm is one popular method of searching for the most probable parse.",https://en.wikipedia.org/wiki/Statistical_parsing
Grammar induction,"Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.",https://en.wikipedia.org/wiki/Grammar_induction
Linde–Buzo–Gray algorithm,"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.",https://en.wikipedia.org/wiki/Linde%E2%80%93Buzo%E2%80%93Gray_algorithm
k-nearest neighbors algorithm,"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.",https://en.wikipedia.org/wiki/Nearest_neighbor_(pattern_recognition)
Transform coding,"Transform coding is a type of data compression for ""natural"" data like audio signals or photographic images. The transformation is typically lossless (perfectly reversible) on its own but is used to enable better (more targeted) quantization, which then results in a lower quality copy of the original input (lossy compression).",https://en.wikipedia.org/wiki/Transform_coding
Minimum degree algorithm,"In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor.This results in reduced storage requirements and means that the Cholesky factor can be applied with fewer arithmetic operations. (Sometimes it may also pertain to an incomplete Cholesky factor used as a preconditioner, for example in the preconditioned conjugate gradient algorithm.)",https://en.wikipedia.org/wiki/Minimum_degree_algorithm
Congruence coefficient,"In multivariate statistics, the congruence coefficient is an index of the similarity between factors that have been derived in a factor analysis. It was introduced in 1948 by Cyril Burt who referred to it as unadjusted correlation. It is also called Tucker's congruence coefficient after Ledyard Tucker who popularized the technique. Its values range between -1 and +1. It can be used to study the similarity of extracted factors across different samples of, for example, test takers who have taken the same test.",https://en.wikipedia.org/wiki/Congruence_coefficient
Gaussian elimination,"Gaussian elimination, also known as row reduction, is an algorithm in linear algebra for solving a system of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855). Some special cases of the method - albeit presented without proof - were known to Chinese mathematicians as early as circa 179 AD.",https://en.wikipedia.org/wiki/Gauss%E2%80%93Jordan_elimination
Universal code (data compression),"In data compression, a universal code for integers is a prefix code that maps the positive integers onto  binary codewords, with the additional property that whatever the true probability distribution on integers, as long as the distribution is monotonic (i.e., p(i) ≥ p(i + 1) for all positive i), the expected lengths of the codewords are within a constant factor of the expected lengths that the optimal code for that probability distribution would have assigned.  A universal code is asymptotically optimal if the ratio between actual and optimal expected lengths is bounded by a function of the information entropy of the code that, in addition to being bounded, approaches 1 as entropy approaches infinity.",https://en.wikipedia.org/wiki/Universal_code_(data_compression)
Fuzzy clustering,Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.,https://en.wikipedia.org/wiki/Fuzzy_clustering
Vapnik–Chervonenkis dimension,"In Vapnik–Chervonenkis theory, the Vapnik–Chervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a space of functions that can be learned by a statistical classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.",https://en.wikipedia.org/wiki/VC_dimension
British Museum algorithm,,https://en.wikipedia.org/wiki/British_Museum_algorithm
Feature Selection Toolbox,,https://en.wikipedia.org/wiki/Feature_Selection_Toolbox
Trapezoidal rule (differential equations),"In numerical analysis and scientific computing, the trapezoidal rule is a numerical method to solve ordinary differential equations derived from the trapezoidal rule for computing integrals. The trapezoidal rule is an implicit second-order method, which can be considered as both a Runge–Kutta method and a linear multistep method.",https://en.wikipedia.org/wiki/Trapezoidal_rule_(differential_equations)
Sliced inverse regression,Sliced inverse regression (SIR) is a tool for dimension reduction in the field of multivariate statistics.,https://en.wikipedia.org/wiki/Sliced_inverse_regression
LIBSVM,"LIBSVM and LIBLINEAR are two popular open source machine learning libraries, both developed at the National Taiwan University and both written in C++ though with a C API. LIBSVM implements the Sequential minimal optimization (SMO) algorithm for kernelized support vector machines (SVMs), supporting classification and regression.LIBLINEAR implements linear SVMs and logistic regression models trained using a coordinate descent algorithm.",https://en.wikipedia.org/wiki/LIBSVM
Journal of Machine Learning Research,"The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria), David Blei (Columbia University) and Bernhard Schölkopf (Max Planck Institute for Intelligent Systems).",https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research
Incremental heuristic search,"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s.",https://en.wikipedia.org/wiki/Incremental_heuristic_search
Dual-phase evolution,"Dual phase evolution (DPE) is a process that drives self-organization within complex adaptive systems. It arises in response to phase changes within the network of connections formed by a system's components. DPE occurs in a wide range of physical, biological and social systems. Its applications to technology include methods for manufacturing novel materials and algorithms to solve complex problems in computation.",https://en.wikipedia.org/wiki/Dual-phase_evolution
Cultural algorithm,"Cultural algorithms (CA) are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component. In this sense, cultural algorithms can be seen as an extension to a conventional genetic algorithm. Cultural algorithms were introduced by Reynolds (see references).",https://en.wikipedia.org/wiki/Cultural_algorithm
List of metaphor-based metaheuristics,This is a chronologically ordered list of metaphor-based metaheuristics and swarm intelligence algorithms.,https://en.wikipedia.org/wiki/Harmony_search
Classifier chains,Classifier chains is a machine learning method for problem transformation in multi-label classification. It combines the computational efficiency of the Binary Relevance method while still being able to take the label dependencies  into account for classification.,https://en.wikipedia.org/wiki/Classifier_chains
Cohen–Sutherland algorithm,The Cohen–Sutherland algorithm is a computer-graphics algorithm used for line clipping. The algorithm divides a two-dimensional space into 9 regions and then efficiently determines the lines and portions of lines that are visible in the central region of interest (the viewport).,https://en.wikipedia.org/wiki/Cohen%E2%80%93Sutherland
Dunn index,"The Dunn index (DI) (introduced by J. C. Dunn in 1974) is a metric for evaluating clustering algorithms. This is part of a group of validity indices including the Davies–Bouldin index or Silhouette index, in that it is an internal evaluation scheme, where the result is based on the clustered data itself. As do all other such indices, the aim is to identify sets of clusters that are compact, with a small variance between members of the cluster, and well separated, where the means of different clusters are sufficiently far apart, as compared to the within cluster variance. For a given assignment of clusters, a higher Dunn index indicates better clustering. One of the drawbacks of using this is the computational cost as the number of clusters and dimensionality of the data increase.",https://en.wikipedia.org/wiki/Dunn_index
Cholesky decomposition,"In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced /ʃo-LESS-key/) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e.g., Monte Carlo simulations. It was discovered by André-Louis Cholesky for real matrices. When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.",https://en.wikipedia.org/wiki/Cholesky_decomposition
Pratt parser,"In computer science, a Pratt parser is an improved recursive descent parser that associates semantics with tokens instead of grammar rules. It was first described by Vaughan Pratt in the 1973 paper ""Top down operator precedence"", and was treated in much more depth in a Masters Thesis under his supervision.  Pratt designed the parser originally to implement the CGOL programming language. Douglas Crockford used the technique to build JSLint.",https://en.wikipedia.org/wiki/Pratt_parser
Decision tree learning,"Decision tree learning is one of the predictive modeling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.",https://en.wikipedia.org/wiki/Classification_and_regression_tree
Coppersmith–Winograd algorithm,"In linear algebra, the Coppersmith–Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known matrix multiplication algorithm from 1990 until 2010. It can multiply two n×n matrices in O(n2.375477)(n^{2.375477})} time  (see Big O notation). This is an improvement over the naïve O(n3) time algorithm and the O(n2.807355)(n^{2.807355})} time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical.It is possible to improve the exponent further; however, the exponent must be at least 2 (because an n×n values, and all of them have to be read at least once to calculate the exact result).",https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm
Genetic algorithms in economics,"Genetic algorithms have increasingly been applied to economics since the pioneering work by John H. Miller in 1986. It has been used to characterize a variety of models including the cobweb model, the overlapping generations model, game theory, schedule optimization and asset pricing. Specifically, it has been used as a model to represent learning, rather than as a means for fitting a model.",https://en.wikipedia.org/wiki/Genetic_algorithms_in_economics
MaxCliqueDyn maximum clique algorithm,"The MaxCliqueDyn algorithm is an algorithm for finding a maximum clique in an undirected graph. It is based on a basic algorithm (MaxClique algorithm) which finds a maximum clique of bounded size. The bound is found using improved coloring algorithm. The MaxCliqueDyn extends MaxClique algorithm to include dynamically varying bounds. This algorithm was designed by Janez Konc and description was published in 2007. In comparison to earlier algorithms described in the published article  the MaxCliqueDyn algorithm is improved by an improved approximate coloring algorithm (ColorSort algorithm) and by applying tighter, more computationally expensive upper bounds on a fraction of the search space. Both improvements reduce time to find maximum clique. In addition to reducing time improved coloring algorithm also reduces the number of steps needed to find a maximum clique.",https://en.wikipedia.org/wiki/MaxCliqueDyn_maximum_clique_algorithm
Join (SQL),"An SQL join clause - corresponding to a join operation in relational algebra - combines columns from one or more tables in a relational database. It creates a set that can be saved as a table or used as it is. A JOIN is a means for combining columns from one (self-join) or more tables by using values common to each. ANSI-standard SQL specifies five types of JOIN: INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. As a special case, a table (base table, view, or joined table) can JOIN to itself in a self-join.",https://en.wikipedia.org/wiki/Join_(SQL)
Time complexity,"In computer science, the time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.",https://en.wikipedia.org/wiki/Linear_time
Error-driven learning,Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning.,https://en.wikipedia.org/wiki/Error-driven_learning
Multilevel feedback queue,"In computer science, a multilevel feedback queue is a scheduling algorithm. Solaris 2.6 Time-Sharing (TS) scheduler implements this algorithm. The MacOS and Microsoft Windows schedulers can both be regarded as examples of the broader class of multilevel feedback queue schedulers.",https://en.wikipedia.org/wiki/Multi_level_feedback_queue
Elbow method (clustering),"The elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help find the appropriate number of clusters in a dataset. It is often ambiguous and not very reliable, and hence other approaches for determining the number of clusters such as the silhouette method are preferable. ",https://en.wikipedia.org/wiki/Elbow_method_(clustering)
Sutherland–Hodgman algorithm,The Sutherland–Hodgman algorithm is an algorithm used for clipping polygons. It works by extending each line of the convex clip polygon in turn and selecting only vertices from the subject polygon that are on the visible side.,https://en.wikipedia.org/wiki/Sutherland%E2%80%93Hodgman
Luleå algorithm,"The Luleå algorithm of computer science, designed by Degermark et al. (1997), is a  technique for storing and searching internet routing tables efficiently. It is named after the Luleå University of Technology, the home institute/university of the technique's authors. The name of the algorithm does not appear in the original paper describing it, but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.",https://en.wikipedia.org/wiki/Lule%C3%A5_algorithm
Median filter,"The Median Filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image). Median filtering is very widely used in digital image processing because, under certain conditions, it preserves edges while removing noise (but see discussion below), also having applications in signal processing.",https://en.wikipedia.org/wiki/Median_filtering
Biclustering,"Biclustering, block clustering, co-clustering, or two-mode clustering is a data mining technique which allows simultaneous clustering of the rows and columns of a matrix.The term was first introduced by Boris Mirkin to name a technique introduced many years earlier, in 1972, by J. A. Hartigan.",https://en.wikipedia.org/wiki/Biclustering
Selection algorithm,"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.",https://en.wikipedia.org/wiki/Selection_algorithm
Spectral envelope,"A spectral envelope  is the envelope curve of the amplitude spectrum. It describes one point in time (one window, to be precise).",https://en.wikipedia.org/wiki/Spectral_envelope
Samplesort,"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing p−1 < s elements from the result. These elements (called splitters) then divide the sample into p equal-sized buckets. Samplesort is described in the 1970 paper, ""Samplesort: A Sampling Approach to Minimal Storage Tree Sorting"", by W. D. Frazer and A. C. McKellar.",https://en.wikipedia.org/wiki/Samplesort
Michael Collins (computational linguist),,https://en.wikipedia.org/wiki/Michael_Collins_(computational_linguist)
Intel RealSense,"Intel RealSense Technology is a suite of depth and tracking technologies designed to give machines and devices depth perceptions capabilities that will enable them to “see” and understand the world.  There are many uses for these computer vision capabilities including autonomous drones, robots, AR/VR, smart home devices amongst many others broad market products.  RealSense technology is made of Vision Processors, Depth and Tracking Modules, and Depth Cameras, supported by an open source, cross-platform SDK called librealsense that simplifies supporting cameras for third party software developers, system integrators, ODMs and OEMs.",https://en.wikipedia.org/wiki/Intel_RealSense
WPGMA,"WPGMA (Weighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method, generally attributed to Sokal and Michener.",https://en.wikipedia.org/wiki/WPGMA
Buddy memory allocation,"The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. This system makes use of splitting memory into halves to try to give a best fit.  According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, and was first described by Kenneth C. Knowlton (published 1965). The Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks.",https://en.wikipedia.org/wiki/Buddy_memory_allocation
Inside–outside algorithm,"In computer science, the inside–outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar. It was introduced by James K. Baker in 1979 as a generalization of the forward–backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation–maximization algorithm (an unsupervised learning algorithm).",https://en.wikipedia.org/wiki/Inside-outside_algorithm
Apache Mahout,"Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark. Mahout also provides Java/Scala libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.",https://en.wikipedia.org/wiki/Apache_Mahout
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.",https://en.wikipedia.org/wiki/Feature_vector
Cryptographically secure pseudorandom number generator,"A cryptographically secure pseudorandom number generator (CSPRNG) or cryptographic pseudorandom number generator (CPRNG) is a pseudorandom number generator (PRNG) with properties that make it suitable for use in cryptography. It is also loosely known as a cryptographic random number generator (CRNG) (see Random number generation#""True"" vs. pseudo-random numbers).",https://en.wikipedia.org/wiki/Cryptographically_secure_pseudo-random_number_generator
Complete-linkage clustering,"Complete-linkage clustering is one of several methods of agglomerative hierarchical clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. The method is also known as farthest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place.",https://en.wikipedia.org/wiki/Complete-linkage_clustering
Statistical semantics,"In linguistics, statistical semantics applies the methods of statistics to the problem of determining the meaning of words or phrases, ideally through unsupervised learning, to a degree of precision at least sufficient for the purpose of information retrieval.",https://en.wikipedia.org/wiki/Statistical_semantics
Sequitur algorithm,Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. The algorithm operates in linear space and time. It can be used in data compression software applications.,https://en.wikipedia.org/wiki/SEQUITUR_algorithm
niki.ai,,https://en.wikipedia.org/wiki/Niki.ai
Cantor–Zassenhaus algorithm,"In computational algebra, the Cantor–Zassenhaus algorithm is a method for factoring polynomials over finite fields (also called Galois fields).",https://en.wikipedia.org/wiki/Cantor%E2%80%93Zassenhaus_algorithm
Hierarchical classification,"A hierarchical classifier is a classifier that maps input data into defined subsumptive output categories.[example  needed] The classification occurs first on a low-level with highly specific pieces of input data. The classifications of the individual pieces of data are then combined systematically and classified on a higher level iteratively until one output is produced. This final output is the overall classification of the data. Depending on application-specific details, this output can be one of a set of pre-defined outputs, one of a set of on-line learned outputs, or even a new novel classification that hasn't been seen before. Generally, such systems rely on relatively simple individual units of the hierarchy that have only one universal function to do the classification. In a sense, these machines rely on the power of the hierarchical structure itself instead of the computational abilities of the individual components. This makes them relatively simple, easily expandable, and very powerful.",https://en.wikipedia.org/wiki/Hierarchical_classifier
Hash join,The hash join is an example of a join algorithm and is used in the implementation of a relational database management system.,https://en.wikipedia.org/wiki/Hash_join
Davis–Putnam algorithm,"The Davis–Putnam algorithm was developed by Martin Davis and Hilary Putnam for checking the validity of a first-order logic formula using a resolution-based decision procedure for propositional logic. Since the set of valid first-order formulas is recursively enumerable but not recursive, there exists no general algorithm to solve this problem. Therefore, the Davis–Putnam algorithm only terminates on valid formulas. Today, the term ""Davis–Putnam algorithm"" is often used synonymously with the resolution-based propositional decision procedure that is actually only one of the steps of the original algorithm.",https://en.wikipedia.org/wiki/Davis%E2%80%93Putnam_algorithm
Convolutional neural network,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series..mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}",https://en.wikipedia.org/wiki/Convolutional_neural_network
Strongly connected component,"In the mathematical theory of directed graphs, a graph is said to be strongly connected or diconnected if every vertex is reachable from every other vertex. The strongly connected components or diconnected components of an arbitrary directed graph form a partition into subgraphs that are themselves strongly connected. It is possible to test the strong connectivity of a graph, or to find its strongly connected components, in linear time (that is, Θ(V+E)).",https://en.wikipedia.org/wiki/Strongly_connected_components
Multiple factor analysis,Multiple factor analysis (MFA) is a factorial method devoted to the study of tables in which a group of individuals is described by a set of variables (quantitative and / or qualitative) structured in groups. It may be seen as an extension of: ,https://en.wikipedia.org/wiki/Multiple_factor_analysis
Levenberg–Marquardt algorithm,"In mathematics and computing, the Levenberg–Marquardt algorithm (LMA or just LM), also known as the  Damped least-squares (DLS) method, is used to solve non-linear least squares problems.  These minimization problems arise especially in least squares curve fitting.",https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm
Variable-order Markov model,"In the mathematical theory of stochastic processes, variable-order Markov (VOM) models are an important class of models that extend the well known Markov chain models. In contrast to the Markov chain models, where each random variable in a sequence with a Markov property depends on a fixed number of random variables, in VOM models this number of conditioning random variables may vary based on the specific observed realization.",https://en.wikipedia.org/wiki/Variable-order_Markov_model
Dialogue system,"A dialogue system, or conversational agent (CA), is a computer system intended to converse with a human. Dialogue systems employed one or more of text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.",https://en.wikipedia.org/wiki/Dialog_system
n-body problem,"In physics, the  n-body problem is the problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally. Solving this problem has been motivated by the desire to understand the motions of the Sun, Moon, planets, and visible stars. In the 20th century, understanding the dynamics of globular cluster star systems became an important n-body problem. The n-body problem in general relativity is considerably more difficult to solve.",https://en.wikipedia.org/wiki/N-body_problem
Risch algorithm,"In symbolic computation (or computer algebra), at the intersection of mathematics and computer science, the Risch algorithm is an algorithm for indefinite integration. It is used in some computer algebra systems to find antiderivatives. It is named after the American mathematician Robert Henry Risch, a specialist in computer algebra who developed it in 1968.",https://en.wikipedia.org/wiki/Risch_algorithm
Portable Format for Analytics,The Portable Format for Analytics (PFA) is a JSON-based predictive model interchange format conceived and developed by Jim Pivarski.[citation needed] PFA provides a way for analytic applications to describe and exchange predictive models produced by analytics and machine learning algorithms. It supports common models such as logistic regression and decision trees. Version 0.8 was published in 2015.  Subsequent versions have been developed by the Data Mining Group.,https://en.wikipedia.org/wiki/Portable_Format_for_Analytics
Ant colony optimization algorithms,"In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. The pheromone-based communication of biological ants is often the predominant paradigm used.   Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima.",https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms
Hopkins statistic,"The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set. It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0[citation needed].",https://en.wikipedia.org/wiki/Hopkins_statistic
k-nearest neighbors algorithm,"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.",https://en.wikipedia.org/wiki/K-nearest_neighbors
Grammatical evolution,"Grammatical evolution is a evolutionary computation technique pioneered by Conor Ryan, JJ Collins and Michael O'Neill in 1998 at the BDS Group in the University of Limerick.",https://en.wikipedia.org/wiki/Grammatical_evolution
Part-of-speech tagging,"In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.",https://en.wikipedia.org/wiki/Part-of-speech_tagging
GNU Octave,"GNU Octave is software featuring a High-level programming language, primarily intended for numerical computations. Octave helps in solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language.Since it is part of the GNU Project, it is free software under the terms of the GNU General Public License.",https://en.wikipedia.org/wiki/GNU_Octave
Adjusted mutual information,"In probability theory and information theory, adjusted mutual information, a variation of mutual information may be used for comparing clusterings. It corrects the effect of agreement solely due to chance between clusterings, similar to the way the adjusted rand index corrects the Rand index. It is closely related to variation of information: when a similar adjustment is made to the VI index, it becomes equivalent to the AMI. The adjusted measure however is no longer metrical.",https://en.wikipedia.org/wiki/Adjusted_mutual_information
C3 linearization,"In computing, C3 superclass linearization is an algorithm used primarily to obtain the order in which methods should be inherited in the presence of multiple inheritance. In other words, the output of C3 superclass linearization is a deterministic Method Resolution Order (MRO).  ",https://en.wikipedia.org/wiki/C3_linearization
Exponentiation by squaring,"In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix. Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. These can be of quite general use, for example in modular arithmetic or powering of matrices. For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add.",https://en.wikipedia.org/wiki/Exponentiating_by_squaring
Recursion,,https://en.wikipedia.org/wiki/Recursion
Multilayer perceptron,"A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to refer to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology. Multilayer perceptrons are sometimes colloquially referred to as ""vanilla"" neural networks, especially when they have a single hidden layer.",https://en.wikipedia.org/wiki/Multilayer_perceptron
Keras,"Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, a Google engineer. Chollet also is the author of the XCeption deep neural network model.",https://en.wikipedia.org/wiki/Keras
CORDIC,,https://en.wikipedia.org/wiki/CORDIC
Ernst Dickmanns,"Ernst Dieter Dickmanns is a German pioneer of dynamic computer vision and of driverless cars. Dickmanns has been a professor at Bundeswehr University Munich (1975–2001), and visiting professor to Caltech and to MIT, teaching courses on ""dynamic vision"".",https://en.wikipedia.org/wiki/Ernst_Dickmanns
Bubble sort,"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements ""bubble"" to the top of the list. ",https://en.wikipedia.org/wiki/Bubble_sort
Handwriting recognition,,https://en.wikipedia.org/wiki/Handwriting_recognition
Rattle GUI,"Rattle GUI is a free and open source software (GNU GPL v2) package providing a graphical user interface (GUI) for data mining using the R statistical programming language. Rattle is used in a variety of situations. Currently there are 15 different government departments in Australia, in addition to various other organisations around the world, which use Rattle in their data mining activities and as a statistical package.",https://en.wikipedia.org/wiki/Rattle_GUI
FELICS,,https://en.wikipedia.org/wiki/FELICS
Primality test,"A primality test is an algorithm for determining whether an input number is prime. Among other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might more accurately be called compositeness tests instead of primality tests.",https://en.wikipedia.org/wiki/Primality_test
Binary GCD algorithm,"The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction. Although the algorithm was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known in 1st-century China.",https://en.wikipedia.org/wiki/Binary_GCD_algorithm
Multiplicative inverse,"In mathematics, a multiplicative inverse or reciprocal for a number x, denoted by 1/x or x−1, is a number which when multiplied by x yields the multiplicative identity, 1. The multiplicative inverse of a fraction a/b is b/a. For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The reciprocal function, the function f(x) that maps x to 1/x, is one of the simplest examples of a function which is its own inverse (an involution).",https://en.wikipedia.org/wiki/Multiplicative_inverse
State–action–reward–state–action,"State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name ""Modified Connectionist Q-Learning"" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.",https://en.wikipedia.org/wiki/State-Action-Reward-State-Action
End-to-end reinforcement learning,"In end-to-end reinforcement learning, the end-to-end process, in other words, the entire process from sensors to motors in a robot or agent involves a single, layered or recurrent neural network without modularization, and is trained by reinforcement learning (RL). The approach has been proposed for a long time, but was reenergized by the successful results in learning to play Atari video games (2013–15) and AlphaGo (2016) by Google DeepMind.",https://en.wikipedia.org/wiki/End-to-end_reinforcement_learning
Data compression,"In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.",https://en.wikipedia.org/wiki/Audio_data_compression
Multi-label classification,"In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to.",https://en.wikipedia.org/wiki/Multi-label_classification
The Master Algorithm,The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.,https://en.wikipedia.org/wiki/The_Master_Algorithm
Multifactor dimensionality reduction,"Multifactor dimensionality reduction (MDR) is a statistical approach, also used in machine learning automatic approaches, for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify nonadditive interactions among discrete variables that influence a binary outcome and is considered a nonparametric and model-free alternative to traditional statistical methods such as logistic regression.",https://en.wikipedia.org/wiki/Multifactor_dimensionality_reduction
Baum–Welch algorithm,"In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is a special case of the EM algorithm used to find the unknown parameters of a hidden Markov model (HMM).  It makes use of the forward-backward algorithm to compute the statistics for the expectation step.",https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
Nelder–Mead method,Nelder–Mead simplex search over the Rosenbrock banana function (above) and Himmelblau's function (below),https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method
Cache language model,"A cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of  speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems.[citation needed]",https://en.wikipedia.org/wiki/Cache_language_model
Lucas primality test,"In computational number theory, the Lucas test is a primality test for a natural number n; it requires that the prime factors of n − 1 be already known. It is the basis of the Pratt certificate that gives a concise verification that n is prime.",https://en.wikipedia.org/wiki/Lucas_primality_test
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-06570-7
AIVA,,https://en.wikipedia.org/wiki/AIVA
Dissociated press,"Dissociated press is a parody generator (a computer program that generates nonsensical text). The generated text is based on another text using the Markov chain technique. The name is a play on ""Associated Press"" and the psychiatric term dissociative identity disorder (which may result in somewhat similar word salad during quick switches between personalities).",https://en.wikipedia.org/wiki/Dissociated_press
PSIPRED,,https://en.wikipedia.org/wiki/PSIPRED
Key derivation function,,https://en.wikipedia.org/wiki/Password_hashing
Clonal selection algorithm,"In artificial immune systems, clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen-antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.",https://en.wikipedia.org/wiki/Clonal_selection_algorithm
Best bin first,Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.,https://en.wikipedia.org/wiki/Best_Bin_First
Paxos (computer science),"Paxos is a family of protocols for solving consensus in a network of unreliable processors (that is, processors that may fail).Consensus is the process of agreeing on one result among a group of participants.  This problem becomes difficult when the participants or their communication medium may experience failures.",https://en.wikipedia.org/wiki/Paxos_algorithm
Lagged Fibonacci generator,A Lagged Fibonacci generator (LFG or sometimes LFib) is an example of a pseudorandom number generator. This class of random number generator is aimed at being an improvement on the 'standard' linear congruential generator. These are based on a generalisation of the Fibonacci sequence.,https://en.wikipedia.org/wiki/Lagged_Fibonacci_generator
Histogram equalization,Histogram equalization is a method in image processing of contrast adjustment using the image's histogram.,https://en.wikipedia.org/wiki/Histogram_equalization
Buchberger's algorithm,"In computational algebraic geometry and computational commutative algebra, Buchberger's algorithm is a method of transforming a given set of generators for a polynomial ideal into a Gröbner basis with respect to some monomial order. It was invented by Austrian mathematician Bruno Buchberger. One can view it as a generalization of the Euclidean algorithm for univariate GCD computation and of Gaussian elimination for linear systems.",https://en.wikipedia.org/wiki/Buchberger%27s_algorithm
Convex set,"In geometry, a subset of a Euclidean space, or more generally an affine space over the reals, is convex if, with any two points, it contains the whole line segment that joins them. Equivalently, a convex set or a convex region is a subset that intersect every line into a single line segment (possibly empty).For example, a solid cube is a convex set, but anything that is hollow or has an indent, for example, a crescent shape, is not convex.",https://en.wikipedia.org/wiki/Convex_set
Conceptual clustering,"Conceptual clustering is a machine learning paradigm for unsupervised classification developed mainly during the 1980s.  It is distinguished from ordinary data clustering by generating a concept description for each generated class.  Most conceptual clustering methods are capable of generating hierarchical category structures;  see Categorization for more information on hierarchy.  Conceptual clustering is closely related to formal concept analysis, decision tree learning, and mixture model learning.",https://en.wikipedia.org/wiki/Conceptual_clustering
Constructive cooperative coevolution,"The constructive cooperative coevolutionary algorithm (also called C3) is a global optimisation algorithm in artificial intelligence based on the multi-start architecture of the greedy randomized adaptive search procedure (GRASP). It incorporates the existing cooperative coevolutionary algorithm (CC). The considered problem is decomposed into subproblems. These subproblems are optimised separately while exchanging information in order to solve the complete problem. An optimisation algorithm, usually but not necessarily an evolutionary algorithm, is embedded in C3 for optimising those subproblems. The nature of the embedded optimisation algorithm determines whether C3's behaviour is deterministic or stochastic.",https://en.wikipedia.org/wiki/Constructive_cooperative_coevolution
Action model learning,Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.,https://en.wikipedia.org/wiki/Action_model_learning
Positron emission tomography,"Positron-emission tomography (PET) is a nuclear medicine functional imaging technique that is used to observe metabolic processes in the body as an aid to the diagnosis of disease. The system detects pairs of gamma rays emitted indirectly by a positron-emitting radioligand, most commonly fluorine-18, which is introduced into the body on a biologically active molecule called a radioactive tracer. Different ligands are used for different imaging purposes, depending on what the radiologist/researcher wants to detect. Three-dimensional images of tracer concentration within the body are then constructed by computer analysis. In modern PET computed tomography scanners, three-dimensional imaging is often accomplished with the aid of a computed tomography X-ray scan performed on the patient during the same session, in the same machine. ",https://en.wikipedia.org/wiki/Positron_emission_tomography
Caffe (software),"CAFFE (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.",https://en.wikipedia.org/wiki/Caffe_(software)
Hamming distance,,https://en.wikipedia.org/wiki/Hamming_distance
Yann LeCun,,https://en.wikipedia.org/wiki/Yann_LeCun
Sequential minimal optimization,"Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support-vector machines (SVM). It was invented by John Platt in 1998 at Microsoft Research. SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool. The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.",https://en.wikipedia.org/wiki/Sequential_minimal_optimization
Digital differential analyzer (graphics algorithm),"In computer graphics, a digital differential analyzer (DDA) is hardware or software used for interpolation of variables over an interval between start and end point. DDAs are used for rasterization of lines, triangles and polygons. They can be extended to non linear functions, such as perspective correct texture mapping, quadratic curves, and traversing voxels.",https://en.wikipedia.org/wiki/Digital_Differential_Analyzer_(graphics_algorithm)
Rader's FFT algorithm,"Rader's algorithm (1968), named for Charles M. Rader of MIT Lincoln Laboratory, is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).",https://en.wikipedia.org/wiki/Rader%27s_FFT_algorithm
Data mining,"Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.",https://en.wikipedia.org/wiki/Data_mining
Apache SINGA,"Apache SINGA is an Apache top-level project for developing an open source machine learning library. It provides a flexible architecture for scalable distributed training, is extensible to run over a wide range of hardware, and has a focus on health-care applications.",https://en.wikipedia.org/wiki/Apache_SINGA
Sammon mapping,"Sammon mapping or Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality (see multidimensional scaling) by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. The method was proposed by John W. Sammon in 1969. It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications.",https://en.wikipedia.org/wiki/Sammon_mapping
Question answering,"Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.",https://en.wikipedia.org/wiki/Question_answering
Gene expression programming,,https://en.wikipedia.org/wiki/Gene_expression_programming
Brown clustering,"Brown clustering is a hard hierarchical agglomerative clustering problem based on distributional information proposed by Peter Brown, Vincent Della Pietra, Peter deSouza, Jennifer Lai, and Robert Mercer. It is typically applied to text, grouping words into clusters that are assumed to be semantically related by virtue of their having been embedded in similar contexts.",https://en.wikipedia.org/wiki/Brown_clustering
Quadratic unconstrained binary optimization,"Quadratic unconstrained binary optimization (QUBO) is a pattern matching technique, common in machine learning applications. QUBO is an NP hard problem. Examples of problems that can be formulated as QUBO problems are the Maximum cut, Graph coloring and the Partition problem.",https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization
Fermat's factorization method,"Fermat's factorization method, named after Pierre de Fermat, is based on the representation of an odd integer as the difference of two squares:",https://en.wikipedia.org/wiki/Fermat%27s_factorization_method
Closest pair of points problem,"The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. The closest pair problem for points in the Euclidean plane was among the first geometric problems that were treated at the origins of the systematic study of the computational complexity of geometric algorithms.",https://en.wikipedia.org/wiki/Closest_pair_problem
Preference regression,Preference regression is a statistical technique used by marketers to determine consumers’ preferred core benefits. It usually supplements  product positioning techniques like multi dimensional scaling or factor analysis and is used to create ideal vectors on perceptual maps.,https://en.wikipedia.org/wiki/Preference_regression
Machine learning control,"Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theorywhich solves optimal control problems with methods of machine learning.Key applications are complex nonlinear systemsfor which linear control theory methods are not applicable.",https://en.wikipedia.org/wiki/Machine_learning_control
Tikhonov regularization,"Tikhonov regularization, named for Andrey Tikhonov, is a method of regularization of ill-posed problems. Also known as ridge regression, it is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).",https://en.wikipedia.org/wiki/Ridge_regression
Documenting Hate,"Documenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. As of October 2017, over 100 news organizations had joined the project.",https://en.wikipedia.org/wiki/Documenting_Hate
Dynamic Markov compression,"Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool.  It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time).  DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented.  Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney.  These are based on the 1993 implementation in C by Gordon Cormack.",https://en.wikipedia.org/wiki/Dynamic_Markov_compression
FastICA,"FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.",https://en.wikipedia.org/wiki/FastICA
Computer science,,https://en.wikipedia.org/wiki/Computer_science
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-02093-3
TIMIT,TIMIT is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects.  Each transcribed element has been delineated in time.,https://en.wikipedia.org/wiki/TIMIT
Alpha max plus beta min algorithm,"The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, because it finds the hypotenuse of a right triangle given the two side lengths, the norm of a 2-D vector, or the magnitude |z|=a2+b2 of a complex number z = a + bi given the real and imaginary parts.",https://en.wikipedia.org/wiki/Alpha_max_plus_beta_min_algorithm
Accord.NET,"Accord.NET is a framework for scientific computing in .NET. The source code of the project is available under the terms of the Gnu Lesser Public License, version 2.1.",https://en.wikipedia.org/wiki/Accord.NET
Machine learning in bioinformatics,"Machine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data. ",https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics
Semantic mapping (statistics),"Semantic mapping (SM) is a method in statistics for dimensionality reduction that can be used in a set of multidimensional vectors of features to extract a few new features that preserves the main data characteristics. SM performs dimensionality reduction by clustering the original features in semantic clusters and combining features mapped in the same cluster to generate an extracted feature. Given a data set, this method constructs a projection matrix that can be used to map a data element from a high-dimensional space into a reduced dimensional space. SM can be applied in construction of text mining and information retrieval systems, as well as systems managing vectors of high dimensionality.SM is an alternative to random mapping, principal components analysis and latent semantic indexing methods.",https://en.wikipedia.org/wiki/Semantic_mapping_(statistics)
Extreme learning machine,"Extreme learning machines are feedforward neural networks for classification, regression, clustering, sparse approximation, compression and feature learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. The name ""extreme learning machine"" (ELM) was given to such models by its main inventor Guang-Bin Huang.",https://en.wikipedia.org/wiki/Extreme_learning_machine
Elias delta coding,Elias δ code or Elias delta code is a universal code encoding the positive integers developed by Peter Elias.:200,https://en.wikipedia.org/wiki/Elias_delta_coding
Cluster analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.",https://en.wikipedia.org/wiki/Data_clustering
Self-Service Semantic Suite,"The Self-Service Semantic Suite (S4) provides on-demand access to text mining and linked open data technology in the cloud.The S4 stack is based on enterprise-grade technology from Ontotext including their leading RDF engine (GraphDB, formerly OWLIM) and high performance text mining solutions successfully applied in some of the largest enterprises in the world.",https://en.wikipedia.org/wiki/Self-Service_Semantic_Suite
Genetic programming,"In artificial intelligence, genetic programming (GP) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs.  It is essentially a heuristic search technique often described as 'hill climbing', i.e. searching for an optimal or at least suitable program among the space of all programs.",https://en.wikipedia.org/wiki/Genetic_programming
Todd–Coxeter algorithm,"In group theory, the Todd–Coxeter algorithm, created by J. A. Todd and H. S. M. Coxeter in 1936, is an algorithm for solving the coset enumeration problem.  Given a presentation of a group G by generators and relations and a subgroup H of G, the algorithm enumerates the cosets of H on G and describes the permutation representation of G on the space of the cosets (given by the left multiplication action). If the order of a group G is relatively small and the subgroup H is known to be uncomplicated (for example, a cyclic group), then the algorithm can be carried out by hand and gives a reasonable description of the group G. Using their algorithm, Coxeter and Todd showed that certain systems of relations between generators of known groups are complete, i.e. constitute systems of defining relations.",https://en.wikipedia.org/wiki/Todd%E2%80%93Coxeter_algorithm
Line search,"In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum x∗. The other approach is trust region.",https://en.wikipedia.org/wiki/Line_search
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-0-12-374856-0
Novelty detection,"Novelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing. The principle is long known in neurophysiology, with roots in the orienting response research by E. N. Sokholov in the 1950s. The reverse phenomenon is habituation, i.e., the phenomenon that known patterns yield a less marked response. Early neural modeling attempts were by Yehuda Salu. An increasing body of knowledge has been collected concerning the corresponding mechanisms in the brain. In technology, the principle became important for radar detection methods during the Cold War, where unusual aircraft-reflection patterns could indicate an attack by a new type of aircraft. Today, the phenomenon plays an important role in machine learning and data science, where the corresponding methods are known as anomaly detection or outlier detection. An extensive methodological overview is given by Markou and Singh.",https://en.wikipedia.org/wiki/Novelty_detection
Shunting-yard algorithm,"In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the ""shunting yard"" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61.",https://en.wikipedia.org/wiki/Shunting_yard_algorithm
Kosaraju's algorithm,"In computer science, Kosaraju's algorithm (also known as the Kosaraju–Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph.  Aho, Hopcroft and Ullman credit it to S. Rao Kosaraju and Micha Sharir. Kosaraju suggested it in 1978 but did not publish it, while Sharir independently discovered it and published it in 1981. It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph.",https://en.wikipedia.org/wiki/Kosaraju%27s_algorithm
Pearson hashing,"Pearson hashing is a hash function designed for fast execution on processors with 8-bit registers. Given an input consisting of any number of bytes, it produces as output a single byte that is strongly dependent on every byte of the input. Its implementation requires only a few instructions, plus a 256-byte lookup table containing a permutation of the values 0 through 255.",https://en.wikipedia.org/wiki/Pearson_hashing
Monte Carlo integration,"In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly choose points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.",https://en.wikipedia.org/wiki/MISER_algorithm
Collision detection,"Collision detection is the computational problem of detecting the intersection of two or more objects. While collision detection is most often associated with its use in video games and other physical simulations, it also has applications in robotics. In addition to determining whether two objects have collided, collision detection systems may also calculate time of impact (TOI), and report a contact manifold (the set of intersecting points). Collision response deals with simulating what happens when a collision is detected (see physics engine, ragdoll physics). Solving collision detection problems requires extensive use of concepts from linear algebra and computational geometry.",https://en.wikipedia.org/wiki/Collision_detection
SHA-2,Pseudo-collision attack against up to 46 rounds of SHA-256.,https://en.wikipedia.org/wiki/SHA-2
Rule induction,"Rule induction is an area of machine learning in which formal rules are extracted from a set of observations.  The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.",https://en.wikipedia.org/wiki/Rule_induction
Markov decision process,"A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of the Markov chains.",https://en.wikipedia.org/wiki/Markov_decision_process
Linear programming,"Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).",https://en.wikipedia.org/wiki/Linear_programming
Product of experts,"Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions.It was proposed by Geoff Hinton, along with an algorithm for training the parameters of such a system.",https://en.wikipedia.org/wiki/Product_of_experts
Evolutionary Algorithm for Landmark Detection,"there are several algorithms for locating landmarks in images such as satellite maps, medical images etc.nowadays evolutionary algorithms such as particle swarm optimization are so useful to perform this task. evolutionary algorithms generally have two phase, training and test.",https://en.wikipedia.org/wiki/Evolutionary_Algorithm_for_Landmark_Detection
Successive over-relaxation,"In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence. A similar method can be used for any slowly converging iterative process.",https://en.wikipedia.org/wiki/Successive_over-relaxation
Ensemble averaging (machine learning),"In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models ""average out.""",https://en.wikipedia.org/wiki/Ensemble_averaging_(machine_learning)
Maximum flow problem,"In optimization theory, maximum flow problems involve finding a feasible flow through a flow network that obtains the maximum possible flow rate.",https://en.wikipedia.org/wiki/Maximum_flow_problem
Q-learning,"Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation ""model-free"") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.",https://en.wikipedia.org/wiki/Q-learning
Slerp,"In computer graphics, Slerp is shorthand for spherical linear interpolation, introduced by Ken Shoemake in the context of quaternion interpolation for the purpose of animating 3D rotation. It refers to constant-speed motion along a unit-radius great circle arc, given the ends and an interpolation parameter between 0 and 1.",https://en.wikipedia.org/wiki/Slerp
Deutsch–Jozsa algorithm,"The Deutsch–Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm and is the inspiration for Simon's Algorithm which is, in turn, the inspiration for Shor's Algorithm. It is also a deterministic algorithm, meaning that it always produces an answer, and that answer is always correct.",https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm
Information Harvesting,"Information Harvesting (IH) was an early data mining product from the 1990s.  It was invented by Ralphe Wiggins and produced by the Ryan Corp, later Information Harvesting Inc., of Cambridge, Massachusetts. Wiggins had a background in genetic algorithms and fuzzy logic. IH sought to infer rules from sets of data.  It did this first by classifying various input variables into one of a number of bins, thereby putting some structure on the continuous variables in the input.  IH then proceeds to generate rules, trading off generalization against memorization, that will infer the value of the prediction variable, possibly creating many levels of rules in the process.  It included strategies for checking if overfitting took place and, if so, correcting for it.  Because of its strategies for correcting for overfitting by considering more data, and refining the rules based on that data, IH might also be considered to be a form of machine learning.",https://en.wikipedia.org/wiki/Information_Harvesting
n-gram,"In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles[clarification needed].",https://en.wikipedia.org/wiki/N-gram
Matrix regularization,"In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over",https://en.wikipedia.org/wiki/Matrix_regularization
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/0-521-64298-1
Michael I. Jordan,"Michael Irwin Jordan (born February 25, 1956) is an American scientist, professor at the University of California, Berkeley and researcher in machine learning, statistics, and artificial intelligence. He is one of the leading figures in machine learning, and in 2016 Science reported him as the world's most influential computer scientist.",https://en.wikipedia.org/wiki/Michael_I._Jordan
Bernhard Schölkopf,"Bernhard Schölkopf (born February 20, 1968) is a director at the Max Planck Institute for Intelligent Systems in Tübingen, Germany, where he heads the Department of Empirical Inference.",https://en.wikipedia.org/wiki/Bernhard_Sch%C3%B6lkopf
Ordered subset expectation maximization,"In mathematical optimization, the ordered subset expectation maximization (OSEM) method   is an iterative method that is used in computed tomography.",https://en.wikipedia.org/wiki/Ordered_subset_expectation_maximization
Minimum cut,"In graph theory, a minimum cut or min-cut of a graph is a cut (a partition of the vertices of a graph into two disjoint subsets) that is minimal in some sense.",https://en.wikipedia.org/wiki/Minimum_cut
Bellman–Ford algorithm,"The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published the same algorithm in 1957, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.",https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm
InterNetNews,"InterNetNews (INN) is a Usenet news server package, originally released by Rich Salz in 1991, and presented at the Summer 1992 USENIX conference in San Antonio, Texas. It was the first news server with integrated NNTP functionality.",https://en.wikipedia.org/wiki/InterNetNews
Reed–Solomon error correction,"Reed–Solomon codes are a group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960.They have many applications, the most prominent of which include consumer technologies such as CDs, DVDs, Blu-ray discs, QR codes, data transmission technologies such as DSL and WiMAX, broadcast systems such as satellite communications, DVB and ATSC, and storage systems such as RAID 6.",https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction
National Institute of Standards and Technology,,https://en.wikipedia.org/wiki/NIST
Least squares,The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals made in the results of every single equation.,https://en.wikipedia.org/wiki/Least_squares
Winnow (algorithm),"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples.  It is very similar to the perceptron algorithm.  However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name winnow). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative.  The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.",https://en.wikipedia.org/wiki/Winnow_(algorithm)
Deep Web Technologies,"Deep Web Technologies  is a software company that specializes in mining the Deep Web — the part of the Internet  that is not directly searchable through ordinary web search engines. The company produces a proprietary software platform  ""Explorit""  for such searches. It also produces the federated search engine ScienceResearch.com, which provides free federated public searching of a large number of databases, and is also produced in specialized versions,  Biznar for business research, Mednar  for medical research, and customized versions for individual clients.",https://en.wikipedia.org/wiki/Deep_Web_Technologies
Adler-32,"Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum. Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32.",https://en.wikipedia.org/wiki/Adler-32
Matrix multiplication algorithm,"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such as counting the paths through a graph. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).",https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm
Douglas Lenat,"Douglas Bruce Lenat (born 1950) is the CEO of Cycorp, Inc. of Austin, Texas, and has been a prominent researcher in artificial intelligence; he was awarded the biannual IJCAI Computers and Thought Award in 1976 for creating the machine learning program, AM.  He has worked on (symbolic, not statistical) machine learning (with his AM and Eurisko programs),  knowledge representation, ""cognitive economy"", blackboard systems, and what he dubbed in 1984 ""ontological engineering"" (with his Cyc program at MCC and, since 1994, at Cycorp).  He has also worked in military simulations, and numerous projects for US government, military, intelligence, and scientific organizations.  In 1980, he published a critique of conventional random-mutation Darwinism.  He authored a series of articles in the Journal of Artificial Intelligence exploring the nature of heuristic rules.",https://en.wikipedia.org/wiki/Douglas_Lenat
Probably approximately correct learning,"In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.",https://en.wikipedia.org/wiki/PAC_learning
Eigenvalue algorithm,"In numerical analysis, one of the most important problems is designing efficient and stable algorithms for finding the eigenvalues of a matrix.  These eigenvalue algorithms may also find eigenvectors.",https://en.wikipedia.org/wiki/Eigenvalue_algorithm
Contour line,"A contour line (also isoline, isopleth, or isarithm) of a function of two variables is a curve along which the function has a constant value, so that the curve joins points of equal value. It is a plane section of the three-dimensional graph of the function f(x, y) parallel to the (x, y)-plane. In cartography, a contour line (often just called a ""contour"") joins points of equal elevation (height) above a given level, such as mean sea level. A contour map is a map illustrated with contour lines, for example a topographic map, which thus shows valleys and hills, and the steepness or gentleness of slopes.  The contour interval of a contour map is the difference in elevation between successive contour lines.",https://en.wikipedia.org/wiki/Contour_line
Measurement invariance,"Measurement invariance or measurement equivalence is a statistical property of measurement that indicates that the same construct is being measured across some specified groups. For example, measurement invariance can be used to study whether a given measure is interpreted in a conceptually similar manner by respondents representing different  genders or cultural backgrounds. Violations of measurement invariance  may preclude meaningful interpretation of measurement data. Tests of measurement invariance are increasingly used in fields such as psychology to supplement evaluation of measurement quality rooted in classical test theory.",https://en.wikipedia.org/wiki/Measurement_invariance
Affine transformation,"In geometry, an affine transformation, or an affinity (from the Latin, affinis, ""connected with"") is an automorphism of an affine space. More specifically, it is a function mapping an affine space onto itself that preserves the dimension of any affine subspaces (meaning that it sends points to points, lines to lines, planes to planes, and so on) and also preserves the ratio of the lengths of parallel line segments. Consequently, sets of parallel affine subspaces remain parallel after an affine transformation. An affine transformation does not necessarily preserve angles between lines or distances between points, though it does preserve ratios of distances between points lying on a straight line.",https://en.wikipedia.org/wiki/Affine_transformation
Nearest centroid classifier,"In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation.",https://en.wikipedia.org/wiki/Nearest_centroid_classifier
Xiaolin Wu's line algorithm,Xiaolin Wu's line algorithm is an algorithm for line antialiasing.,https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm
Java Grammatical Evolution,"In computer science, Java Grammatical Evolution is an implementation of grammatical evolution in the Java programming language. Examples include jGE library and GEVA.",https://en.wikipedia.org/wiki/Java_Grammatical_Evolution
Levenshtein coding,"Levenstein coding, or Levenshtein coding, is a universal code encoding the non-negative integers developed by Vladimir Levenshtein.",https://en.wikipedia.org/wiki/Levenshtein_coding
Ray tracing (graphics),,https://en.wikipedia.org/wiki/Ray_tracing_(graphics)
Lowest common ancestor,"In graph theory and computer science, the lowest common ancestor (LCA) of two nodes v and w in a tree or directed acyclic graph (DAG)  T is the lowest (i.e. deepest) node that has both v and w as descendants, where we define each node to be a descendant of itself (so if v has a direct connection from w, w is the lowest common ancestor).",https://en.wikipedia.org/wiki/Lowest_common_ancestor
Yoshua Bengio,,https://en.wikipedia.org/wiki/Yoshua_Bengio
DARPA LAGR Program,"The Learning Applied to Ground Vehicles (LAGR) program, which ran from 2004 until 2008, had the goal of accelerating progress in autonomous, perception-based, off-road navigation in robotic unmanned ground vehicles (UGVs).  LAGR was funded by DARPA, a research agency of the United States Department of Defense.",https://en.wikipedia.org/wiki/DARPA_LAGR_Program
Behavioral clustering,Behavioral clustering is a statistical analysis method used in retailing to identify consumer purchase trends and group stores based on consumer buying behaviors.,https://en.wikipedia.org/wiki/Behavioral_clustering
Christofides algorithm,"The Christofides algorithm is an algorithm for finding approximate solutions to the travelling salesman problem, on instances where the distances form a metric space (they are symmetric and obey the triangle inequality).It is an approximation algorithm that guarantees that its solutions will be within a factor of 3/2 of the optimal solution length, and is named after Nicos Christofides, who published it in 1976. As of 2019, this is the best approximation ratio that has been proven for the traveling salesman problem on general metric spaces, although better approximations are known for some special cases.",https://en.wikipedia.org/wiki/Christofides_algorithm
Vatti clipping algorithm,"The Vatti clipping algorithm is used in computer graphics. It allows clipping of any number of arbitrarily shaped subject polygons by any number of arbitrarily shaped clip polygons. Unlike the Sutherland–Hodgman and Weiler–Atherton polygon clipping algorithms, the Vatti algorithm does not restrict the types of polygons that can be used as subjects or clips. Even complex (self-intersecting) polygons, and polygons with holes can be processed. The algorithm is generally applicable only in 2D space.",https://en.wikipedia.org/wiki/Vatti_clipping_algorithm
Learning rule,"An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time.  Usually, this rule is applied repeatedly over the network.  It is done by updating the weights and bias levels of a network when a network is simulated in a specific data environment.  A learning rule may accept existing conditions (weights and biases) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias.  Depending on the complexity of actual model being simulated, the learning rule of the network can be as simple as an XOR gate or mean squared error, or as complex as the result of a system of differential equations.",https://en.wikipedia.org/wiki/Learning_rule
ELKI,"ELKI (for Environment for DeveLoping KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.",https://en.wikipedia.org/wiki/ELKI
Trie,"In computer science, a trie, also called digital tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node. For the space-optimized presentation of prefix tree, see compact prefix tree.",https://en.wikipedia.org/wiki/Trie
Gordon–Newell theorem,"In queueing theory, a discipline within the mathematical theory of probability, the Gordon–Newell theorem is an extension of Jackson's theorem from open queueing networks to closed queueing networks of exponential servers where customers cannot leave the network. Jackson's theorem cannot be applied to closed networks because the queue length at a node in the closed network is limited by the population of the network. The Gordon–Newell theorem calculates the open network solution and then eliminates the infeasible states by renormalizing the probabilities. Calculation of the normalizing constant makes the treatment more awkward as the whole state space must be enumerated. Buzen's algorithm or mean value analysis can be used to calculate the normalizing constant more efficiently.",https://en.wikipedia.org/wiki/Gordon%E2%80%93Newell_theorem
Fractal dimension,,https://en.wikipedia.org/wiki/Fractal_dimension
Pulse-coupled networks,"Pulse-coupled networks or pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat's visual cortex, and developed for high-performance biomimetic image processing.",https://en.wikipedia.org/wiki/Pulse-coupled_neural_networks
Pushpak Bhattacharyya,,https://en.wikipedia.org/wiki/Pushpak_Bhattacharyya
CoBoosting,CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999. The original application for the algorithm was the task of Named Entity Classification using very weak learners. It can be used for performing semi-supervised learning in cases in which there exist redundancy in features.,https://en.wikipedia.org/wiki/CoBoosting
Grafting (decision trees),Grafting is the process of adding nodes to inferred decision trees to improve the predictive accuracy.[clarification needed] A decision tree is a graphical model that is used as a support tool for decision process.,https://en.wikipedia.org/wiki/Grafting_(decision_trees)
Speech coding,"Speech coding is an application of data compression of digital audio signals containing speech. Speech coding uses speech-specific parameter estimation using audio signal processing techniques to model the speech signal, combined with generic data compression algorithms to represent the resulting modeled parameters in a compact bitstream.",https://en.wikipedia.org/wiki/Speech_encoding
HMAC,"In cryptography, an HMAC (sometimes expanded as either keyed-hash message authentication code or hash-based message authentication code) is a specific type of message authentication code (MAC) involving a cryptographic hash function and a secret cryptographic key. As with any MAC, it may be used to simultaneously verify both the data integrity and the authenticity of a message. Any cryptographic hash function, such as SHA-256 or SHA-3, may be used in the calculation of an HMAC; the resulting MAC algorithm is termed HMAC-X, where X is the hash function used (e.g. HMAC-SHA256 or HMAC-SHA3). The cryptographic strength of the HMAC depends upon the cryptographic strength of the underlying hash function, the size of its hash output, and the size and quality of the key.",https://en.wikipedia.org/wiki/Keyed-hash_message_authentication_code
Cultural consensus theory,"Cultural consensus theory is an approach to information pooling (aggregation, data fusion) which supports a framework for the measurement and evaluation of beliefs as cultural; shared to some extent by a group of individuals.  Cultural consensus models guide the aggregation of responses from individuals to estimate (1) the culturally appropriate answers to a series of related questions (when the answers are unknown) and (2) individual competence (cultural competence) in answering those questions.  The theory is applicable when there is sufficient agreement across people to assume that a single set of answers exists.  The agreement between pairs of individuals is used to estimate individual cultural competence.  Answers are estimated by weighting responses of individuals by their competence and then combining responses.",https://en.wikipedia.org/wiki/Cultural_consensus_theory
Multi expression programming,"Multi Expression Programming (MEP) is a genetic programming variant encoding multiple solutions in the same chromosome. MEP representation is not specific (multiple representations have been tested). In the simplest variant, MEP chromosomes are linear strings of instructions. This representation was inspired by Three-address code. MEP strength consists in the ability to encode multiple solutions, of a problem, in the same chromosome. In this way one can explore larger zones of the search space. For most of the problems this advantage comes with no running-time penalty compared with genetic programming variants encoding a single solution in a chromosome.",https://en.wikipedia.org/wiki/Multi_expression_programming
Evolutionary music,"Evolutionary music is the audio counterpart to evolutionary art, whereby algorithmic music is created using an evolutionary algorithm.  The process begins with a population of individuals which by some means or other produce audio (e.g. a piece, melody, or loop), which is either initialized randomly or based on human-generated music.  Then through the repeated application of computational steps analogous to biological selection, recombination and mutation the aim is for the produced audio to become more musical.  Evolutionary sound synthesis is a related technique for generating sounds or synthesizer instruments.  Evolutionary music is typically generated using an interactive evolutionary algorithm where the fitness function is the user or audience, as it is difficult to capture the aesthetic qualities of music computationally.  However, research into automated measures of musical quality is also active.  Evolutionary computation techniques have also been applied to harmonization and accompaniment tasks.  The most commonly used evolutionary computation techniques are genetic algorithms and genetic programming.",https://en.wikipedia.org/wiki/Evolutionary_music
Local case-control sampling,"In machine learning, local case-control sampling  is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most ""surprising"" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling.",https://en.wikipedia.org/wiki/Local_case-control_sampling
Swarm intelligence,"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.",https://en.wikipedia.org/wiki/Swarm_intelligence
Pseudorandom number generator,"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is  an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by an initial value, called the PRNG's seed (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility.",https://en.wikipedia.org/wiki/Pseudorandom_number_generator
Particle swarm optimization,,https://en.wikipedia.org/wiki/Particle_swarm_optimization
Data pre-processing,"Data preprocessing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.",https://en.wikipedia.org/wiki/Data_pre-processing
Verhoeff algorithm,"The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969.  It was the first decimal check digit algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code.",https://en.wikipedia.org/wiki/Verhoeff_algorithm
Addition-chain exponentiation,"In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. This corresponds to the sequence A003313 on the Online Encyclopedia of Integer Sequences. It works by creating the shortest addition chain that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying  two of the earlier exponentiation results. More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).",https://en.wikipedia.org/wiki/Addition-chain_exponentiation
Linear regression,"In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.",https://en.wikipedia.org/wiki/Linear_regression
Mexican International Conference on Artificial Intelligence,"The Mexican International Conference on Artificial Intelligence (MICAI) is the name of an annual conference covering all areas of Artificial Intelligence (AI), held in Mexico. The first MICAI conference was held in 2000. The conference is attended every year by about two hundred of AI researchers and PhD students and 500−1000 local graduate students.",https://en.wikipedia.org/wiki/Mexican_International_Conference_on_Artificial_Intelligence
Block nested loop,A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.,https://en.wikipedia.org/wiki/Block_nested_loop
Chirp Z-transform,"The chirp Z-transform (CZT) is a generalization of the discrete Fourier transform (DFT).  While the DFT samples the Z plane at uniformly-spaced points along the unit circle, the chirp Z-transform samples along spiral arcs in the Z-plane, corresponding to straight lines in the S plane.  The DFT, real DFT, and zoom DFT can be calculated as special cases of the CZT.",https://en.wikipedia.org/wiki/Bluestein%27s_FFT_algorithm
Julie Beth Lovins,Julie Beth Lovins was a computational linguist who first published a stemming algorithm for word matching in 1968.,https://en.wikipedia.org/wiki/Julie_Beth_Lovins
Dynamic programming,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.",https://en.wikipedia.org/wiki/Dynamic_programming
Fletcher's checksum,The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934–2012) at Lawrence Livermore Labs in the late 1970s.  The objective of the Fletcher checksum was to provide error-detection properties approaching those of a cyclic redundancy check but with the lower computational effort associated with summation techniques.,https://en.wikipedia.org/wiki/Fletcher%27s_checksum
MeeMix,"MeeMix Ltd is a company specializing in personalizing media-related content recommendations, discovery and advertising for the telecommunication industry, founded in 2006.",https://en.wikipedia.org/wiki/MeeMix
Gouraud shading,"Gouraud shading, named after Henri Gouraud, is an interpolation method used in computer graphics to produce continuous shading of surfaces represented by polygon meshes. In practice, Gouraud shading is most often used to achieve continuous lighting on triangle surfaces by computing the lighting at the corners of each triangle and linearly interpolating the resulting colours for each pixel covered by the triangle. Gouraud first published the technique in 1971.",https://en.wikipedia.org/wiki/Gouraud_shading
Barney Pell,"Barney Pell (born March 18, 1968) is an American entrepreneur, angel investor and computer scientist. He was co-founder, Vice Chairman and Chief Strategy Officer of Moon Express; co-founder and Chairman of LocoMobi; and Associate Founder of Singularity University. He was co-founder and CEO of Powerset, a pioneering natural language search startup, search strategist and architect for Microsoft's Bing search engine, a pioneer in the field of General Game Playing in Artificial Intelligence, and the architect of the first intelligent agent to fly onboard and control a spacecraft.",https://en.wikipedia.org/wiki/Barney_Pell
Lazy learning,"In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. ",https://en.wikipedia.org/wiki/Lazy_learning
Fortune's algorithm,"Fortune's algorithm is a sweep line algorithm for generating a Voronoi diagram from a set of points in a plane using O(n log n) time and O(n) space. It was originally published by Steven Fortune in 1986 in his paper ""A sweepline algorithm for Voronoi diagrams.""",https://en.wikipedia.org/wiki/Fortune%27s_Algorithm
Generalization error,"In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data. Because learning algorithms are evaluated on finite samples, the evaluation of a learning algorithm may be sensitive to sampling error. As a result, measurements of prediction error on the current data may not provide much information about predictive ability on new data. Generalization error can be minimized by avoiding overfitting in the learning algorithm. The performance of a machine learning algorithm is measured by plots of the generalization error values through the learning process, which are called learning curves.",https://en.wikipedia.org/wiki/Generalization_error
Multinomial logistic regression,"In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes.  That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.).",https://en.wikipedia.org/wiki/Multinomial_logistic_regression
Photon mapping,"In computer graphics, photon mapping is a two-pass global illumination rendering algorithm developed by Henrik Wann Jensen between 1995 and 2001 that approximately solves the rendering equation for integrating light radiance at a given point in space. Rays from the light source (like photons) and rays from the camera are traced independently until some termination criterion is met, then they are connected in a second step to produce a radiance value. The algorithm is used to realistically simulate the interaction of light with different types of objects (similar to other photorealistic rendering techniques). Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water (including caustics), diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, and some of the effects caused by particulate matter such as smoke or water vapor.  Photon mapping can also be extended to more accurate simulations of light, such as spectral rendering. Progressive photon mapping (PPM) starts with ray tracing and then adds more and more photon mapping passes to provide a progressively more accurate render.",https://en.wikipedia.org/wiki/Photon_mapping
Robot learning,"Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).",https://en.wikipedia.org/wiki/Robot_learning
Dependability state model,A dependability state diagram is a method for modelling a system as a Markov chain. It is used in reliability engineering for availability and reliability analysis.,https://en.wikipedia.org/wiki/Dependability_state_model
Decision tree pruning,"Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances.  Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.",https://en.wikipedia.org/wiki/Pruning_(decision_trees)
Burstsort,"Burstsort and its variants are cache-efficient algorithms for sorting strings.  They are variants of the traditional radix sort but faster for large data sets of common strings, first published in 2003, with some optimizing versions published in later years.",https://en.wikipedia.org/wiki/Burstsort
Information bottleneck method,"The information bottleneck method is a technique in information theory introduced by Naftali Tishby, Fernando C. Pereira, and William Bialek. It is designed for finding the best tradeoff between accuracy and complexity (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution p(X,Y) between X and an observed relevant variable Y - and described as providing ""a surprisingly rich framework for discussing a variety of problems in signal processing and learning"".",https://en.wikipedia.org/wiki/Information_bottleneck_method
Accuracy paradox,"The accuracy paradox is the paradoxical finding that accuracy is not a good metric for predictive models when classifying in predictive analytics.  This is because a simple model may have a high level of accuracy but be too crude to be useful.  For example, if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%.  Precision and recall are better measures in such cases.The underlying issue is that there is a class imbalance between the positive class and the negative class. Prior probabilities for these classes need to be accounted for in error analysis. Precision and recall help, but precision too can be biased by very unbalanced class priors in the test sets.",https://en.wikipedia.org/wiki/Accuracy_paradox
Comb sort,"Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980, later rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort.",https://en.wikipedia.org/wiki/Comb_sort
Adaptive histogram equalization,"Adaptive histogram equalization (AHE) is a computer image processing technique used to improve contrast in images. It differs from ordinary histogram equalization in the respect that the adaptive method computes several histograms, each corresponding to a distinct section of the image, and uses them to redistribute the lightness values of the image.  It is therefore suitable for improving the local contrast and enhancing the definitions of edges in each region of an image.",https://en.wikipedia.org/wiki/Adaptive_histogram_equalization
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/SRT_division
Relational data mining,"Relational data mining is the data mining technique for relationaldatabases. Unlike traditional data mining algorithms, which look forpatterns in a single table (propositional patterns), relational data mining algorithms look for patterns among multiple tables(relational patterns). For most types of propositionalpatterns, there are corresponding relational patterns. For example,there are relational classification rules (relational classification), relational regression tree, and relational association rules.",https://en.wikipedia.org/wiki/Relational_data_mining
Random projection,"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are known for their power, simplicity, and low error rates when compared to other methods[citation needed]. According to experimental results, random projection preserves distances well, but empirical results are sparse.They have been applied to many natural language tasks under the name random indexing.",https://en.wikipedia.org/wiki/Random_projection
Marzullo's algorithm,"Marzullo's algorithm, invented by Keith Marzullo for his Ph.D. dissertation in 1984, is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources. A refined version of it, renamed the ""intersection algorithm"", forms part of the modern Network Time Protocol.Marzullo's algorithm is also used to compute the relaxed intersection of n boxes (or more generally n subsets of Rn), as required by several robust set estimation methods.",https://en.wikipedia.org/wiki/Marzullo%27s_algorithm
Ranking SVM,"In machine learning, a Ranking SVM is a variant of the support vector machine algorithm, which is used to solve certain ranking problems (via learning to rank). The ranking SVM algorithm was published by Thorsten Joachims in 2002. The original purpose of the algorithm was to improve the performance of an internet search engine. However, it was found that Ranking SVM also can be used to solve other problems such as Rank SIFT.",https://en.wikipedia.org/wiki/Ranking_SVM
KHOPCA clustering algorithm,"KHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (k{\textstyle k}-hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA  operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.",https://en.wikipedia.org/wiki/KHOPCA_clustering_algorithm
Timeline of machine learning,"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.",https://en.wikipedia.org/wiki/Timeline_of_machine_learning
Multi-task learning,"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called ""hints"".",https://en.wikipedia.org/wiki/Multi-task_learning
Quadratic classifier,A quadratic classifier is used in machine learning and statistical classification to separate measurements of two or more classes of objects or events by a quadric surface. It is a more general version of the linear classifier.,https://en.wikipedia.org/wiki/Quadratic_classifier
Fault tolerance,,https://en.wikipedia.org/wiki/Fault-tolerant_system
Markov chain Monte Carlo,"In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis–Hastings algorithm.",https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo
LogitBoost,"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani.  The original paper casts the AdaBoost algorithm into a statistical framework.  Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost function of logistic regression, one can derive the LogitBoost algorithm.",https://en.wikipedia.org/wiki/LogitBoost
Apache MXNet,"Apache MXNet is an open-source deep learning software framework, used to train, and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl, and Wolfram Language.)",https://en.wikipedia.org/wiki/MXNet
SSS*,"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.",https://en.wikipedia.org/wiki/SSS*
Deductive classifier,"A deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies.",https://en.wikipedia.org/wiki/Deductive_classifier
Root-mean-square deviation,"The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.",https://en.wikipedia.org/wiki/RMSD
Midpoint circle algorithm,"In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for rasterizing a circle. Bresenham's circle algorithm is derived from the midpoint circle algorithm.[citation needed] The algorithm can be generalized to conic sections.",https://en.wikipedia.org/wiki/Midpoint_circle_algorithm
Transitive closure,"In mathematics, the transitive closure of a binary relation R on a set X is the smallest relation on X that contains R and is transitive.",https://en.wikipedia.org/wiki/Transitive_closure
Coset,"In mathematics, specifically group theory, given an element g of a group G and a subgroup H of G,",https://en.wikipedia.org/wiki/Coset
Match rating approach,The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.,https://en.wikipedia.org/wiki/Match_rating_approach
Hans-Peter Kriegel,"Hans-Peter Kriegel (1 October 1948, Germany) is a German computer scientist and professor at the Ludwig Maximilian University of Munich and leading the Database Systems Group in the Department of Computer Science.",https://en.wikipedia.org/wiki/Hans-Peter_Kriegel
ADALINE,"ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.",https://en.wikipedia.org/wiki/ADALINE
Polynomial long division,"In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations. Another abbreviated method is polynomial short division (Blomqvist's method).",https://en.wikipedia.org/wiki/Polynomial_long_division
Generalized filtering,"Generalized filtering is a generic Bayesian filtering scheme for nonlinear state-space models. It is based on a variational principle of least action, formulated in generalized coordinates. Note that the concept of ""generalized coordinates"" as used here differs from the concept of generalized coordinates of motion as used in (multibody) dynamical systems analysis. Generalized filtering furnishes posterior densities over hidden states (and parameters) generating observed data using a generalized gradient descent on variational free energy, under the Laplace assumption. Unlike classical (e.g. Kalman-Bucy or particle) filtering, generalized filtering eschews Markovian assumptions about random fluctuations. Furthermore, it operates online, assimilating data to approximate the posterior density  over unknown quantities, without the need for a backward pass. Special cases include variational filtering, dynamic expectation maximization and generalized predictive coding.",https://en.wikipedia.org/wiki/Generalized_filtering
Counting sort,"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently.",https://en.wikipedia.org/wiki/Counting_sort
Hough transform,"The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing.  The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.",https://en.wikipedia.org/wiki/Hough_transform
Gröbner basis,,https://en.wikipedia.org/wiki/Multivariate_division_algorithm
Muller's method,"Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. It was first presented by David E. Muller in 1956.",https://en.wikipedia.org/wiki/Muller%27s_method
Rand index,"The Rand index or Rand measure (named after William M. Rand) in statistics, and in particular in data clustering, is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance grouping of elements, this is the adjusted Rand index. From a mathematical standpoint, Rand index is related to the accuracy, but is applicable even when class labels are not used.",https://en.wikipedia.org/wiki/Rand_index
Lempel–Ziv–Welch,"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement and has the potential for very high throughput in hardware implementations.  It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format.",https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch
Google matrix,"A Google matrix is a particular stochastic matrix that is used by Google's PageRank algorithm. The matrix represents a graph with edges representing links between pages. The PageRank of each page can then be generated iteratively from the Google matrix using the power method. However, in order for the power method to converge, the matrix must be stochastic, irreducible and aperiodic.",https://en.wikipedia.org/wiki/Google_matrix
Nondeterministic algorithm,"In computer science, a nondeterministic algorithm is an algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm. There are several ways an algorithm may behave differently from run to run. A concurrent algorithm can perform differently on different runs due to a race condition. A probabilistic algorithm's behaviors depends on a random number generator. An algorithm that solves a problem in nondeterministic polynomial time can run in polynomial time or exponential time depending on the choices it makes during execution. The nondeterministic algorithms are often used to find an approximation to a solution, when the exact solution would be too costly to obtain using a deterministic one.",https://en.wikipedia.org/wiki/Non-deterministic_algorithm
Hidden semi-Markov model,A hidden semi-Markov model (HSMM) is a statistical model with the same structure as a hidden Markov model except that the unobservable process is semi-Markov rather than Markov. This means that the probability of there being a change in the hidden state depends on the amount of time that has elapsed since entry into the current state. This is in contrast to hidden Markov models where there is a constant probability of changing state given survival in the state up to that time.,https://en.wikipedia.org/wiki/Hidden_semi-Markov_model
Bresenham's line algorithm,"Bresenham's line algorithm is a line drawing algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points. It is commonly used to draw line primitives in a bitmap image (e.g. on a computer screen), as it uses only integer addition, subtraction and bit shifting, all of which are very cheap operations in standard computer architectures. It is an incremental error algorithm. It is one of the earliest algorithms developed in the field of computer graphics. An extension to the original algorithm may be used for drawing circles.",https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm
Cristian's algorithm,"Cristian's algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization if the round-trip time (RTT) of the request is short compared to required accuracy. It also suffers in implementations using a single server, making it unsuitable for many distributive applications where redundancy may be crucial.",https://en.wikipedia.org/wiki/Cristian%27s_algorithm
Dataiku,Dataiku is a computer software company headquartered in New York City. The company develops collaborative data science software marketed for big data.,https://en.wikipedia.org/wiki/Dataiku
Luhn mod N algorithm,"The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters. This can be useful when a check digit is required to validate an identification string composed of letters, a combination of letters and digits or even any arbitrary set of characters.",https://en.wikipedia.org/wiki/Luhn_mod_N_algorithm
Mutual exclusion,,https://en.wikipedia.org/wiki/Mutual_exclusion
Local search (optimization),"In computer science, local search is a heuristic method for solving computationally hard optimization problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes, until a solution deemed optimal is found or a time bound is elapsed.",https://en.wikipedia.org/wiki/Local_search_(optimization)
Sieve of Eratosthenes,"In mathematics, the Sieve of Eratosthenes is a simple and ingenious ancient algorithm for finding all prime numbers up to any given limit.",https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes
Open-source software,,https://en.wikipedia.org/wiki/Open-source_software
Kirkpatrick–Seidel algorithm,"The Kirkpatrick–Seidel algorithm, proposed by its authors as a potential ""ultimate planar convex hull algorithm"", is an algorithm for computing the convex hull of a set of points in the plane, with O(nlog⁡h) time complexity, where n is the number of points (non dominated or maximal points, as called in some texts) in the hull. Thus, the algorithm is output-sensitive: its running time depends on both the input size and the output size. Another output-sensitive algorithm, the gift wrapping algorithm, was known much earlier, but the Kirkpatrick–Seidel algorithm has an asymptotic running time that is significantly smaller and that always improves on the O(nlog⁡n) bounds of non-output-sensitive algorithms. The Kirkpatrick–Seidel algorithm is named after its inventors, David G. Kirkpatrick and Raimund Seidel.",https://en.wikipedia.org/wiki/Kirkpatrick%E2%80%93Seidel_algorithm
Cyrus–Beck algorithm,"The Cyrus–Beck algorithm is a generalized line clipping algorithm. It was designed to be more efficient than the Cohen–Sutherland algorithm, which uses repetitive clipping.  Cyrus–Beck is a general algorithm and can be used with a convex polygon clipping window, unlike Sutherland–Cohen, which can be used only on a rectangular clipping area.",https://en.wikipedia.org/wiki/Cyrus%E2%80%93Beck
Multiple instance learning,"In machine learning, multiple-instance learning (MIL) is a type of supervised learning.  Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative.  On the other hand, a bag is labeled positive if there is at least one instance in it which is positive.  From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.",https://en.wikipedia.org/wiki/Multiple_instance_learning
Maximum a posteriori estimation,"In Bayesian statistics, a maximum a posterior probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate.  MAP estimation can therefore be seen as a regularization of ML estimation.",https://en.wikipedia.org/wiki/Maximum_a_posteriori
Doomsday rule,"The Doomsday rule is an algorithm of determination of the day of the week for a given date. It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years. The algorithm for mental calculation was devised by John Conway in 1973, drawing inspiration from Lewis Carroll's perpetual calendar algorithm. It takes advantage of each year having a certain day of the week, called the doomsday, upon which certain easy-to-remember dates fall; for example, 4/4, 6/6, 8/8, 10/10, 12/12, and the last day of February all occur on the same day of the week in any year. Applying the Doomsday algorithm involves three steps: Determination of the anchor day for the century, calculation of the doomsday for the year from the anchor day, and selection of the closest date out of those that always fall on the doomsday, e.g., 4/4 and 6/6, and count of the number of days (modulo 7) between that date and the date in question to arrive at the day of the week. The technique applies to both the Gregorian calendar and the Julian calendar, although their doomsdays are usually different days of the week.",https://en.wikipedia.org/wiki/Doomsday_algorithm
Symbolic Cholesky decomposition,In the mathematical subfield of numerical analysis the symbolic Cholesky decomposition is an algorithm used to determine the non-zero pattern for the L factors of a symmetric sparse matrix when applying the Cholesky decomposition or variants.,https://en.wikipedia.org/wiki/Symbolic_Cholesky_decomposition
Center for Biological and Computational Learning,The Center for Biological & Computational Learning is a research lab at the Massachusetts Institute of Technology.,https://en.wikipedia.org/wiki/CBCL_(MIT)
Golomb coding,"Golomb coding is a lossless data compression method using a family of data compression codes invented by Solomon W. Golomb in the 1960s.  Alphabets following a geometric distribution will have a Golomb code as an optimal prefix code, making Golomb coding highly suitable for situations in which the occurrence of small values in the input stream is significantly more likely than large values.",https://en.wikipedia.org/wiki/Golomb_coding
Factor analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ""error"" terms. Factor analysis aims to find independent latent variables.",https://en.wikipedia.org/wiki/Factor_analysis
Junction tree algorithm,"The junction tree algorithm (also known as 'Clique Tree')  is a method used in machine learning to extract marginalization in general graphs.  In essence, it entails performing belief propagation on a modified graph called a junction tree. The graph is called a tree because it branches into different sections of data; nodes of variables are the branches. The basic premise is to eliminate cycles by clustering them into single nodes. Multiple extensive classes of queries can be compiled at the same time into larger structures of data. There are different algorithms to meet specific needs and for what needs to be calculated. Inference algorithms gather new developments in the data and calculate it based on the new information provided.",https://en.wikipedia.org/wiki/Junction_tree_algorithm
Probabilistic Action Cores,"PRAC (Probabilistic Action Cores) is an interpreter for natural-language instructions for robotic applications developed at the Institute for Artificial Intelligence at the University of Bremen, Germany, and is supported in parts by the European Commission and the German Research Foundation (DFG).",https://en.wikipedia.org/wiki/Probabilistic_Action_Cores
Arthur Zimek,"Arthur Zimek is a professor in data mining, data science and machine learning at the University of Southern Denmark in Odense, Denmark.",https://en.wikipedia.org/wiki/Arthur_Zimek
Shortest path problem,"In graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.",https://en.wikipedia.org/wiki/All_pairs_shortest_path
Depth-first search,Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.,https://en.wikipedia.org/wiki/Depth-first_search
Kalman filter,,https://en.wikipedia.org/wiki/Kalman_filter
Knowledge integration,Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation).,https://en.wikipedia.org/wiki/Knowledge_integration
Pareto distribution,"I(xm,α)=[αxm2−1xm−1xm1α2],\alpha )={\begin{bmatrix}{\dfrac {\alpha }{x_{\mathrm {m} }^{2}}}&-{\dfrac {1}{x_{\mathrm {m} }}}\\-{\dfrac {1}{x_{\mathrm {m} }}}&{\dfrac {1}{\alpha ^{2}}}\end{bmatrix}}}",https://en.wikipedia.org/wiki/Pareto_distribution
Proper generalized decomposition,The proper generalized decomposition (PGD) is a numerical method for solving partial differential equation. It assumes that the solution of a multidimensional (or multiparametric) problem can be expressed in a separated representation of the form,https://en.wikipedia.org/wiki/Proper_generalized_decomposition
Support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.",https://en.wikipedia.org/wiki/Support_vector_machine
Online machine learning,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.",https://en.wikipedia.org/wiki/Online_machine_learning
Generative adversarial network,"A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning. In a 2016 seminar, Yann LeCun described GANs as ""the coolest idea in machine learning in the last twenty years"".",https://en.wikipedia.org/wiki/Generative_adversarial_network
Iterative deepening depth-first search,"In computer science, iterative deepening search or more specifically iterative deepening depth-first search (IDS or IDDFS) is a state space/graph search strategy in which a depth-limited version of depth-first search is run repeatedly with increasing depth limits until the goal is found.  IDDFS is optimal like breadth-first search, but uses much less memory; at each iteration, it visits the nodes in the search tree in the same order as depth-first search, but the cumulative order in which nodes are first visited is effectively breadth-first.",https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search
Lamport's distributed mutual exclusion algorithm,Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system. ,https://en.wikipedia.org/wiki/Lamport%27s_Distributed_Mutual_Exclusion_Algorithm
De Casteljau's algorithm,"In the mathematical field of numerical analysis, De Casteljau's algorithm is a recursive method to evaluate polynomials in Bernstein form or Bézier curves, named after its inventor Paul de Casteljau. De Casteljau's algorithm can also be used to split a single Bézier curve into two Bézier curves at an arbitrary parameter value.",https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm
Mean shift,"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.",https://en.wikipedia.org/wiki/Mean-shift
Gauss–Newton algorithm,"The Gauss–Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.",https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm
Scanline rendering,"Scanline rendering (also scan line rendering and scan-line rendering) is an algorithm for visible surface determination, in 3D computer graphics,that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis.  All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scan line of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.",https://en.wikipedia.org/wiki/Scanline_rendering
Logistic model tree,"In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning.",https://en.wikipedia.org/wiki/Logistic_Model_Tree
Connected-component labeling,"Connected-component labeling (CCL), connected-component analysis (CCA), blob extraction, region labeling, blob discovery, or region extraction is an algorithmic application of graph theory, where subsets of connected components are uniquely labeled based on a given heuristic. Connected-component labeling is not to be confused with segmentation.",https://en.wikipedia.org/wiki/Connected-component_labeling
Corinna Cortes,"Corinna Cortes is a Danish computer scientist known for her contributions to machine learning. She is currently the Head of Google Research, New York. Cortes is a recipient of the Paris Kanellakis Theory and Practice Award for her work on theoretical foundations of support vector machines.",https://en.wikipedia.org/wiki/Corinna_Cortes
Pareto interpolation,"Pareto interpolation is a method of estimating the median and other properties of a population that follows a Pareto distribution.  It is used in economics when analysing the distribution of incomes in a population, when one must base estimates on a relatively small random sample taken from the population.",https://en.wikipedia.org/wiki/Pareto_interpolation
Dual-tone multi-frequency signaling,"Dual-tone multi-frequency signaling (DTMF) is a telecommunication signaling system using the voice-frequency band over telephone lines between telephone equipment and other communications devices and switching centers. DTMF was first developed in the Bell System in the United States, and became known under the trademark Touch-Tone for use in push-button telephones supplied to telephone customers, starting in 1963.  DTMF is standardized as ITU-T Recommendation Q.23. It is also known in the UK as MF4.",https://en.wikipedia.org/wiki/DTMF
Manning criteria,The Manning criteria are a diagnostic algorithm used in the diagnosis of irritable bowel syndrome (IBS).  The criteria consist of a list of questions the physician can ask the patient. The answers are used in a process to produce a diagnostic decision regarding whether the patient can be considered to have IBS.,https://en.wikipedia.org/wiki/Manning_Criteria
Algorithm selection,,https://en.wikipedia.org/wiki/Algorithm_selection
Sieve of Sundaram,"In mathematics, the sieve of Sundaram is a simple deterministic algorithm for finding all the prime numbers up to a specified integer. It was discovered by Indian  mathematician S. P. Sundaram in 1934.",https://en.wikipedia.org/wiki/Sieve_of_Sundaram
Overfitting,"In statistics, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably"". An overfitted model is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.:45",https://en.wikipedia.org/wiki/Overfitting
Bag-of-words model,"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.",https://en.wikipedia.org/wiki/Bag-of-words_model
Bongard problem,"A Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his 1967 book on pattern recognition. The objective is to spot the differences between the two sides. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov.",https://en.wikipedia.org/wiki/Bongard_problem
Variable rules analysis,"In linguistics, variable rules analysis is a set of statistical analysis methods commonly used in sociolinguistics and historical linguistics to describe patterns of variation between alternative forms in language use. It is also sometimes known as Varbrul analysis, after the name of a software package dedicated to carrying out the relevant statistical computations (Varbrul, from ""variable rule""). The method goes back to a theoretical approach developed by the sociolinguist William Labov in the late 1960s and early 1970s, and its mathematical implementation was developed by Henrietta Cedergren and David Sankoff in 1974.",https://en.wikipedia.org/wiki/Variable_rules_analysis
Edmonds–Karp algorithm,,https://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm
M-Theory (learning framework),"In Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.",https://en.wikipedia.org/wiki/M-Theory_(learning_framework)
Ritz method,The Ritz method is a direct method to find an approximate solution for boundary value problems.  The method is named after Walther Ritz.,https://en.wikipedia.org/wiki/Ritz_method
Archetypal analysis,"Archetypal analysis in statistics is an unsupervised learning method similar to cluster analysis and introduced by Adele Cutler and Leo Breiman in 1994. Rather than ""typical"" observations (cluster centers), it seeks extremal points in the multidimensional data, the ""archetypes"". The archetypes are convex combinations of observations chosen so that observations can be approximated by convex combinations of the archetypes.",https://en.wikipedia.org/wiki/Archetypal_analysis
Genetic representation,"In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. Genetic representation can encode appearance, behavior, physical qualities of individuals. Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.",https://en.wikipedia.org/wiki/Genetic_representation
Mixture model,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.",https://en.wikipedia.org/wiki/Mixture_model
Brute-force search,"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.",https://en.wikipedia.org/wiki/Brute-force_search
PBKDF2,"In cryptography, PBKDF1 and PBKDF2 (Password-Based Key Derivation Function 2) are key derivation functions with a sliding computational cost, used to reduce vulnerabilities to brute force attacks. ",https://en.wikipedia.org/wiki/PBKDF2
Painter's algorithm,"The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.",https://en.wikipedia.org/wiki/Painter%27s_algorithm
Demis Hassabis,,https://en.wikipedia.org/wiki/Demis_Hassabis
Pierre Baldi,Pierre Baldi is a chancellor's professor of computer science at University of California Irvine and the director of its Institute for Genomics and Bioinformatics.,https://en.wikipedia.org/wiki/Pierre_Baldi
Dynamic time warping,"In time series analysis, dynamic time warping (DTW) is one of the algorithms for measuring similarity between two temporal sequences, which may vary in speed.  For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.",https://en.wikipedia.org/wiki/Dynamic_time_warping
Entropy rate,"In the mathematical theory of probability, the entropy rate or source information rate of a stochastic process is, informally, the time density of the average information in a stochastic process.",https://en.wikipedia.org/wiki/Entropy_rate
Linear predictor function,"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".",https://en.wikipedia.org/wiki/Linear_predictor_function
Pietro Perona,Pietro Perona is the Allan E. Puckett Professor of Electrical Engineering and Computation and Neural Systems at the California Institute of Technology and director of the National Science Foundation Engineering Research Center in Neuromorphic Systems Engineering.He is known for his research in computer vision and is the director of the Caltech Computational Vision Group.,https://en.wikipedia.org/wiki/Pietro_Perona
Trial division,"Trial division is the most laborious but easiest to understand of the integer factorization algorithms.  The essential idea behind trial division tests to see if an integer n, the integer to be factored, can be divided by each number in turn that is less than n.  For example, for the integer n = 12, the only numbers that divide it are 1, 2, 3, 4, 6, 12.  Selecting only the largest powers of primes in this list gives that 12 = 3 × 4 = 3 × 22.",https://en.wikipedia.org/wiki/Trial_division
Key derivation function,,https://en.wikipedia.org/wiki/Key_derivation_function
Manifold regularization,"In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.",https://en.wikipedia.org/wiki/Manifold_regularization
Outline of artificial intelligence,The following outline is provided as an overview of and topical guide to artificial intelligence:,https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence
Growing self-organizing map,"A growing self-organizing map (GSOM) is a growing variant of a self-organizing map (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM.",https://en.wikipedia.org/wiki/Growing_self-organizing_map
Hungarian algorithm,"The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time and which anticipated later primal-dual methods. It was developed and published in 1955 by Harold Kuhn, who gave the name ""Hungarian method"" because the algorithm was largely based on the earlier works of two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry.",https://en.wikipedia.org/wiki/Hungarian_algorithm
Euclidean algorithm,"In mathematics, the Euclidean algorithm,[note 1] or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC).It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules,and is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.",https://en.wikipedia.org/wiki/Euclidean_algorithm
Population process,"In applied probability, a population process is a Markov chain in which the state of the chain is analogous to the number of individuals in a population (0, 1, 2, etc.), and changes to the state are analogous to the addition or removal of individuals from the population.  ",https://en.wikipedia.org/wiki/Population_process
Skill chaining,Skill chaining is a skill discovery method in continuous reinforcement learning.,https://en.wikipedia.org/wiki/Skill_chaining
Byte pair encoding,"Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data.  A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article ""A New Algorithm for Data Compression"" in the C Users Journal.",https://en.wikipedia.org/wiki/Byte_pair_encoding
Tom M. Mitchell,,https://en.wikipedia.org/wiki/Tom_M._Mitchell
Twofish,"In cryptography, Twofish is a symmetric key block cipher with a block size of 128 bits and key sizes up to 256 bits. It was one of the five finalists of the Advanced Encryption Standard contest, but it was not selected for standardization. Twofish is related to the earlier block cipher Blowfish.",https://en.wikipedia.org/wiki/Twofish
Silhouette (clustering),Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.,https://en.wikipedia.org/wiki/Silhouette_(clustering)
Digital Signature Algorithm,"The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures, based on the mathematical concept of modular exponentiation and the discrete logarithm problem. DSA is a variant of the Schnorr and ElGamal signature schemes.:486",https://en.wikipedia.org/wiki/Digital_Signature_Algorithm
Principal component regression,"In statistics, principal component regression (PCR) is a regression analysis technique that is based on principal component analysis (PCA). More specifically, PCR is used for estimating the unknown regression coefficients in a standard linear regression model.",https://en.wikipedia.org/wiki/Principal_component_regression
Karplus–Strong string synthesis,Karplus–Strong string synthesis is a method of physical modelling synthesis that loops a short waveform through a filtered delay line to simulate the sound of a hammered or plucked string or some types of percussion. ,https://en.wikipedia.org/wiki/Karplus-Strong_string_synthesis
Velvet assembler,"Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions. Velvet has also been implemented in commercial packages, such as Sequencher, Geneious, MacVector and BioNumerics.",https://en.wikipedia.org/wiki/Velvet_(algorithm)
Multiclass classification,"In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes. (Classifying instances into one of two classes is called binary classification.)",https://en.wikipedia.org/wiki/Multiclass_classification
Unique negative dimension,"Unique negative dimension (UND) is a complexity measure for the model of learning from positive examples.The unique negative dimension of a class C, we have ∩(D∖{c})∖c is nonempty.",https://en.wikipedia.org/wiki/Unique_negative_dimension
Fluentd,Fluentd is a cross platform open-source data collection software project originally developed at Treasure Data. It is written primarily in the Ruby programming language.,https://en.wikipedia.org/wiki/Fluentd
Iris flower data set,"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".",https://en.wikipedia.org/wiki/Iris_flower_data_set
Levenshtein distance,"In information theory, linguistics and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965.",https://en.wikipedia.org/wiki/Levenshtein_distance
AlexNet,"AlexNet is the name of a convolutional neural network (CNN), designed by Alex Krizhevsky, and published with Ilya Sutskever and Krizhevsky's doctoral advisor Geoffrey Hinton.",https://en.wikipedia.org/wiki/AlexNet
DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.",https://en.wikipedia.org/wiki/DBSCAN
Sukhotin's algorithm,"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in.",https://en.wikipedia.org/wiki/Sukhotin%27s_algorithm
A* search algorithm,"A* (pronounced ""A-star"") is a graph traversal and path search algorithm, which is often used in computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is its O(bd) space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.",https://en.wikipedia.org/wiki/A*_search_algorithm
Shannon–Fano–Elias coding,"In information theory, Shannon–Fano–Elias coding is a precursor to arithmetic coding, in which probabilities are used to determine codewords.",https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano%E2%80%93Elias_coding
Vladimir Vapnik,"Vladimir Naumovich Vapnik (Russian: Владимир Наумович Вапник; born 6 December 1936) is one of the main developers of the Vapnik–Chervonenkis theory of statistical learning, and the co-inventor of the support-vector machine method, and support-vector clustering algorithm.",https://en.wikipedia.org/wiki/Vladimir_Vapnik
Linear separability,"In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by hyperplane.",https://en.wikipedia.org/wiki/Linear_separability
Consensus clustering,"Consensus clustering is an important elaboration of traditional cluster analysis. Consensus clustering, also called cluster ensembles or aggregation of clustering (or partitions), refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Consensus clustering is thus the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. When cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete, even when the number of input clusterings is three. Consensus clustering for unsupervised learning is analogous to ensemble learning in supervised learning.",https://en.wikipedia.org/wiki/Consensus_clustering
Folding@home,"Folding@home (FAH or F@h) is a distributed computing project for disease research that simulates protein folding, computational drug design, and other types of molecular dynamics. The project uses the idle processing resources of hundreds of thousands of personal computers owned by volunteers who have installed the software on their systems.[citation needed] Its main purpose is to determine the mechanisms of protein folding, which is the process by which proteins reach their final three-dimensional structure, and to examine the causes of protein misfolding. This is of interest to medical research into Alzheimer's disease, Huntington's disease, and many forms of cancer, among other diseases. To a lesser extent, Folding@home also tries to predict a protein's final structure and determine how other molecules may interact with it, which has applications in drug design. Folding@home is developed and operated by the Pande Laboratory at Stanford University, under the direction of Prof. Vijay Pande, and is shared by various scientific institutions and research laboratories across the world.",https://en.wikipedia.org/wiki/Folding@home
Gramian matrix,"In linear algebra, the Gram matrix (a. k. a. Gramian matrix or Gramian) of a set of vectors v1,…,vn,\dots ,v_{n}} in an inner product space is the Hermitian matrix of inner products, whose entries are given by Gij=⟨vi,vj⟩,v_{j}\rangle }.",https://en.wikipedia.org/wiki/Gramian_matrix
Laplacian smoothing,"Laplacian smoothing is an algorithm to smooth a polygonal mesh. For each vertex in a mesh, a new position is chosen based on local information (such as the position of neighbors) and the vertex is moved there. In the case that a mesh is topologically a rectangular grid (that is, each internal vertex is connected to four neighbors) then this operation produces the Laplacian of the mesh.",https://en.wikipedia.org/wiki/Laplacian_smoothing
Nando de Freitas,,https://en.wikipedia.org/wiki/Nando_de_Freitas
Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.",https://en.wikipedia.org/wiki/Bias-variance_dilemma
Bondy's theorem,"In mathematics, Bondy's theorem is a bound on the number of elements needed to distinguish the sets in a family of sets from each other. It belongs to the field of combinatorics, and is named after John Adrian Bondy, who published it in 1972.",https://en.wikipedia.org/wiki/Bondy%27s_theorem
Lattice (group),"In geometry and group theory, a lattice in Rn, and which spans the real vector space Rn. In other words, for any basis of Rn, the subgroup of all linear combinations with integer coefficients of the basis vectors forms a lattice. A lattice may be viewed as a regular tiling of a space by a primitive cell.",https://en.wikipedia.org/wiki/Lattice_(group)
Dantzig–Wolfe decomposition,Dantzig–Wolfe decomposition is an algorithm for solving linear programming problems with special structure.  It was originally developed by George Dantzig and Philip Wolfe and initially published in 1960. Many texts on linear programming have sections dedicated to discussing this decomposition algorithm.,https://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition
Jump point search,"In computer science, jump point search (JPS) is an optimization to the A* search algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long ""jumps"" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.",https://en.wikipedia.org/wiki/Jump_point_search
Dynamic Bayesian network,"A Dynamic Bayesian Network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that at any point in time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1).  DBNs were developed by Paul Dagum in the early 1990s at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains.",https://en.wikipedia.org/wiki/Dynamic_Bayesian_network
Transposition table,"A transposition table is a cache of previously seen positions, and associated evaluations, in a game tree generated by a computer game playing program.  If a position recurs via a different sequence of moves, the value of the position is retrieved from the table, avoiding re-searching the game tree below that position. Transposition tables are primarily useful in perfect-information games (where the entire state of the game is known to all players at all times). The usage of transposition tables is essentially memoization applied to the tree search and is a form of dynamic programming.  ",https://en.wikipedia.org/wiki/Transposition_table
Chi-square automatic interaction detection,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing). The technique was developed in South Africa and was published in 1980 by Gordon V. Kass, who had completed a PhD thesis on this topic. CHAID can be used for prediction (in a similar fashion to regression analysis, this version of CHAID being originally known as XAID) as well as classification, and for detection of interaction between variables. CHAID is based on a formal extension of the United States' AID (Automatic Interaction Detection) and THAID (THeta Automatic Interaction Detection) procedures of the 1960s and 1970s, which in turn were extensions of earlier research, including that performed in the UK in the 1950s.",https://en.wikipedia.org/wiki/Chi-squared_Automatic_Interaction_Detection
Dynamic unobserved effects model,"A dynamic unobserved effects model is a statistical model used in econometrics. It is characterized by the influence of previous values of the dependent variable on its present value, and by the presence of unobservable explanatory variables.",https://en.wikipedia.org/wiki/Dynamic_unobserved_effects_model
B*,"In computer science, B* (pronounced ""B star"") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm.",https://en.wikipedia.org/wiki/B*
Dynamic programming,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.",https://en.wikipedia.org/wiki/Dynamic_Programming
Prim's algorithm,"In computer science, Prim's (also known as Jarník's) algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph.  This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.",https://en.wikipedia.org/wiki/Prim%27s_algorithm
Schreier–Sims algorithm,"The Schreier–Sims algorithm is an algorithm in computational group theory named after mathematicians Otto Schreier and Charles Sims.  Once performed, it allows a linear time computation of the order of a finite permutation group, group membership test (is a given permutation contained in a group?), and many other tasks.  The algorithm was introduced by Sims in 1970, based on Schreier's subgroup lemma.  The timing was subsequently improved by Donald Knuth in 1991.  Later, an even faster randomized version of the algorithm was developed.",https://en.wikipedia.org/wiki/Schreier%E2%80%93Sims_algorithm
Learning automaton,A learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used.,https://en.wikipedia.org/wiki/Learning_Automata
Multimodal learning,"The information in real world usually comes as different modalities. For example, images are usually associated with tags and text explanations; texts contain images to more clearly express the main idea of the article. Different modalities are characterized by very different statistical properties. For instance, images are usually represented as pixel intensities or outputs of feature extractors, while texts are represented as discrete word count vectors. Due to the distinct statistical properties of different information resources, it is very important to discover the relationship between different modalities. Multimodal learning is a good model to represent the joint representations of different modalities. The multimodal learning model is also capable to fill missing modality given the observed ones. The multimodal learning model combines two deep Boltzmann machines each corresponds to one modality. An additional hidden layer is placed on top of the two Boltzmann Machines to give the joint representation.",https://en.wikipedia.org/wiki/Multimodal_learning
SMA*,"SMA* or Simplified Memory Bounded A* is a shortest path algorithm based on the A* algorithm. The main advantage of SMA* is that it uses a bounded memory, while the A* algorithm might need exponential memory. All other characteristics of SMA* are inherited from A*.",https://en.wikipedia.org/wiki/SMA*
Cascading classifiers,"Cascading is a particular case of ensemble learning based on the concatenation of several classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade. Unlike voting or stacking ensembles, which are multiexpert systems, cascading is a multistage one.",https://en.wikipedia.org/wiki/Cascading_classifiers
Raft (computer science),"Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some additional features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions. It has a number of open-source reference implementations, with full-specification implementations in Go, C++, Java, and Scala. It is named after Reliable, Replicated, Redundant, And Fault-Tolerant.",https://en.wikipedia.org/wiki/Raft_(computer_science)
Chan's algorithm,"In computational geometry, Chan's algorithm, named after Timothy M. Chan, is an optimal output-sensitive algorithm to compute the convex hull of a set P points, in 2- or 3-dimensional space. The algorithm takes O(nlog⁡h) time, where h is the number of vertices of the output (the convex hull). In the planar case, the algorithm combines an O(nlog⁡n) algorithm (Graham scan, for example) with Jarvis march (O(nh)), in order to obtain an optimal O(nlog⁡h) time. Chan's algorithm is notable because it is much simpler than the Kirkpatrick–Seidel algorithm, and it naturally extends to 3-dimensional space. This paradigm has been independently developed by Frank Nielsen in his Ph.D. thesis.",https://en.wikipedia.org/wiki/Chan%27s_algorithm
Zobrist hashing,"Zobrist hashing (also referred to as Zobrist keys or Zobrist signatures ) is a hash function construction used in computer programs that play abstract board games, such as chess and Go, to implement transposition tables, a special kind of hash table that is indexed by a board position and used to avoid analyzing the same position more than once. Zobrist hashing is named for its inventor, Albert Lindsey Zobrist.  It has also been applied as a method for recognizing substitutional alloy configurations in simulations of crystalline materials.",https://en.wikipedia.org/wiki/Zobrist_hashing
Incremental decision tree,"An incremental decision tree algorithm is an online machine learning algorithm that outputs a decision tree. Many decision tree methods, such as C4.5, construct a tree using a complete dataset. Incremental decision tree methods allow an existing tree to be updated using only new individual data instances, without having to re-process past instances. This may be useful in situations where the entire dataset is not available when the tree is updated (i.e. the data was not stored), the original data set is too large to process or the characteristics of the data change over time.",https://en.wikipedia.org/wiki/Incremental_decision_tree
SimHash,"In computer science, SimHash is a technique for quickly estimating how similar two sets are. The algorithm is used by the Google Crawler to find near duplicate pages. It was created by Moses Charikar.",https://en.wikipedia.org/wiki/SimHash
Subset sum problem,"In computer science, the subset sum problem is an important decision problem in complexity theory and cryptography. There are several equivalent formulations of the problem. One of them is: given a set (or multiset) of integers, is there a non-empty subset whose sum is zero?  For example, given the set {−7,−3,−2,5,8}{\displaystyle \{-7,-3,-2,5,8\}}, the answer is yes because the subset {−3,−2,5}{\displaystyle \{-3,-2,5\}} sums to zero.  The problem is NP-complete, meaning roughly that while it is easy to confirm whether a proposed solution is valid, it may inherently be prohibitively difficult to determine in the first place whether any solution exists.",https://en.wikipedia.org/wiki/Subset_sum_problem
Category:CS1 maint: archived copy as title,This is a tracking category for CS1 citations that have |title=Archived copy or |title=Archive copy.,https://en.wikipedia.org/wiki/Category:CS1_maint:_archived_copy_as_title
Shading,Shading refers to the depiction of depth perception in 3D models (within the field of 3D computer graphics) or illustrations (in visual art) by varying the level of darkness.,https://en.wikipedia.org/wiki/Shading
ALOPEX,"ALOPEX (an acronym from ""ALgorithms Of Pattern EXtraction"") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.",https://en.wikipedia.org/wiki/ALOPEX
LPBoost,Linear Programming Boosting (LPBoost) is a supervised classifier from the boosting family of classifiers.  LPBoost maximizes a margin between training samples of different classes and hence also belongs to the class of margin-maximizing supervised classification algorithms.  Consider a classification function,https://en.wikipedia.org/wiki/LPBoost
Markov model,"In probability theory, a Markov model is a stochastic model used to model randomly changing systems. It is assumed that future states depend only on the current state, not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to exhibit the Markov property.",https://en.wikipedia.org/wiki/Markov_model
Richard O. Duda,,https://en.wikipedia.org/wiki/Richard_O._Duda
Rules extraction system family,The rules extraction system (RULES) family is a family of inductive learning that includes several covering algorithms. This family is used to build a predictive model based on given observation. It works based on the concept of separate-and-conquer to directly induce rules from a given training set and build its knowledge repository.,https://en.wikipedia.org/wiki/Rules_extraction_system_family
Euler method,"In mathematics and computational science, the Euler method (also called forward Euler method) is a first-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value. It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–1870).",https://en.wikipedia.org/wiki/Euler_method
Davies–Bouldin index,"The Davies–Bouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979) is a metric for evaluating clustering algorithms. This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.[citation needed]",https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index
ACORN (PRNG),"ACORN or ″Additive Congruential Random Number″  generators are a robust family of PRNGs (pseudorandom number generators) for sequences of uniformly distributed pseudo-random numbers, introduced in 1989 and still valid in 2019, thirty years later. ",https://en.wikipedia.org/wiki/ACORN_(PRNG)
Error detection and correction,"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.",https://en.wikipedia.org/wiki/Redundancy_check
Generalised Hough transform,"The generalized Hough transform (GHT), introduced by Dana H. Ballard in 1981, is the modification of the Hough transform using the principle of template matching. The Hough transform was initially developed to detect analytically defined shapes (e.g., line, circle, ellipse etc.). In these cases, we have knowledge of the shape and aim to find out its location and orientation in the image. This modification enables the Hough transform to be used to detect an arbitrary object described with its model.",https://en.wikipedia.org/wiki/Generalised_Hough_transform
Active learning (machine learning),"Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.",https://en.wikipedia.org/wiki/Active_learning_(machine_learning)
Shamir's Secret Sharing,"Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir. It is a form of secret sharing, where a secret is divided into parts, giving each participant its own unique part.",https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing
Low-rank matrix approximations,,https://en.wikipedia.org/wiki/Low-rank_matrix_approximations
Riemann zeta function,"The Riemann zeta function or Euler–Riemann zeta function, ζ(s), is a function of a complex variable s that analytically continues the sum of the Dirichlet series",https://en.wikipedia.org/wiki/Riemann_zeta_function
Plate notation,"In Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model.  Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate.  The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.",https://en.wikipedia.org/wiki/Plate_notation
JOONE,JOONE (Java Object Oriented Neural Engine) is a component based neural network framework built in Java.,https://en.wikipedia.org/wiki/JOONE
Learning classifier system,"Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling, classification, data mining, regression, function approximation, or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.",https://en.wikipedia.org/wiki/Learning_classifier_system
Divide-and-conquer algorithm,"In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.",https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm
Sethi–Ullman algorithm,"In computer science, the Sethi–Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible.",https://en.wikipedia.org/wiki/Sethi-Ullman_algorithm
Marching cubes,"Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called a voxel). The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces.  An analogous two-dimensional method is called the marching squares algorithm.",https://en.wikipedia.org/wiki/Marching_cubes
Group method of data handling,Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models.,https://en.wikipedia.org/wiki/Group_method_of_data_handling
Restricted Boltzmann machine,A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.,https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine
Chromosome (genetic algorithm),"In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.",https://en.wikipedia.org/wiki/Chromosome_(genetic_algorithm)
Stochastic diffusion search,"Stochastic diffusion search (SDS) was first described in 1989 as a population-based, pattern-matching algorithm [Bishop, 1989]. It belongs to a family of swarm intelligence and naturally inspired search and optimisation algorithms which includes ant colony optimization, particle swarm optimization and genetic algorithms; as such SDS was the first Swarm Intelligence metaheuristic. Unlike stigmergetic communication employed in ant colony optimization, which is based on modification of the physical properties of a simulated environment, SDS uses a form of direct (one-to-one) communication between the agents similar to the tandem calling mechanism employed by one species of ants, Leptothorax acervorum.",https://en.wikipedia.org/wiki/Stochastic_diffusion_search
Nonlinear dimensionality reduction,"High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be  difficult to interpret. One approach to simplification is to assume that the data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.",https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction
Sufficient dimension reduction,"In statistics, sufficient dimension reduction (SDR) is a paradigm for analyzing data that combines the ideas of dimension reduction with the concept of sufficiency.",https://en.wikipedia.org/wiki/Sufficient_dimension_reduction
Spectral layout,"Spectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.",https://en.wikipedia.org/wiki/Spectral_layout
Polynomial,"In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate, x, is x2 − 4x + 7. An example in three variables is x3 + 2xyz2 − yz + 1.",https://en.wikipedia.org/wiki/Polynomial
Hidden-surface determination,"In 3D computer graphics, shown-surface determination (also known as hidden-surface removal (HSR), occlusion culling (OC) or visible-surface determination (VSD)) is the process used to determine which surfaces and parts of surfaces are not visible from a certain viewpoint. A hidden-surface determination algorithm is a solution to the visibility problem, which was one of the first major problems in the field of 3D computer graphics. The process of hidden-surface determination is sometimes called hiding, and such an algorithm is sometimes called a hider. The analogue for line rendering is hidden-line removal. Hidden-surface determination is necessary to render an image correctly, so that one may not view features hidden behind the model itself, allowing only the naturally viewable portion of the graphic to be visible.",https://en.wikipedia.org/wiki/Hidden_surface_determination
Multilinear subspace learning,"Multilinear subspace learning is an approach to dimensionality reduction.Dimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).",https://en.wikipedia.org/wiki/Multilinear_subspace_learning
Ben Goertzel,Ben Goertzel is an artificial intelligence researcher.,https://en.wikipedia.org/wiki/Ben_Goertzel
Clustering high-dimensional data,"Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional spaces of data are often encountered in areas such as medicine, where DNA microarray technology can produce many measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.",https://en.wikipedia.org/wiki/Clustering_high-dimensional_data
CIML community portal,"The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative.  Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning.  This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site.",https://en.wikipedia.org/wiki/CIML_community_portal
Tomasulo algorithm,Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.,https://en.wikipedia.org/wiki/Tomasulo_algorithm
VEGAS algorithm,"The VEGAS algorithm, due to G. Peter Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.",https://en.wikipedia.org/wiki/VEGAS_algorithm
Knowledge Engineering and Machine Learning Group,,https://en.wikipedia.org/wiki/Knowledge_Engineering_and_Machine_Learning_Group
Arithmetical hierarchy,"In mathematical logic, the arithmetical hierarchy, arithmetic hierarchy or Kleene–Mostowski hierarchy classifies certain sets based on the complexity of formulas that define them.  Any set that receives a classification is called arithmetical.",https://en.wikipedia.org/wiki/Arithmetical_hierarchy
Benson's algorithm,"Benson's algorithm, named after Harold Benson, is a method for solving multi-objective linear programming problems and vector linear programs.  This works by finding the ""efficient extreme points in the outcome set"".  The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes.",https://en.wikipedia.org/wiki/Benson%27s_algorithm
Maximum-entropy Markov model,"In machine learning, a maximum-entropy Markov model (MEMM), or conditional Markov model (CMM), is a graphical model for sequence labeling that combines features of hidden Markov models (HMMs) and maximum entropy (MaxEnt) models. An MEMM is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learnt are connected in a Markov chain rather than being conditionally independent of each other. MEMMs find applications in natural language processing, specifically in part-of-speech tagging and information extraction.",https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model
Decision tree learning,"Decision tree learning is one of the predictive modeling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.",https://en.wikipedia.org/wiki/Decision_tree_learning
Baby-step giant-step,"In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm for computing the discrete logarithm or order of an element in a finite abelian group due to Daniel Shanks. The discrete log problem is of fundamental importance to the area of public key cryptography.",https://en.wikipedia.org/wiki/Baby-step_giant-step
Golden-section search,"The golden-section search is a technique for finding an extremum (minimum or maximum) of a function inside a specified interval. For a strictly unimodal function with an extremum inside the interval, it will find that extremum, while for an interval containing multiple extrema (possibly including the interval boundaries), it will converge to one of them. If the only extremum on the interval is on a boundary of the interval, it will converge to that boundary point. The method operates by successively narrowing the range of values on the specified interval, which makes it relatively slow, but very robust. The technique derives its name from the fact that the algorithm maintains the function values for four points whose three interval widths are in the ratio 2-φ:2φ-3:2-φ where φ is the golden ratio. These ratios are maintained for each iteration and are maximally efficient. Excepting boundary points, when searching for a minimum, the central point is always less than or equal to the outer points, assuring that a minimum is contained between the outer points. The converse is true when searching for a maximum. The algorithm is the limit of Fibonacci search (also described below) for many function evaluations. Fibonacci search and golden-section search were discovered by Kiefer (1953) (see also Avriel and Wilde (1966)).",https://en.wikipedia.org/wiki/Golden_section_search
Cobweb (clustering),"COBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.",https://en.wikipedia.org/wiki/Cobweb_(clustering)
TensorFlow,,https://en.wikipedia.org/wiki/DistBelief
Tiger (hash function),"In cryptography, Tiger is a cryptographic hash function designed by Ross Anderson and Eli Biham in 1995 for efficiency on 64-bit platforms. The size of a Tiger hash value is 192 bits. Truncated versions (known as Tiger/128 and Tiger/160) can be used for compatibility with protocols assuming a particular hash size. Unlike the SHA-2 family, no distinguishing initialization values are defined; they are simply prefixes of the full Tiger/192 hash value.",https://en.wikipedia.org/wiki/Tiger_(hash)
Yarrow algorithm,"The Yarrow algorithm is a family of cryptographic pseudorandom number generators (CPRNG) devised by John Kelsey, Bruce Schneier, and Niels Ferguson and published in 1999. The Yarrow algorithm is explicitly unpatented, royalty-free, and open source; no license is required to use it. Yarrow is incorporated in iOS and macOS for their /dev/random devices, and was in FreeBSD (where it is superseded by Fortuna).",https://en.wikipedia.org/wiki/Yarrow_algorithm
Category:CS1 maint: multiple names: authors list,"This is a hidden tracking category for CS1 citations that use |author=, or its aliases.",https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list
Top-nodes algorithm,"The top-nodes algorithm is an algorithm for managing a resource reservation calendar. The algorithm has been first published in 2003, and has been improved in 2009. It is used when a resource is shared among lots of users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).",https://en.wikipedia.org/wiki/Top-nodes_algorithm
Structured prediction,"Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.",https://en.wikipedia.org/wiki/Structured_prediction
Semantic analysis (machine learning),"In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.",https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)
Hierarchical clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters.",https://en.wikipedia.org/wiki/Hierarchical_Clustering
Trax Retail,"Trax is a technology company headquartered in Singapore, with offices throughout the Asia-Pacific, Europe, the Middle East, North America, and South America. Founded in 2010 by Joel Bar-El and Dror Feldheim, Trax has more than 150 customers in the retail and FMCG industries, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Customers use the company’s computer vision technology to collect, measure, and analyze what’s happening on physical store shelves. Trax’s services are available in 45 markets. ",https://en.wikipedia.org/wiki/Trax_Image_Recognition
Cannon's algorithm,"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.",https://en.wikipedia.org/wiki/Cannon%27s_algorithm
Theano (software),,https://en.wikipedia.org/wiki/Theano_(software)
Golomb coding,"Golomb coding is a lossless data compression method using a family of data compression codes invented by Solomon W. Golomb in the 1960s.  Alphabets following a geometric distribution will have a Golomb code as an optimal prefix code, making Golomb coding highly suitable for situations in which the occurrence of small values in the input stream is significantly more likely than large values.",https://en.wikipedia.org/wiki/Rice_coding
Random indexing,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately.",https://en.wikipedia.org/wiki/Random_indexing
Brian D. Ripley,,https://en.wikipedia.org/wiki/Brian_D._Ripley
Optimal substructure,"In computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.",https://en.wikipedia.org/wiki/Optimal_substructure
Delta encoding,"Delta encoding is a way of storing or transmitting data in the form of differences (deltas) between sequential data rather than complete files; more generally this is known as data differencing. Delta encoding is sometimes called delta compression, particularly where archival histories of changes are required (e.g., in revision control software).",https://en.wikipedia.org/wiki/Delta_encoding
Petrick's method,"In Boolean algebra, Petrick's method (also known as the branch-and-bound method) is a technique described by Stanley R. Petrick (1931–2006) in 1956 for determining all minimum sum-of-products solutions from a prime implicant chart. Petrick's method is very tedious for large charts, but it is easy to implement on a computer.",https://en.wikipedia.org/wiki/Petrick%27s_method
Runge's phenomenon,"In the mathematical field of numerical analysis, Runge's phenomenon (German: ) is a problem of oscillation at the edges of an interval that occurs when using polynomial interpolation with polynomials of high degree over a set of equispaced interpolation points. It was discovered by Carl David Tolmé Runge (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions.The discovery was important because it shows that going to higher degrees does not always improve accuracy. The phenomenon is similar to the Gibbs phenomenon in Fourier series approximations.",https://en.wikipedia.org/wiki/Runge%27s_phenomenon
Slowsort,"Slowsort is a sorting algorithm. It is of humorous nature and not useful. It's based on the principle of multiply and surrender, a tongue-in-cheek joke of divide and conquer. It was published in 1986 by Andrei Broder and Jorge Stolfi in their paper Pessimal Algorithms and Simplexity Analysis (a parody of optimal algorithms and complexity analysis).",https://en.wikipedia.org/wiki/Slowsort
Time complexity,"In computer science, the time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.",https://en.wikipedia.org/wiki/Linear-time
Linear genetic programming,Linear genetic programming (LGP) is a particular subset of genetic programming wherein computer programs in a population are represented as a sequence of instructions from imperative programming language or machine language. The graph-based data flow that results from a multiple usage of register contents and the existence of structurally noneffective code (introns) are two main differences of this genetic representation from the more common tree-based genetic programming (TGP) variant.,https://en.wikipedia.org/wiki/Linear_genetic_programming
Odd–even sort,"In computing, an odd–even sort or odd–even transposition sort (also known as brick sort[self-published source]) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections.  It is a comparison sort related to bubble sort, with which it shares many characteristics.  It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched.  The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.",https://en.wikipedia.org/wiki/Odd%E2%80%93even_sort
Post-quantum cryptography,"Post-quantum cryptography (sometimes referred to as quantum-proof, quantum-safe or quantum-resistant) refers to cryptographic algorithms (usually public-key algorithms) that are thought to be secure against an attack by a quantum computer. As of 2019, this is not true for the most popular public-key algorithms, which can be efficiently broken by a sufficiently strong quantum computer. The problem with currently popular algorithms is that their security relies on one of three hard mathematical problems:  the integer factorization problem, the discrete logarithm problem or the elliptic-curve discrete logarithm problem. All of these problems can be easily solved on a sufficiently powerful quantum computer running Shor's algorithm. Even though current, publicly known, experimental quantum computers lack processing power to break any real cryptographic algorithm, many cryptographers are designing new algorithms to prepare for a time when quantum computing becomes a threat. This work has gained greater attention from academics and industry through the PQCrypto conference series since 2006 and more recently by several workshops on Quantum Safe Cryptography hosted by the European Telecommunications Standards Institute (ETSI) and the Institute for Quantum Computing.",https://en.wikipedia.org/wiki/Post-quantum_cryptography
Domain adaptation,"Domain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources.Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.",https://en.wikipedia.org/wiki/Domain_adaptation
Shellsort,"Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort). The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. Starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange. Donald Shell published the first version of this sort in 1959. The running time of Shellsort is heavily dependent on the gap sequence it uses. For many practical variants, determining their time complexity remains an open problem.",https://en.wikipedia.org/wiki/Shellsort
Embedded Zerotrees of Wavelet transforms,"Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a subband transform (such as the wavelet transform)will be zero, or very close to zero. This occurs because ""real world"" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.",https://en.wikipedia.org/wiki/Embedded_Zerotree_Wavelet
Machine learning,"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to perform the task.:2 Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.",https://en.wikipedia.org/wiki/Statistical_learning
Evaluation of binary classifiers,Sources: Fawcett (2006),https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers
Ambient occlusion,"In computer graphics, ambient occlusion is a shading and rendering technique used to calculate how exposed each point in a scene is to ambient lighting. For example, the interior of a tube is typically more occluded (and hence darker) than the exposed outer surfaces, and the deeper you go inside the tube, the more occluded (and darker) the lighting becomes. Ambient occlusion can be seen as an accessibility value that is calculated for each surface point. In scenes with open sky this is done by estimating the amount of visible sky for each point, while in indoor environments only objects within a certain radius are taken into account and the walls are assumed to be the origin of the ambient light. The result is a diffuse, non-directional shading effect that casts no clear shadows but that darkens enclosed and sheltered areas and can affect the rendered image's overall tone. It is often used as a post-processing effect.",https://en.wikipedia.org/wiki/Ambient_occlusion
Snapshot algorithm,"A snapshot algorithm is used to create a consistent snapshot of the global state of a distributed system. Due to the lack of globally shared memory and a global clock, this isn't trivially possible.",https://en.wikipedia.org/wiki/Snapshot_algorithm
Image compression,"Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.",https://en.wikipedia.org/wiki/Image_compression
Geoffrey Hinton,,https://en.wikipedia.org/wiki/Geoffrey_Hinton
Huffman coding,,https://en.wikipedia.org/wiki/Huffman_coding
Rprop,"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.",https://en.wikipedia.org/wiki/Rprop
Feature selection,"In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.",https://en.wikipedia.org/wiki/Feature_selection
Cleverbot,,https://en.wikipedia.org/wiki/Cleverbot
Backward Euler method,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. It is similar to the (standard) Euler method, but differs in that it is an implicit method. The backward Euler method has error of order one in time.",https://en.wikipedia.org/wiki/Backward_Euler_method
Heuristic,"A heuristic technique (/hjʊəˈrɪstɪk/; Ancient Greek: εὑρίσκω, ""find"" or ""discover""), or a heuristic for short, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect or rational, but which is nevertheless sufficient for reaching an immediate, short-term goal. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.:94 Examples that employ heuristics include using trial and error, a rule of thumb, an educated guess, an intuitive judgment, a guesstimate, profiling, or common sense.",https://en.wikipedia.org/wiki/Heuristic
Manifold alignment,"Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.",https://en.wikipedia.org/wiki/Manifold_alignment
Extended Euclidean algorithm,"In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, and computes, in addition to the greatest common divisor of integers a and b, also  the coefficients of Bézout's identity, which are integers x and y such that",https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm
Wolfram Mathematica,"Wolfram Mathematica (usually termed Mathematica) is a modern technical computing system spanning most areas of technical computing — including neural networks, machine learning, image processing, geometry, data science, visualizations, and others. The system is used in many technical, scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica.",https://en.wikipedia.org/wiki/Wolfram_Mathematica
Stephen Wolfram,,https://en.wikipedia.org/wiki/Stephen_Wolfram
Data compression,"In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.",https://en.wikipedia.org/wiki/Video_compression
Anthony Levandowski,,https://en.wikipedia.org/wiki/Anthony_Levandowski
Hamiltonian Monte Carlo,"In computational physics and statistics, the Hamiltonian Monte Carlo algorithm (also known as hybrid Monte Carlo), is a Markov chain Monte Carlo method for obtaining a sequence of random samples which converge to being distributed according to a target probability distribution for which direct sampling is difficult. This sequence can be used to estimate integrals with respect to the target distribution (expected values).",https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo
AKS primality test,"The AKS primality test (also known as Agrawal–Kayal–Saxena primality test and cyclotomic AKS test) is a deterministic primality-proving algorithm created and published by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in an article titled ""PRIMES is in P"". The algorithm was the first that can provably determine whether any given number is prime or composite within polynomial time, without relying on the generalized Riemann hypothesis, or any mathematical conjecture. The proof is also notable for not relying on the field of analysis. The authors received the 2006 Gödel Prize and the 2006 Fulkerson Prize for this work. ",https://en.wikipedia.org/wiki/AKS_primality_test
Freivalds' algorithm,"Freivalds' algorithm (named after Rūsiņš Mārtiņš Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three n × n matrices A, B, and C, a general problem is to verify whether A×B=C. A naïve algorithm would compute the product A×B. However, the best known matrix multiplication algorithm runs in O(n2.3729) time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n2)with high probability. In O(kn2).",https://en.wikipedia.org/wiki/Freivalds%27_algorithm
Sweep and prune,"In physical simulations, sweep and prune is a broad phase algorithm used during collision detection to limit the number of pairs of solids that need to be checked for collision, i.e. intersection. This is achieved by sorting the starts (lower bound) and ends (upper bound) of the bounding volume of each solid along a number of arbitrary axes. As the solids move, their starts and ends may overlap. When the bounding volumes of two solids overlap in all axes they are flagged to be tested by more precise and time-consuming algorithms.",https://en.wikipedia.org/wiki/Sweep_and_prune
Fast-and-frugal trees,"In the study of decision-making, including the disciplines of psychology, artificial intelligence, and management science, a fast-and-frugal tree is a type of classification tree or decision tree. As shown in Figure 1--which will be explained in detail later--fast-and-frugal trees are simple graphical structures that ask one question at a time. The goal is to classify an object (in Figure 1: a patient suspected of heart disease) into a category for the purpose of making a decision (in Figure 1 there are two possibilities, patient assigned to a regular nursing bed or to emergency care). Unlike other classification and decision trees, such as Leo Breiman's CART, fast-and-frugal trees have been defined to be intentionally simple, both in their construction as well as their execution, and operate speedily with little information. For example, the tree of Figure 1 only asks from one to maximum three questions.      ",https://en.wikipedia.org/wiki/Fast-and-frugal_trees
General Architecture for Text Engineering,"General Architecture for Text Engineering or GATE is a Java suite of tools originally developed at the University of Sheffield beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many natural language processing tasks, including information extraction in many languages.",https://en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering
Concept class,A concept over a domain X is a total Boolean function over X. A concept class is a class of concepts. Concept class is a subject of computational learning theory.,https://en.wikipedia.org/wiki/Concept_class
Sieve of Atkin,"In mathematics, the sieve of Atkin is a modern algorithm for finding all prime numbers up to a specified integer.  Compared with the ancient sieve of Eratosthenes, which marks off multiples of  primes, the sieve of Atkin does some preliminary work and then marks off multiples of squares of primes, thus achieving a better theoretical asymptotic complexity. It was created in 2003 by A. O. L. Atkin and Daniel J. Bernstein.",https://en.wikipedia.org/wiki/Sieve_of_Atkin
Association rule learning,Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.,https://en.wikipedia.org/wiki/One-attribute_rule
Jacek M. Zurada,"Jacek M. Zurada serves as a Professor of Electrical and Computer Engineering Department at the University of Louisville, Kentucky. He has held visiting appointments at Princeton, Northeastern, Auburn, and at overseas universities in Australia, Chile, China, France, Germany, Hong Kong, Italy, Japan, Poland, Singapore, Spain, and South Africa. He is a Life Fellow of IEEE.",https://en.wikipedia.org/wiki/Jacek_M._Zurada
Interpolation search,"Interpolation search is an algorithm for searching for a key in an array that has been ordered by numerical values assigned to the keys (key values). It was first described by W. W. Peterson in 1957. Interpolation search resembles the method by which people search a telephone directory for a name (the key value by which the book's entries are ordered): in each step the algorithm calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.",https://en.wikipedia.org/wiki/Interpolation_search
Vicarious (company),"Vicarious is an artificial intelligence company based in the San Francisco Bay Area, California. They are using the theorized computational principles of the brain to build software that can think and learn like a human.",https://en.wikipedia.org/wiki/Vicarious_(company)
Geographical cluster,"A geographical cluster is a localised anomaly, usually an excess of something given the distribution or variation of something else. Often it is considered as an incidence rate that is unusual in that there is more of some variable than might be expected. Examples would include: a local excess disease rate, a crime hot spot, areas of high unemployment, accident blackspots, unusually high positive residuals from a model, high concentrations of flora or fauna, physical features or events like earthquake epicenters etc...[citation needed]",https://en.wikipedia.org/wiki/Geographical_cluster
Neville's algorithm,"In mathematics, Neville's algorithm is an algorithm used for polynomial interpolation that was derived by the mathematician Eric Harold Neville[citation needed]. Given n + 1 points, there is a unique polynomial of degree ≤ n which goes through the given points. Neville's algorithm evaluates this polynomial.",https://en.wikipedia.org/wiki/Neville%27s_algorithm
UIMA,"UIMA (/juˈiːmə/ yoo-EE-mə), short for Unstructured Information Management Architecture, is an OASIS standard  for content analytics, originally developed at IBM.  It provides a component software architecture for the development, discovery, composition, and deployment of multi-modal analytics for the analysis of unstructured information and integration with search technologies.",https://en.wikipedia.org/wiki/UIMA
EdDSA,"In public-key cryptography, Edwards-curve Digital Signature Algorithm (EdDSA) is a digital signature scheme using a variant of Schnorr signature based on twisted Edwards curves.It is designed to be faster than existing digital signature schemes without sacrificing security. It was developed by a team including Daniel J. Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, and Bo-Yin Yang.The reference implementation is public domain software.",https://en.wikipedia.org/wiki/EdDSA
IRCF360,,https://en.wikipedia.org/wiki/IRCF360
Cluster-weighted modeling,"In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.",https://en.wikipedia.org/wiki/Cluster-weighted_modeling
Computer vision,"Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.",https://en.wikipedia.org/wiki/Computer_Vision
Peterson's algorithm,"Peterson's algorithm (or Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two or more processes to share a single-use resource without conflict, using only shared memory for communication. It was formulated by Gary L. Peterson in 1981. While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two.",https://en.wikipedia.org/wiki/Peterson%27s_algorithm
Maximum likelihood estimation,"In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.",https://en.wikipedia.org/wiki/Maximum_likelihood
Polynomial kernel,"In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.",https://en.wikipedia.org/wiki/Polynomial_kernel
Klaus-Robert Müller,Shun'ichi Amari,https://en.wikipedia.org/wiki/Klaus-Robert_M%C3%BCller
Raymond Cattell,,https://en.wikipedia.org/wiki/Raymond_Cattell
Stress majorization,"Stress majorization is an optimization strategy used in multidimensional scaling (MDS) where, for a set of n m-dimensional data items, a configuration X of n points in r(<<m)-dimensional space is sought that minimizes the so-called stress function σ(X).  Usually r is 2 or 3, i.e. the (n x r) matrix X lists points in 2- or 3-dimensional Euclidean space so that the result may be visualised (i.e. an MDS plot).  The function σ-dimensional) distances and actual distances in r-dimensional space.",https://en.wikipedia.org/wiki/Stress_majorization
Uniform convergence in probability,"Uniform convergence in probability is a form of convergence in probability in statistical asymptotic theory and probability theory. It means that, under certain conditions, the empirical frequencies of all events in a certain event-family converge to their theoretical probabilities.  Uniform convergence in probability has applications to statistics as well as machine learning as part of statistical learning theory.",https://en.wikipedia.org/wiki/Uniform_convergence_in_probability
Inductive logic programming,"Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.",https://en.wikipedia.org/wiki/Inductive_logic_programming
Mixture of experts,"Mixture of experts refers to a machine learning technique where multiple experts (learners) are used to divide the problem space into homogeneous regions. An example from the computer vision domain is combining a neural network model for human detection with another for pose estimation. If the output is conditioned on multiple levels of probabilistic gating functions, the mixture is called a hierarchical mixture of experts.",https://en.wikipedia.org/wiki/Mixture_of_experts
Vapnik–Chervonenkis theory,"Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.",https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory
Nearest-neighbor interpolation,"Nearest-neighbor interpolation (also known as proximal interpolation or, in some contexts, point sampling) is a simple method of multivariate interpolation in one or more dimensions.",https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
Radix sort,"In computer science, radix sort is a non-comparative sorting algorithm. It avoids comparison by creating and distributing elements into buckets according to their radix. For elements with more than one significant digit, this bucketing process is repeated for each digit, while preserving the ordering of the prior step, until all digits have been considered. For this reason, radix sort has also been called bucket sort and digital sort.",https://en.wikipedia.org/wiki/Radix_sort
Vector quantization,"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression.  It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.",https://en.wikipedia.org/wiki/Vector_Quantization
Evolving classification function,"Evolving classification functions (ECF), evolving classifier functions or evolving classifiers are used for classifying and clustering in the field of machine learning and artificial intelligence, typically employed for data stream mining tasks in dynamic and changing environments.",https://en.wikipedia.org/wiki/Evolving_classification_function
Fractal compression,"Fractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image.[citation needed] Fractal algorithms convert these parts into mathematical data called ""fractal codes"" which are used to recreate the encoded image.",https://en.wikipedia.org/wiki/Fractal_compression
Feature extraction,"In machine learning, pattern recognition and in image processing, feature extraction  starts from an initial set of  measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.",https://en.wikipedia.org/wiki/Feature_extraction
Deep learning,"Deep learning  (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.",https://en.wikipedia.org/wiki/Stacked_Auto-Encoders
Ayanna Howard,"Ayanna MacCalla Howard (born January 24, 1972) is an American roboticist and the School Chair for Interactive Computing, Georgia Institute of Technology. She is also the Linda J. and Mark C. Smith Endowed Chair in Bioengineering in the School of Electrical and Computer Engineering, and the director of the Human-Automation Systems (HumAnS) Lab. Currently, she is the Chair of the School of Interactive Computing in the Georgia Tech College of Computing.",https://en.wikipedia.org/wiki/Ayanna_Howard
C4.5 algorithm,"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. In 2011, authors of the Weka machine learning software described the C4.5 algorithm as ""a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date"".",https://en.wikipedia.org/wiki/C4.5_algorithm
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/Goldschmidt_division
Kaggle,"Kaggle, a subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges. ",https://en.wikipedia.org/wiki/Kaggle
Mutation (genetic algorithm),"Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to a better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.",https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)
Causal Markov condition,"The Markov condition, sometimes called the Markov assumption, is an assumption made in Bayesian probability theory, that every node in a Bayesian network is conditionally independent of its nondescendents, given its parents. Stated loosely, it is assumed that a node has no bearing on nodes which do not descend from it. This is equivalent to stating that a node is conditionally independent of the entire network, given its Markov blanket.",https://en.wikipedia.org/wiki/Causal_Markov_condition
Lenstra elliptic-curve factorization,"The Lenstra elliptic-curve factorization or the elliptic-curve factorization method (ECM) is a fast, sub-exponential running time, algorithm for integer factorization, which employs elliptic curves.  For general-purpose factoring, ECM is the third-fastest known factoring method.  The second-fastest is the multiple polynomial quadratic sieve, and the fastest is the general number field sieve. The Lenstra elliptic-curve factorization is named after Hendrik Lenstra.",https://en.wikipedia.org/wiki/Lenstra_elliptic_curve_factorization
Mallet (software project),"MALLET is a Java ""Machine Learning for Language Toolkit"".",https://en.wikipedia.org/wiki/Mallet_(software_project)
Yamartino method,The Yamartino method is an algorithm for calculating an approximation of wind direction during a single pass through the incoming data.,https://en.wikipedia.org/wiki/Yamartino_method
Assignment problem,The assignment problem is a fundamental combinatorial optimization problem.,https://en.wikipedia.org/wiki/Assignment_problem
Key exchange,"Key exchange (also key establishment) is a method in cryptography by which cryptographic keys are exchanged between two parties, allowing use of a cryptographic algorithm.",https://en.wikipedia.org/wiki/Key_exchange
Winnow (algorithm),"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples.  It is very similar to the perceptron algorithm.  However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name winnow). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative.  The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.",https://en.wikipedia.org/wiki/Winnow_algorithm
Pollard's rho algorithm for logarithms,"Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.",https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm_for_logarithms
Richardson–Lucy deconvolution,"The Richardson–Lucy algorithm, also known as Lucy–Richardson deconvolution, is an iterative procedure for recovering an underlying image that has been blurred by a known point spread function. It was named after William Richardson and Leon Lucy, who described it independently.",https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution
Rayid Ghani,"Rayid Ghani was the Director of the Center for Data Science and Public Policy, Research Associate Professor in the Department of Computer Science, and a Senior Fellow at the Harris School of Public Policy at the  University of Chicago. He was also the co-founder of Edgeflip, an analytics startup that grew out of the Obama 2012 Campaign, focused on social media products for non-profits, advocacy groups, and charities. Recently, it was announced that he will be leaving the University of Chicago and joining Carnegie Mellon University's School of Computer Science and Heinz College of Information Systems and Public Policy. ",https://en.wikipedia.org/wiki/Rayid_Ghani
Algorithmic learning theory,Algorithmic learning theory is a mathematical framework for analyzing machine learning problems and algorithms. Synonyms include formal learning theory and algorithmic inductive inference. Algorithmic learning theory is different from statistical learning theory in that it does not make use of statistical assumptions and analysis. Both algorithmic and statistical learning theory are concerned with machine learning and can thus be viewed as branches of computational learning theory.,https://en.wikipedia.org/wiki/Algorithmic_learning_theory
Structured support vector machine,"The structured support vector machine is a machine learning algorithm that generalizes the Support Vector Machine (SVM) classifier.  Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.",https://en.wikipedia.org/wiki/Structured_support_vector_machine
Spline interpolation,"In the mathematical field of numerical analysis, spline interpolation is a form of interpolation where the interpolant is a special type of piecewise polynomial called a spline. Spline interpolation is often preferred over polynomial interpolation because the interpolation error can be made small even when using low degree polynomials for the spline. Spline interpolation avoids the problem of Runge's phenomenon, in which oscillation can occur between points when interpolating using high degree polynomials.",https://en.wikipedia.org/wiki/Spline_interpolation
Linear search,"In computer science, a linear search or sequential search is a method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched.",https://en.wikipedia.org/wiki/Linear_search
Genetic Algorithm for Rule Set Production,"Genetic Algorithm for Rule Set Production (GARP) is a computer program based on genetic algorithm that creates ecological niche models for species. The generated models describe environmental conditions (precipitation, temperatures, elevation, etc.) under which the species should be able to maintain populations. As input, local observations of species and related environmental parameters are used which describe potential limits of the species' capabilities to survive. Such environmental parameters are commonly stored in geographical information systems. A GARP model is a random set of mathematical rules which can be read as limiting environmental conditions. Each rule is considered as a gene; the set of genes is combined in random ways to further generate many possible models describing the potential of the species to occur.",https://en.wikipedia.org/wiki/Genetic_Algorithm_for_Rule_Set_Production
Gated recurrent unit,"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.",https://en.wikipedia.org/wiki/Gated_recurrent_unit
Pachinko allocation,"In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents.  The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive powerthan latent Dirichlet allocation.  While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics.  Themodel is named for pachinko machines—a game popular in Japan, in which metal balls bounce down arounda complex collection of pins until they land in variousbins at the bottom.",https://en.wikipedia.org/wiki/Pachinko_allocation
Christopher G. Atkeson,"Christopher Granger Atkeson (born 1959) is an American roboticist and a Professor at the Robotics Institute and Human-Computer Interaction Institute at Carnegie Mellon University (CMU). Atkeson is known for his work in humanoid robots, soft robotics, and machine learning, most notably on locally weighted learning.",https://en.wikipedia.org/wiki/Christopher_G._Atkeson
Duality (mathematics),"In mathematics, a duality translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of A is B, then the dual of B is A. Such involutions sometimes have fixed points, so that the dual of A is A itself. For example, Desargues' theorem is self-dual in this sense under the standard duality in projective geometry.",https://en.wikipedia.org/wiki/Duality_(mathematics)
Robinson–Schensted correspondence,"In mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and  a further generalization to pictures by Zelevinsky.",https://en.wikipedia.org/wiki/Schensted_algorithm
CiteSeerX,"CiteSeerx (originally called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science. CiteSeer is considered as a predecessor of academic search tools such as Google Scholar and Microsoft Academic Search.[citation needed] CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index.",https://en.wikipedia.org/wiki/CiteSeerX
SHA-3,,https://en.wikipedia.org/wiki/SHA-3
C4.5 algorithm,"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. In 2011, authors of the Weka machine learning software described the C4.5 algorithm as ""a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date"".",https://en.wikipedia.org/wiki/C5.0_algorithm
Floyd–Warshall algorithm,"In computer science, the Floyd–Warshall algorithm (also known as Floyd's algorithm, the Roy–Warshall algorithm, the Roy–Floyd algorithm, or the WFI algorithm) is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. Versions of the algorithm can also be used for finding the transitive closure of a relation R, or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.",https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm
Selection (genetic algorithm),Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).,https://en.wikipedia.org/wiki/Selection_(genetic_algorithm)
Vector quantization,"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression.  It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.",https://en.wikipedia.org/wiki/Vector_quantization
Edge recombination operator,The edge recombination operator (ERO) is an operator that creates a path that is similar to a set of existing paths (parents) by looking at the edges rather than the vertices. The main application of this is for crossover in genetic algorithms when a genotype with non-repeating gene sequences is needed such as for the travelling salesman problem. It was described by Darrell Whitley and others in 1989.,https://en.wikipedia.org/wiki/Edge_recombination_operator
Warped linear predictive coding,"Warped linear predictive coding (warped LPC or WLPC) is a variant of linear predictive coding in which the spectral representation of the system is modified, for example by replacing the unit delays used in an LPC implementation with first-order allpass filters. This can have advantages in reducing the bitrate required for a given level of perceived audio quality/intelligibility, especially in wideband audio coding.",https://en.wikipedia.org/wiki/Warped_Linear_Predictive_Coding
Semi-supervised learning,Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).,https://en.wikipedia.org/wiki/Semi-supervised_learning
Granular computing,"Granular computing (GrC) is an emerging computing paradigm of information processing that concerns the processing of complex information entities called ""information granules"", which arise in the process of data abstraction and derivation of knowledge from information or data.  Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.",https://en.wikipedia.org/wiki/Granular_computing
Conditional random field,"Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighboring"" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.",https://en.wikipedia.org/wiki/Conditional_random_field
Alternating decision tree,An alternating decision tree (ADTree) is a machine learning method for classification. It generalizes decision trees and has connections to boosting.,https://en.wikipedia.org/wiki/Alternating_decision_tree
Algorithms for Recovery and Isolation Exploiting Semantics,"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems. IBM Fellow Dr. C. Mohan is the primary inventor of the ARIES family of algo.",https://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics
Quine–McCluskey algorithm,"The Quine–McCluskey algorithm (or the method of prime implicants) is a method used for minimization of Boolean functions that was developed by Willard V. Quine and extended by Edward J. McCluskey. It is functionally identical to Karnaugh mapping, but the tabular form makes it more efficient for use in computer algorithms, and it also gives a deterministic way to check that the minimal form of a Boolean function has been reached. It is sometimes referred to as the tabulation method.",https://en.wikipedia.org/wiki/Quine%E2%80%93McCluskey_algorithm
Fast multipole method,"The fast multipole method (FMM) is a numerical technique that was developed to speed up the calculation of long-ranged forces in the n-body problem. It does this by expanding the system Green's function using a multipole expansion, which allows one to group sources that lie close together and treat them as if they are a single source.",https://en.wikipedia.org/wiki/Fast_multipole_method
BIRCH,"BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.",https://en.wikipedia.org/wiki/BIRCH
Loss functions for classification,"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).  Given X as the vector space of all possible inputs, and Y = {–1,1} as the vector space of all possible outputs, we wish to find a function f:X↦R.  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same x→.  As a result, the goal of the learning problem is to minimize expected risk, defined as",https://en.wikipedia.org/wiki/Loss_functions_for_classification
Tracing garbage collection,"In computer programming, tracing garbage collection is a form of automatic memory management that consists of determining which objects should be deallocated (""garbage collected"") by tracing which objects are reachable by a chain of references from certain ""root"" objects, and considering the rest as ""garbage"" and collecting them. Tracing garbage collection is the most common type of garbage collection – so much so that ""garbage collection"" often refers to tracing garbage collection, rather than other methods such as reference counting – and there are a large number of algorithms used in implementation.",https://en.wikipedia.org/wiki/Mark_and_sweep
Instance selection,"Instance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.",https://en.wikipedia.org/wiki/Instance_selection
Golem (ILP),"Golem is an inductive logic programming algorithm developed by Stephen Muggleton and Feng. It uses the technique relative least general generalization proposed by Gordon Plotkin. Therefore, only positive examples are used and the search is bottom-up.Negative examples can be used to reduce the size of the hypothesis by deleting useless literals from the body clause.",https://en.wikipedia.org/wiki/Golem_(ILP)
Bayesian network,"A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.",https://en.wikipedia.org/wiki/Bayesian_Network
Tricubic interpolation,"In the mathematical subfield numerical analysis, tricubic interpolation is a method for obtaining values at arbitrary points in 3D space of a function defined on a regular grid.  The approach involves approximating the function locally by an expression of the form ",https://en.wikipedia.org/wiki/Tricubic_interpolation
Overlapping subproblems,"In computer science, a problem is said to have overlapping subproblems if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.",https://en.wikipedia.org/wiki/Overlapping_subproblem
Zeroth (software),"Zeroth is a platform for brain-inspired computing from Qualcomm. It is based around a neural processing unit (NPU) AI accelerator chip and a software API to interact with the platform. It makes a form of machine learning known as deep learning available to mobile devices. It is used for image and sound processing, including speech recognition. The software operates locally rather than as a cloud application.",https://en.wikipedia.org/wiki/Zeroth_(software)
Offline learning,"In machine learning, systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed.[citation needed] These systems are also typically examples of eager learning.[citation needed]",https://en.wikipedia.org/wiki/Offline_learning
Gradient descent,"Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847.",https://en.wikipedia.org/wiki/Gradient_descent
Factor graph,"A factor graph is a bipartite graph representing the factorization of a function.  In probability theory and its applications, factor graphs are used to represent factorization of a probability distribution function, enabling efficient computations, such as the computation of marginal distributions through the sum-product algorithm. One of the important success stories of factor graphs and the sum-product algorithm is the decoding of capacity-approaching error-correcting codes, such as LDPC and turbo codes.",https://en.wikipedia.org/wiki/Factor_graph
Ricart–Agrawala algorithm,"The Ricart-Agrawala algorithm is an algorithm for mutual exclusion on a distributed system. This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, by removing the need for ack messages. It was developed by Glenn Ricart and Ashok Agrawala.",https://en.wikipedia.org/wiki/Ricart-Agrawala_Algorithm
Summed-area table,"A summed-area table is a data structure and algorithm for quickly and efficiently generating the sum of values in a rectangular subset of a grid. In the image processing domain, it is also known as an integral image. It was introduced to computer graphics in 1984 by Frank Crow for use with mipmaps. In computer vision it was popularized by Lewis and then given the name ""integral image"" and prominently used within the Viola–Jones object detection framework in 2001. Historically, this principle is very well known in the study of multi-dimensional probability distribution functions, namely in computing 2D (or ND) probabilities (area under the probability distribution) from the respective cumulative distribution functions.",https://en.wikipedia.org/wiki/Summed_area_table
Discrete cosine transform,"A discrete cosine transform (DCT) expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. The DCT, first proposed by Nasir Ahmed in 1972, is a widely used transformation technique in signal processing and data compression. It is used in most digital media, including digital images (such as JPEG and HEIF, where small high-frequency components can be discarded), digital video (such as MPEG and H.26x), digital audio (such as Dolby Digital, MP3 and AAC), digital television (such as SDTV, HDTV and VOD), digital radio (such as AAC+ and DAB+), and speech coding (such as AAC-LD, Siren and Opus). DCTs are also important to numerous other applications in science and engineering, such as digital signal processing, communications devices, reducing network bandwidth usage, and spectral methods for the numerical solution of partial differential equations.",https://en.wikipedia.org/wiki/Fast_Cosine_Transform
Ramer–Douglas–Peucker algorithm,"The Ramer–Douglas–Peucker algorithm, also known as the Douglas–Peucker algorithm and iterative end-point fit algorithm, is an algorithm that decimates a curve composed of line segments to a similar curve with fewer points.",https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm
Temporal difference learning,"Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.",https://en.wikipedia.org/wiki/Temporal_difference_learning
DarwinTunes,"DarwinTunes is a research project into the use of natural selection to create music led by Bob MacCallum and Armand Leroi, scientists at Imperial College London. The project asks volunteers on the Internet to listen to automatically generated sound loops and rate them based on aesthetic preference. After the volunteers rate the loops on a five-point scale, software permits the highest rated loops to 'reproduce sexually' and populate the next generation of musical loops.",https://en.wikipedia.org/wiki/DarwinTunes
Pollard's kangaroo algorithm,"In computational number theory and computational algebra, Pollard's kangaroo algorithm (also Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem.  The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper  as his better-known Pollard's rho algorithm for solving the same problem.  Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group.",https://en.wikipedia.org/wiki/Pollard%27s_kangaroo_algorithm
Kernel adaptive filter,"In signal processing, a  kernel adaptive filter is a type of nonlinear adaptive filter. An adaptive filter is a filter that adapts its transfer function to changes in signal properties over time by minimizing an error or loss function that characterizes how far the filter deviates from ideal behavior. The adaptation process is based on learning from a sequence of signal samples and is thus an online algorithm. A nonlinear adaptive filter is one in which the transfer function is nonlinear.",https://en.wikipedia.org/wiki/Kernel_adaptive_filter
Gene prediction,"In computational biology, gene prediction or gene finding refers to the process of identifying the regions of genomic DNA that encode genes. This includes protein-coding genes as well as RNA genes, but may also include prediction of other functional elements such as regulatory regions. Gene finding is one of the first and most important steps in understanding the genome of a species once it has been sequenced.",https://en.wikipedia.org/wiki/Gene_prediction
NeuroSolutions,"NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, Levenberg-Marquardt and backpropagation through time. The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.",https://en.wikipedia.org/wiki/NeuroSolutions
Sebastian Thrun,"Sebastian Thrun (born May 14, 1967) is an entrepreneur, educator, and computer scientist from Germany. He is CEO of Kitty Hawk Corporation, and chairman and co-founder of Udacity. Before that, he was a Google VP and Fellow, a Professor of Computer Science at Stanford University, and before that at Carnegie Mellon University. At Google, he founded Google X and Google's self-driving car team. He is also an Adjunct Professor at Stanford University and at Georgia Tech.",https://en.wikipedia.org/wiki/Sebastian_Thrun
Minimum redundancy feature selection,Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR).,https://en.wikipedia.org/wiki/Minimum_redundancy_feature_selection
Statistical machine translation,Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.  The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.,https://en.wikipedia.org/wiki/Statistical_machine_translation
Newton's method,,https://en.wikipedia.org/wiki/Newton%27s_method
Artificial bee colony algorithm,"In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Derviş Karaboğa (Erciyes University) in 2005.",https://en.wikipedia.org/wiki/Artificial_bee_colony_algorithm
Meta learning (computer science),"Meta learningis a subfield of machine learning where automatic learning algorithms are applied on metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.",https://en.wikipedia.org/wiki/Meta_learning_(computer_science)
Pigeonhole sort,"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same. It requires O(n + N) time.  It is similar to counting sort, but differs in that it ""moves items twice: once to the bucket array and again to the final destination  counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.""",https://en.wikipedia.org/wiki/Pigeonhole_sort
Stemming,"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.",https://en.wikipedia.org/wiki/Stemming
Effective fitness,In natural evolution and artificial evolution (e.g. artificial life and evolutionary computation) the  fitness (or performance or objective measure) of a schema is rescaled to give its effective fitness which takes into account crossover and mutation. That is effective fitness can be thought of as the fitness that the schema would need to have in order to increase or decrease as a fraction of the population as it actually does with crossover and mutation present but as if they were not.,https://en.wikipedia.org/wiki/Effective_fitness
LR parser,"In computer science, LR parsers are a type of bottom-up parser that analyses deterministic context-free languages in linear time. There are several variants of LR parsers: SLR parsers, LALR parsers, Canonical LR(1) parsers, Minimal LR(1) parsers, GLR parsers. LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages.",https://en.wikipedia.org/wiki/LR_parser
Random walker algorithm,"The random walker algorithm is an algorithm for image segmentation.  In the first description of the algorithm, a user interactively labels a small number of pixels with known labels (called seeds), e.g., ""object"" and ""background"". The unlabeled pixels are each imagined to release a random walker, and the probability is computed that each pixel's random walker first arrives at a seed bearing each label, i.e., if a user places K seeds, each with a different label, then it is necessary to compute, for each pixel, the probability that a random walker leaving the pixel will first arrive at each seed. These probabilities may be determined analytically by solving a system of linear equations.  After computing these probabilities for each pixel, the pixel is assigned to the label for which it is most likely to send a random walker.  The image is modeled as a graph, in which each pixel corresponds to a node which is connected to neighboring pixels by edges, and the edges are weighted to reflect the similarity between the pixels.  Therefore, the random walk occurs on the weighted graph (see Doyle and Snell for an introduction to random walks on graphs).",https://en.wikipedia.org/wiki/Random_walker_algorithm
Moral graph,"In graph theory, a moral graph is used to find the equivalent undirected form of a directed acyclic graph. It is a key step of the junction tree algorithm, used in belief propagation on graphical models.",https://en.wikipedia.org/wiki/Moral_graph
Fibonacci search technique,"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. Compared to binary search where the sorted array is divided into two equal-sized parts, one of which is examined further, Fibonacci search divides the array into two parts that have sizes that are consecutive Fibonacci numbers. On average, this leads to about 4% more comparisons to be executed, but it has the advantage that one only needs addition and subtraction to calculate the indices of the accessed array elements, while classical binary search needs bit-shift, division or multiplication, operations that were less common at the time Fibonacci search was first published. Fibonacci search has an average- and worst-case complexity of O(log n) (see Big O notation).",https://en.wikipedia.org/wiki/Fibonacci_search_technique
Noisy channel model,"The noisy channel model is a framework used in spell checkers,question answering, speech recognition, and machine translation.In this model, the goal is to find the intended word given a word where theletters have been scrambled in some manner.",https://en.wikipedia.org/wiki/Noisy_channel_model
Stephen Muggleton,,https://en.wikipedia.org/wiki/Stephen_Muggleton
Wikipedia:No original research,,https://en.wikipedia.org/wiki/Wikipedia:Tertiary_sources
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-1-4899-7637-6
Point in polygon,"In computational geometry, the point-in-polygon (PIP) problem asks whether a given point in the plane lies inside, outside, or on the boundary of a polygon. It is a special case of point location problems and finds applications in areas that deal with processing geometrical data, such as computer graphics, computer vision, geographical information systems (GIS), motion planning, and CAD.",https://en.wikipedia.org/wiki/Point_in_polygon
k-nearest neighbors algorithm,"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.",https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
Nearest-neighbor chain algorithm,"In the theory of cluster analysis, the nearest-neighbor chain algorithm is an algorithm that can speed up several methods for agglomerative hierarchical clustering. These are methods that take a collection of points as input, and create a hierarchy of clusters of points by repeatedly merging pairs of smaller clusters to form larger clusters. The clustering methods that the nearest-neighbor chain algorithm can be used for include Ward's method, complete-linkage clustering, and single-linkage clustering; these all work by repeatedly merging the closest two clusters but use different definitions of the distance between clusters. The cluster distances for which the nearest-neighbor chain algorithm works are called reducible and are characterized by a simple inequality among certain cluster distances.",https://en.wikipedia.org/wiki/Nearest-neighbor_chain_algorithm
Gilbert–Johnson–Keerthi distance algorithm,"The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the configuration space obstacle (CSO) of two convex shapes, more commonly known as the Minkowski difference.",https://en.wikipedia.org/wiki/Gilbert%E2%80%93Johnson%E2%80%93Keerthi_distance_algorithm
Leo Breiman,"Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Science.",https://en.wikipedia.org/wiki/Leo_Breiman
Dither,,https://en.wikipedia.org/wiki/Riemersma_dithering
U-Net,"U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.",https://en.wikipedia.org/wiki/U-Net
Picas (app),Picas is free art photo editing application which uses deep neural network and artificial intelligence to automatically redraw photos to artistic effects.,https://en.wikipedia.org/wiki/Picas_(app)
Earley parser,"In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in an abbreviated, more legible, form in a journal).",https://en.wikipedia.org/wiki/Earley_parser
Bayesian interpretation of kernel regularization,"In machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective.  Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces.  In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function.  Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars. More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning.",https://en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization
Rotating calipers,"In computational geometry, the method of rotating calipers is an algorithm design technique that can be used to solve optimization problems including finding the width or diameter of a set of points.",https://en.wikipedia.org/wiki/Rotating_calipers
Population-based incremental learning,"In computer science and machine learning, population-based incremental learning (PBIL) is an optimization algorithm, and an estimation of distribution algorithm. This is a type of genetic algorithm where the genotype of an entire population (probability vector) is evolved rather than individual members. The algorithm is proposed by Shumeet Baluja in 1994. The algorithm is simpler than a standard genetic algorithm, and in many cases leads to better results than a standard genetic algorithm.",https://en.wikipedia.org/wiki/Population-based_incremental_learning
Genetic fuzzy systems,"Genetic fuzzy systems are fuzzy systems constructed by using genetic algorithms or genetic programming, which mimic the process of natural evolution, to identify its structure and parameter.",https://en.wikipedia.org/wiki/Genetic_fuzzy_systems
Facial recognition system,"A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. There are multiple methods in which facial recognition systems work, but in general, they work by comparing selected facial features from given image with faces within a database. It is also described as a Biometric Artificial Intelligence based application that can uniquely identify a person by analyzing patterns based on the person's facial textures and shape.",https://en.wikipedia.org/wiki/Facial_recognition_system
Simplex algorithm,"In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming.",https://en.wikipedia.org/wiki/Simplex_algorithm
Kernel embedding of distributions,"In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space  (RKHS).   A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis.    This learning framework is very general and can be applied to distributions over any space Ω) may be defined.  For example, various kernels have been proposed for learning from data which are: vectors in Rd, discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects.  The theory behind kernel embeddings of distributions has been primarily developed by  Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. A review of recent works on kernel embedding of distributions can be found in.",https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions
Regularization perspectives on support-vector machines,"Regularization perspectives on support-vector machines provide a way of interpreting support-vector machines (SVMs) in the context of other machine-learning algorithms.  SVM algorithms categorize multidimensional data, with the goal of fitting the training set data well, but also avoiding overfitting, so that the solution generalizes to new data points.  Regularization algorithms also aim to fit training set data and avoid overfitting.  They do this by choosing a fitting function that has low error on the training set, but also is not too complicated, where complicated functions are functions with high norms in some function space.  Specifically, Tikhonov regularization algorithms choose a function that minimizes the sum of training-set error plus the function's norm.  The training-set error can be calculated with different loss functions.  For example, regularized least squares is a special case of Tikhonov regularization using the squared error loss as the loss function.",https://en.wikipedia.org/wiki/Regularization_perspectives_on_support_vector_machines
Cocktail shaker sort,"Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is an extension of bubble sort.  The algorithm extends bubble sort by operating in two directions.  While it improves on bubble sort by more quickly moving items to the beginning of the list, it provides only marginal performance improvements. ",https://en.wikipedia.org/wiki/Cocktail_shaker_sort
Frederick Jelinek,,https://en.wikipedia.org/wiki/Frederick_Jelinek
Shattered set,"The concept of shattered sets plays an important role in Vapnik–Chervonenkis theory, also known as VC-theory. Shattering and VC-theory are used in the study of empirical processes as well as in statistical computational learning theory.",https://en.wikipedia.org/wiki/Shattered_set
Chudnovsky algorithm,"The Chudnovsky algorithm is a fast method for calculating the digits of π, based on Ramanujan’s π formulae. It was published by the Chudnovsky brothers in 1988, and was used in the world record calculations of 2.7 trillion digits of π in December 2009, 10 trillion digits in October 2011,, 22.4 trillion digits of π in November 2016. One of Google's employees used Google's supercomputer to calculate 31.4 trillion digits in September 2018–January 2019. Then on January 29, 2020 Timothy Mullican toppled the previous record with a new world record of 50 trillion digits bringing the record back to the personal computer. ",https://en.wikipedia.org/wiki/Chudnovsky_algorithm
Shoelace formula,"The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. The user cross-multiplies corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces. It is also sometimes called the shoelace method.  It has applications in surveying and forestry, among other areas.",https://en.wikipedia.org/wiki/Shoelace_algorithm
Bayesian network,"A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.",https://en.wikipedia.org/wiki/Bayesian_Belief_Network
Eurisko,"Eurisko (Gr., I discover) is a discovery system written by Douglas Lenat in RLL-1, a representation language itself written in the Lisp programming language. A sequel to Automated Mathematician, it consists of heuristics, i.e. rules of thumb, including heuristics describing how to use and change its own heuristics. Lenat was frustrated by Automated Mathematician's constraint to a single domain and so developed Eurisko; his frustration with the effort of encoding domain knowledge for Eurisko led to Lenat's subsequent (and, as of 2020, continuing) development of Cyc. Lenat envisions ultimately coupling the Cyc knowledgebase with the Eurisko discovery engine.",https://en.wikipedia.org/wiki/Eurisko
Strong generating set,"In abstract algebra, especially in the area of group theory, a strong generating set of a permutation group is a generating set that clearly exhibits the permutation structure as described by a stabilizer chain.  A stabilizer chain is a sequence of subgroups, each containing the next and each stabilizing one more point.",https://en.wikipedia.org/wiki/Strong_generating_set
Conference on Neural Information Processing Systems,"The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December.  The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts. ",https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems
Dominance-based rough set approach,"The dominance-based rough set approach (DRSA) is an extension of rough set theory for multi-criteria decision analysis (MCDA), introduced by Greco, Matarazzo and Słowiński. The main change compared to the classical rough sets is the substitution for the indiscernibility relation by a dominance relation, which permits one to deal with inconsistencies typical to consideration of criteria  and preference-ordered decision classes.",https://en.wikipedia.org/wiki/Dominance-based_rough_set_approach
Elliptic-curve cryptography,Elliptic-curve cryptography (ECC) is an approach to public-key cryptography based on the algebraic structure of elliptic curves over finite fields. ECC requires smaller keys compared to non-EC cryptography (based on plain Galois fields) to provide equivalent security.,https://en.wikipedia.org/wiki/Elliptic_curve_cryptography
Monte Carlo method,"Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution.",https://en.wikipedia.org/wiki/Monte_Carlo_simulation
Stress (mechanics),"In continuum mechanics, stress is a physical quantity that expresses the internal forces that neighbouring particles of a continuous material exert on each other, while strain is the measure of the deformation of the material. For example, when a solid vertical bar is supporting an overhead weight, each particle in the bar pushes on the particles immediately below it. When a liquid is in a closed container under pressure, each particle gets pushed against by all the surrounding particles.  The container walls and the pressure-inducing surface (such as a piston) push against them in (Newtonian) reaction. These macroscopic forces are actually the net result of a very large number of intermolecular forces and collisions between the particles in those molecules. Stress is frequently represented by a lowercase Greek letter sigma (σ).",https://en.wikipedia.org/wiki/Stress_(physics)
Newton's method in optimization,"In calculus, Newton's method is an iterative method for finding the roots of a differentiable function F, which are solutions to the equation F (x) = 0. In optimization, Newton's method is applied to the derivative f ′ of a twice-differentiable function f to find the roots of the derivative (solutions to f ′(x) = 0), also known as the stationary points of f. These solutions may be minima, maxima, or saddle points.",https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization
Structured support vector machine,"The structured support vector machine is a machine learning algorithm that generalizes the Support Vector Machine (SVM) classifier.  Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.",https://en.wikipedia.org/wiki/Structured_SVM
Principal geodesic analysis,"In geometric data analysis and statistical shape analysis, principal geodesic analysis is a generalization of principal component analysis to a non-Euclidean, non-linear setting of manifolds suitable for use with shape descriptors such as medial representations.",https://en.wikipedia.org/wiki/Principal_geodesic_analysis
Teuvo Kohonen,"Teuvo Kalevi Kohonen (born July 11, 1934) is a prominent Finnish academic (Dr. Eng.) and researcher. He is currently professor emeritus of the Academy of Finland.",https://en.wikipedia.org/wiki/Teuvo_Kohonen
Isosurface,"An isosurface is a three-dimensional analog of an isoline.  It is a surface that represents points of a constant value (e.g. pressure, temperature, velocity, density) within a volume of space; in other words, it is a level set of a continuous function whose domain is 3D-space.",https://en.wikipedia.org/wiki/Isosurface
Evolutionary art,"Evolutionary art is a branch of generative art, in which the artist does not do the work of constructing the artwork, but rather lets a system do the construction.  In evolutionary art, initially generated art is put through an iterated process of selection and modification to arrive at a final product, where it is the artist who is the selective agent.",https://en.wikipedia.org/wiki/Evolutionary_art
Gnome sort,"Gnome sort (dubbed stupid sort) is a sorting algorithm originally proposed by an Iranian computer scientist Hamid Sarbazi-Azad (professor of Computer Engineering at Sharif University of Technology) in 2000. The sort was first called stupid sort (not to be confused with bogosort), and then later described by Dick Grune and named gnome sort.",https://en.wikipedia.org/wiki/Gnome_sort
Gary B. Fogel,"Gary Bryce Fogel (born 1968) is an American biologist and computer scientist. He is the Chief Executive Officer of Natural Selection, Inc. He is most known for his applications of computational intelligence and machine learning to bioinformatics, computational biology, and industrial optimization.",https://en.wikipedia.org/wiki/Gary_Bryce_Fogel
Competitive learning,"Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network.  It is well suited to finding clusters within data.",https://en.wikipedia.org/wiki/Competitive_learning
Data pre-processing,"Data preprocessing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.",https://en.wikipedia.org/wiki/Data_Pre-processing
KXEN Inc.,KXEN was an American software company which existed from 1998 to 2013 when it was acquired by SAP AG.,https://en.wikipedia.org/wiki/KXEN_Inc.
iDistance,"In pattern recognition, the iDistance is an indexing and query processing technique for k-nearest neighbor queries on point data in multi-dimensional metric spaces. The kNN query is one of the hardest problems on multi-dimensional data, especially when the dimensionality of the data is high. The iDistance is designed to process kNN queries in high-dimensional spaces efficiently and it is especially good for skewed data distributions, which usually occur in real-life data sets.",https://en.wikipedia.org/wiki/IDistance
Robust principal component analysis,"Robust Principal Component Analysis (RPCA) is a modification of the widely used statistical procedure of principal component analysis (PCA) which works well with respect to grossly corrupted observations. A number of different approaches exist for Robust PCA, including an idealized version of Robust PCA, which aims to recover a low-rank matrix L0 from highly corrupted measurements M = L0 +S0. This decomposition in low-rank and sparse matrices can be achieved by techniques such as Principal Component Pursuit method (PCP), Stable PCP, Quantized PCP, Block based PCP, and Local PCP. Then, optimization methods are used such as the Augmented Lagrange Multiplier Method (ALM), Alternating Direction Method (ADM), Fast Alternating Minimization (FAM) or Iteratively Reweighted Least Squares (IRLS ).",https://en.wikipedia.org/wiki/Robust_principal_component_analysis
Tabu search,"Tabu search, created by Fred W. Glover in 1986 and formalized in 1989, is a metaheuristic search method employing local search methods used for mathematical optimization.",https://en.wikipedia.org/wiki/Tabu_search
Threefish,"Threefish is a symmetric-key tweakable block cipher designed as part of the Skein hash function, an entry in the NIST hash function competition. Threefish uses no S-boxes or other table lookups in order to avoid cache timing attacks; its nonlinearity comes from alternating additions with exclusive ORs.  In that respect, it is similar to Salsa20, TEA, and the SHA-3 candidates CubeHash and BLAKE.",https://en.wikipedia.org/wiki/Threefish
Key stretching,"In cryptography, key stretching techniques are used to make a possibly weak key, typically a password or passphrase, more secure against a brute-force attack by increasing the resources (time and possibly space) it takes to test each possible key. Passwords or passphrases created by humans are often short or predictable enough to allow password cracking, and key stretching is intended to make such attacks more difficult by complicating a basic step of trying a single password candidate.",https://en.wikipedia.org/wiki/Key_stretching
Ripple-down rules,Ripple-down rules (RDR) are a way of approaching knowledge acquisition. Knowledge acquisition refers to the transfer of knowledge from human experts to knowledge-based systems.,https://en.wikipedia.org/wiki/Ripple_down_rules
"Hamming(7,4)","In coding theory, Hamming(7,4) is a linear error-correcting code that encodes four bits of data into seven bits by adding three parity bits. It is a member of a larger family of  Hamming codes, but the term Hamming code often refers to this specific code that Richard W. Hamming introduced in 1950. At the time, Hamming worked at Bell Telephone Laboratories and was frustrated with the error-prone punched card reader, which is why he started working on error-correcting codes.","https://en.wikipedia.org/wiki/Hamming(7,4)"
DeepMind,,https://en.wikipedia.org/wiki/DeepMind
Wavelet transform,"In  mathematics, a wavelet series is a representation of a square-integrable (real- or complex-valued) function by a certain orthonormal series generated by a wavelet. This article provides a formal, mathematical definition of an orthonormal wavelet and of the integral wavelet transform. ",https://en.wikipedia.org/wiki/Wavelet_compression
BLAKE (hash function),"BLAKE is a cryptographic hash function based on Dan Bernstein's ChaCha stream cipher, but a permuted copy of the input block, XORed with round constants, is added before each ChaCha round. Like SHA-2, there are two variants differing in the word size. ChaCha operates on a 4×4 array of words. BLAKE repeatedly combines an 8-word hash value with 16 message words, truncating the ChaCha result to obtain the next hash value. BLAKE-256 and BLAKE-224 use 32-bit words and produce digest sizes of 256 bits and 224 bits, respectively, while BLAKE-512 and BLAKE-384 use 64-bit words and produce digest sizes of 512 bits and 384 bits, respectively.",https://en.wikipedia.org/wiki/BLAKE_(hash_function)
Unicode collation algorithm,"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.",https://en.wikipedia.org/wiki/Unicode_Collation_Algorithm
Borwein's algorithm,"In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π. They devised several other algorithms. They published the book Pi and the AGM – A Study in Analytic Number Theory and Computational Complexity.",https://en.wikipedia.org/wiki/Borwein%27s_algorithm
Priority queue,"In  computer science, a priority queue is an abstract data type which is like a regular queue or stack data structure, but where additionally each element has a ""priority"" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.",https://en.wikipedia.org/wiki/Priority_queue
Inductive probability,"Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.",https://en.wikipedia.org/wiki/Inductive_probability
TensorFlow,,https://en.wikipedia.org/wiki/TensorFlow
Multidimensional scaling,"Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset.  MDS is used to translate ""information about the pairwise 'distances' among a set of n objects or individuals"" into a configuration of n points mapped into an abstract Cartesian space.",https://en.wikipedia.org/wiki/Multidimensional_scaling
Gram–Schmidt process,"In mathematics, particularly linear algebra and numerical analysis, the Gram–Schmidt process is a method for orthonormalising a set of vectors in an inner product space, most commonly the Euclidean space Rn equipped with the standard inner product. The Gram–Schmidt process takes a finite, linearly independent set S = {v1, ..., vk} for k ≤ n and generates an orthogonal set S′ = {u1, ..., uk}  that spans the same k-dimensional subspace of Rn as S.",https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process
IRCF360,,https://en.wikipedia.org/wiki/Sensorium_Project
PROGOL,"Progol is Stephen Muggleton's implementation of inductive logic programming used in computer science that combines ""Inverse Entailment"" with ""general-to-specific search"" through a refinement graph. ""Inverse Entailment"" is used with mode declarations to derive the most-specific clause within the mode language which entails a given example. This clause is used to guide a refinement-graph search.",https://en.wikipedia.org/wiki/PROGOL
Pattern recognition,"Pattern recognition is the automated recognition of patterns and regularities in data. Pattern recognition is closely related to artificial intelligence and machine learning, together with applications such as data mining and knowledge discovery in databases (KDD), and is often used interchangeably with these terms. However, these are distinguished: machine learning is one approach to pattern recognition, while other approaches include hand-crafted (not learned) rules or heuristics; and pattern recognition is one approach to artificial intelligence, while other approaches include symbolic artificial intelligence.",https://en.wikipedia.org/wiki/Pattern_recognition
Numerical integration,"In analysis, numerical integration comprises a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations. This article focuses on calculation of definite integrals. The term numerical quadrature (often abbreviated to quadrature) is more or less a synonym for numerical integration, especially as applied to one-dimensional integrals. Some authors refer to numerical integration over more than one dimension as cubature; others take quadrature to include higher-dimensional integration.",https://en.wikipedia.org/wiki/Numerical_integration
Applied science,"Applied science is the application of existing scientific knowledge to practical applications, like technology or inventions.",https://en.wikipedia.org/wiki/Applied_science
Hill climbing,"In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.",https://en.wikipedia.org/wiki/Random-restart_hill_climbing
Category utility,"Category utility is a measure of ""category goodness"" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as ""cue validity"" (Reed 1972; Rosch & Mervis 1975) and ""collocation index"" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp. 260–262).",https://en.wikipedia.org/wiki/Category_utility
Shortest job next,"Shortest job next (SJN), also known as shortest job first (SJF) or shortest process next (SPN), is a scheduling policy that selects for execution the waiting process with the smallest execution time. SJN is a non-preemptive algorithm. Shortest remaining time is a preemptive variant of SJN.",https://en.wikipedia.org/wiki/Shortest_job_next
Rainflow-counting algorithm,"The rainflow-counting algorithm is used in the analysis of fatigue data in order to reduce a spectrum of varying stress into an equivalent set of simple stress reversals. The method successively extracts the smaller interruption cycles from a sequence, which models the material memory effect seen with stress-strain hysteresis cycles. This simplification allows the fatigue life of a component to be determined for each rainflow cycle using either Miner's rule to calculate the fatigue damage, or in a crack growth equation to calculate the crack increment. The algorithm was developed by Tatsuo Endo and M. Matsuishi in 1968.",https://en.wikipedia.org/wiki/Rainflow-counting_algorithm
B-spline,"In the mathematical subfield of numerical analysis, a B-spline, or basis spline, is a spline function that has minimal support with respect to a given degree, smoothness, and domain partition. Any spline function of given degree can be expressed as a linear combination of B-splines of that degree. Cardinal B-splines have knots that are equidistant from each other. B-splines can be used for curve-fitting and numerical differentiation of experimental data.",https://en.wikipedia.org/wiki/B-spline
Perceptron,"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.",https://en.wikipedia.org/wiki/Perceptron
HITS algorithm,"Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held, but were used as compilations of a broad catalog of information that led users direct to other authoritative pages. In other words, a good hub represents a page that pointed to many other pages, while a good authority represents a page that is linked by many different hubs.",https://en.wikipedia.org/wiki/Hyperlink-Induced_Topic_Search
Context tree weighting,"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004).The CTW algorithm is an “ensemble method,” mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators.",https://en.wikipedia.org/wiki/Context_tree_weighting
Hopcroft–Karp algorithm,"In computer science, the Hopcroft–Karp algorithm (sometimes more accurately called the Hopcroft–Karp–Karzanov algorithm) is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching – a set of as many edges as possible with the property that no two edges share an endpoint. It runs in O(|E||V|) time in the worst case, where E is set of edges in the graph, V is set of vertices of the graph, and it is assumed that |E|=Ω(|V|). In the case of dense graphs the time bound becomes O(|V|2.5), and for sparse random graphs it runs in near-linear (in |E|) time.",https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm
Metaphone,"Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar-sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.",https://en.wikipedia.org/wiki/Double_Metaphone
Log-linear model,"A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form",https://en.wikipedia.org/wiki/Log-linear_model
Ant colony optimization algorithms,"In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. The pheromone-based communication of biological ants is often the predominant paradigm used.   Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima.",https://en.wikipedia.org/wiki/Ant_colony_optimization
Linear congruential generator,"A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. The method represents one of the oldest and best-known pseudorandom number generator algorithms. The theory behind them is relatively easy to understand, and they are easily implemented and fast, especially on computer hardware which can provide modular arithmetic by storage-bit truncation.",https://en.wikipedia.org/wiki/Linear_congruential_generator
User behavior analytics,"User behavior analytics (UBA) as defined by Gartner is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats.  Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.",https://en.wikipedia.org/wiki/User_behavior_analytics
mlpy,,https://en.wikipedia.org/wiki/Mlpy
Multi-armed bandit,"In probability theory, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as ""one-armed bandits""), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.",https://en.wikipedia.org/wiki/Multi-armed_bandit
Maximum subarray problem,"In computer science, the maximum sum subarray problem is the task of finding a contiguous subarray with the largest sum, within a given one-dimensional array A[1...n] of numbers.  Formally, the task is to find indices i, such that the sum",https://en.wikipedia.org/wiki/Kadane%27s_algorithm
HITS algorithm,"Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held, but were used as compilations of a broad catalog of information that led users direct to other authoritative pages. In other words, a good hub represents a page that pointed to many other pages, while a good authority represents a page that is linked by many different hubs.",https://en.wikipedia.org/wiki/Hubs_and_authorities
Stochastic universal sampling,Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. It was introduced by James Baker.,https://en.wikipedia.org/wiki/Stochastic_universal_sampling
Transfer learning,"Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.  This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.",https://en.wikipedia.org/wiki/Transfer_learning
Toom–Cook multiplication,"Toom–Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm for large integers.",https://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication
Autoencoder,"An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words..mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}",https://en.wikipedia.org/wiki/Autoencoder
Beam stack search,"Beam stack search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to depth-first beam search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.",https://en.wikipedia.org/wiki/Beam_stack_search
Low-rank approximation,"In mathematics, low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data) and an approximating matrix (the optimization variable), subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure.",https://en.wikipedia.org/wiki/Low-rank_approximation
Immunocomputing,"Immunocomputing explores the principles of information processing that proteins and immune networks utilize in order to solve specific complex problems while protected from viruses, noise, errors and intrusions.",https://en.wikipedia.org/wiki/Immunocomputing
Bilinear interpolation,"In mathematics, bilinear interpolation is an extension of linear interpolation for interpolating functions of two variables (e.g., x and y) on a rectilinear 2D grid.Bilinear interpolation is performed using linear interpolation first in one direction, and then again in the other direction. Although each step is linear in the sampled values and in the position, the interpolation as a whole is not linear but rather quadratic in the sample location.",https://en.wikipedia.org/wiki/Bilinear_interpolation
Differential evolution,"In evolutionary computation, differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. Such methods are commonly known as metaheuristics as they make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as DE do not guarantee an optimal solution is ever found.",https://en.wikipedia.org/wiki/Differential_evolution
Information gain ratio,"In decision tree learning, Information gain ratio is a ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosingan attribute.",https://en.wikipedia.org/wiki/Information_gain_ratio
Single-linkage clustering,"In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.",https://en.wikipedia.org/wiki/Single-linkage_clustering
Optimal discriminant analysis,"Optimal Discriminant Analysis (ODA)  and the related classification tree analysis (CTA) are exact statistical methods that maximize predictive accuracy.  For any specific sample and exploratory or confirmatory hypothesis, optimal discriminant analysis (ODA) identifies the statistical model that yields maximum predictive accuracy, assesses the exact Type I error rate, and evaluates potential cross-generalizability. Optimal discriminant analysis may be applied to > 0 dimensions, with the one-dimensional case being referred to as UniODA and the multidimensional case being referred to as MultiODA.  Classification tree analysis is a generalization of optimal discriminant analysis to non-orthogonal trees. Classification tree analysis has more recently been called ""hierarchical optimal discriminant analysis"".  Optimal discriminant analysis and classification tree analysis may be used to find the combination of variables and cut points that best separate classes of objects or events. These variables and cut points may then be used to reduce dimensions and to then build a  statistical model that optimally describes the data.",https://en.wikipedia.org/wiki/Optimal_discriminant_analysis
"Win–stay, lose–switch","In psychology, game theory, statistics, and machine learning, win–stay, lose–switch (also win–stay, lose–shift) is a heuristic learning strategy used to model learning in decision situations.  It was first invented as an improvement over randomization in bandit problems.  It was later applied to the prisoner's dilemma in order to model the evolution of altruism.","https://en.wikipedia.org/wiki/Win%E2%80%93stay,_lose%E2%80%93switch"
Data stream clustering,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time. ",https://en.wikipedia.org/wiki/Data_stream_clustering
RIPEMD,"RIPEMD (RIPE Message Digest) is a family of cryptographic hash functions developed in 1992 (the original RIPEMD) and 1996 (other variants). There are five functions in the family: RIPEMD, RIPEMD-128, RIPEMD-160, RIPEMD-256, and RIPEMD-320, of which RIPEMD-160 is the most common.",https://en.wikipedia.org/wiki/RIPEMD-160
Cheney's algorithm,,https://en.wikipedia.org/wiki/Cheney%27s_algorithm
Line drawing algorithm,"A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media. On discrete media, such as pixel-based displays and printers, line drawing requires such an approximation (in nontrivial cases). Basic algorithms rasterize lines in one color. A better representation with multiple color gradations requires an advanced process, spatial anti-aliasing.",https://en.wikipedia.org/wiki/Line_drawing_algorithm
Uncertain data,"In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.",https://en.wikipedia.org/wiki/Uncertain_data
Vowpal Wabbit,"Vowpal Wabbit (also known as ""VW"") is an open-source fast online interactive machine learning system library and program developed originally at  Yahoo! Research, and currently at Microsoft Research.  It was started and is led by John Langford. Vowpal Wabbit's interactive learning support is particularly notable including Contextual Bandits, Active Learning, and forms of guided Reinforcement Learning.  Vowpal Wabbit provides an efficient scalable out-of-core implementation with support for a number of machine learning reductions, importance weighting, and a selection of different loss functions and optimization algorithms.",https://en.wikipedia.org/wiki/Vowpal_Wabbit
Markov logic network,"A Markov logic network (MLN) is a probabilistic logic which applies the ideas of a Markov network to first-order logic, enabling uncertain inference. Markov logic networks generalize first-order logic, in the sense that, in a certain limit, all unsatisfiable statements have a probability of zero, and all tautologies have probability one.",https://en.wikipedia.org/wiki/Markov_logic_network
BrownBoost,"BrownBoost is a boosting algorithm that may be robust to noisy datasets.  BrownBoost is an adaptive version of the boost by majority algorithm.  As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods.  BrownBoost was introduced by Yoav Freund in 2001.",https://en.wikipedia.org/wiki/BrownBoost
MinHash,"In computer science and data mining, MinHash (or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly estimating how similar two sets are. The scheme was invented by Andrei Broder (1997), and initially used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results.It has also been applied in large-scale clustering problems, such as clustering documents by the similarity of their sets of words.",https://en.wikipedia.org/wiki/MinHash
Kernel density estimation,"In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.  Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method,  after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.",https://en.wikipedia.org/wiki/Kernel_density_estimation
Hash function,"A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.  The values are used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.",https://en.wikipedia.org/wiki/Hash_Function
Point spread function,"The point spread function (PSF) describes the response of an imaging system to a point source or point object. A more general term for the PSF is a system's impulse response, the PSF being the impulse response of a focused optical system. The PSF in many contexts can be thought of as the extended blob in an image that represents a single point object. In functional terms it is the spatial domain version of the optical transfer function of the imaging system. It is a useful concept in Fourier optics, astronomical imaging, medical imaging, electron microscopy and other imaging techniques such as 3D microscopy (like in confocal laser scanning microscopy) and fluorescence microscopy. The degree of spreading (blurring) of the point object is a measure for the quality of an imaging system. In non-coherent imaging systems such as fluorescent microscopes, telescopes or optical microscopes, the image formation process is linear in the image intensity and described by linear system theory. This means that when two objects A and B are imaged simultaneously, the resulting image is equal to the sum of the independently imaged objects. In other words: the imaging of A is unaffected by the imaging of B and vice versa, owing to the non-interacting property of photons. In space-invariant system, i.e. the PSF is the same everywhere in the imaging space, the image of a complex object is then the convolution of the true object and the PSF. However, when the detected light is coherent, image formation is linear in the complex field. The recorded intensity image then can show cancellations or other non-linear effects.",https://en.wikipedia.org/wiki/Point_spread_function
Bitonic sorter,"Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of O(nlog2⁡(n)), where n is the number of items to be sorted.",https://en.wikipedia.org/wiki/Bitonic_sorter
Multiple instance learning,"In machine learning, multiple-instance learning (MIL) is a type of supervised learning.  Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative.  On the other hand, a bag is labeled positive if there is at least one instance in it which is positive.  From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.",https://en.wikipedia.org/wiki/Multiple-instance_learning
Digital object identifier,,https://en.wikipedia.org/wiki/Digital_object_identifier
Graham scan,"Graham's scan is a method of finding the convex hull of a finite set of points in the plane with time complexity O(n log n). It is named after Ronald Graham, who published the original algorithm in 1972. The algorithm finds all vertices of the convex hull ordered along its boundary. It uses a stack to detect and remove concavities in the boundary efficiently.",https://en.wikipedia.org/wiki/Graham_scan
Boosting (machine learning),"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): ""Can a set of weak learners create a single strong learner?"" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.",https://en.wikipedia.org/wiki/Boosting_(machine_learning)
Peter Sanders (computer scientist),"Peter Sanders (born 1967) is a German computer scientist who works as a professor of computer science at the Karlsruhe Institute of Technology. His research concerns the design, analysis, and implementation of algorithms and data structures, and he is particularly known for his research on suffix sorting finding shortest paths in road networks.",https://en.wikipedia.org/wiki/Peter_Sanders_(computer_scientist)
Odds algorithm,"The odds-algorithm  is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems.  Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below. ",https://en.wikipedia.org/wiki/Odds_algorithm
Analytical hierarchy,"In mathematical logic and descriptive set theory, the analytical hierarchy is an extension of the arithmetical hierarchy. The analytical hierarchy of formulas includes formulas in the language of second-order arithmetic, which can have quantifiers over both the set of natural numbers, N, and over functions from N. The analytical hierarchy of sets classifies sets by the formulas that can be used to define them; it is the lightface version of the projective hierarchy.",https://en.wikipedia.org/wiki/Analytical_hierarchy
Marching squares,Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values). A similar method can be used to contour 2D triangle meshes.,https://en.wikipedia.org/wiki/Marching_squares
Inferential theory of learning,"Inferential Theory of Learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been continuously developed by Ryszard S. Michalski, starting in the 1980s. The first known publication of ITL was in 1983. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored. Stored information will later be used by the learner for future inferences. Inferences are split into multiple categories including conclusive, deduction, and induction. In order for an inference to be considered complete it was required that all categories must be taken into account. This is how the ITL varies from other machine learning theories like Computational Learning Theory and Statistical Learning Theory; which both use singular forms of inference.",https://en.wikipedia.org/wiki/Inferential_theory_of_learning
Chakravala method,"The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. It is commonly attributed to Bhāskara II, (c. 1114 – 1185 CE) although some attribute it to Jayadeva (c.  950 ~ 1000 CE). Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, and he then described this general method, which was later refined by Bhāskara II in his Bijaganita treatise. He called it the Chakravala method: chakra meaning ""wheel"" in Sanskrit, a reference to the cyclic nature of the algorithm. C.-O. Selenius held that no European performances at the time of Bhāskara, nor much later, exceeded its marvellous height of mathematical complexity.",https://en.wikipedia.org/wiki/Chakravala_method
Ross Quinlan,"John Ross Quinlan is a computer science researcher in data mining and decision theory. He has contributed extensively to the development of decision tree algorithms, including inventing the canonical C4.5 and ID3 algorithms. He also contributed to early ILP literature with First Order Inductive Learner (FOIL). He is currently running the company RuleQuest Research which he founded in 1997.",https://en.wikipedia.org/wiki/Ross_Quinlan
Probabilistic context-free grammar,Grammar theory to model symbol strings originated from work in computational linguistics aiming to understand the structure of natural languages. Probabilistic context free grammars (PCFGs) have been applied in probabilistic modeling of RNA structures almost 40 years after they were introduced in computational linguistics.,https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar
Armin B. Cremers,"Armin Bernd Cremers (born June 7, 1946 in Eisenach, Germany) is a German mathematician and computer scientist. He is a Prof. em. in the computer science institute at the University of Bonn, Germany. He is most notable for his contributions to several fields of discrete mathematics including formal languages and automata theory. In more recent years he has been recognized for his work in artificial intelligence, machine learning and robotics as well as in geoinformatics and deductive databases.",https://en.wikipedia.org/wiki/Armin_B._Cremers
Adaptive replacement cache,"Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (least recently used). This is accomplished by keeping track of both frequently used and recently used pages plus a recent eviction history for both. The algorithm was developed at the IBM Almaden Research Center. In 2006, IBM was granted a patent for the adaptive replacement cache policy.",https://en.wikipedia.org/wiki/Adaptive_replacement_cache
Multivariate adaptive regression spline,"In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.",https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines
Hebbian theory,"Hebbian theory is a neuroscientific theory claiming that an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell. It is an attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process. It was introduced by Donald Hebb in his 1949 book The Organization of Behavior. The theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory.",https://en.wikipedia.org/wiki/Hebbian_theory
Long division,"In arithmetic, long division is a standard division algorithm suitable for dividing multi-digit numbers that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps.",https://en.wikipedia.org/wiki/Long_division
Jaime Carbonell,"Jaime Guillermo Carbonell (July 29, 1953 – February 28, 2020) was a computer scientist who made seminal contributions to the development of natural language processing tools and technologies. His extensive research in machine translation resulted in the development of several state-of-the-art language translation and artificial intelligence systems. He earned his B.S. degrees in Physics and in Mathematics from MIT in 1975 and did his Ph.D. under Dr. Roger Schank at Yale University in 1979. He joined Carnegie Mellon University as an assistant professor of computer science in 1979 and lived in Pittsburgh from then. He was affiliated with the Language Technologies Institute, Computer Science Department, Machine Learning Department, and Computational Biology Department at Carnegie Mellon.",https://en.wikipedia.org/wiki/Jaime_Carbonell
Gibbs sampling,"In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximately from a specified multivariate probability distribution, when direct sampling is difficult.  This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables).  Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.",https://en.wikipedia.org/wiki/Gibbs_sampling
Information fuzzy networks,"Information fuzzy networks (IFN) is a greedy machine learning algorithm for supervised learning.The data structure produced by the learning algorithm is also called Info Fuzzy Network.IFN construction is quite similar to decision trees' construction.However, IFN constructs a directed graph and not a tree.IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.",https://en.wikipedia.org/wiki/Information_Fuzzy_Networks
Evolvability (computer science),The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries.,https://en.wikipedia.org/wiki/Evolvability_(computer_science)
Meta-optimization,"In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson for finding optimal parameter settings of a genetic algorithm.",https://en.wikipedia.org/wiki/Meta-optimization
Medical imaging,"Medical imaging is the technique and process of creating visual representations of the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging.",https://en.wikipedia.org/wiki/Medical_imaging
Evolutionary multimodal optimization,"In applied mathematics, multimodal optimization deals with optimization tasks that involve finding all or most of the multiple (at least locally optimal) solutions of a problem, as opposed to a single best solution.Evolutionary multimodal optimization is a branch of evolutionary computation, which is closely related to machine learning. Wong provides a short survey, wherein the chapter of Shir and the book of Preuss cover the topic in more detail.",https://en.wikipedia.org/wiki/Evolutionary_multimodal_optimization
Doubly stochastic model,"In statistics, a doubly stochastic model is a type of model that can arise in many contexts, but in particular in modelling time-series and stochastic processes. ",https://en.wikipedia.org/wiki/Doubly_stochastic_model
Error tolerance (PAC learning),,https://en.wikipedia.org/wiki/Error_tolerance_(PAC_learning)
Lempel–Ziv–Stac,"Lempel–Ziv–Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. It was originally developed by Stac Electronics for tape compression, and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. It was later specified as a compression algorithm for various network protocols. LZS is specified in the Cisco IOS stack.",https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Stac
Polynomial interpolation,"In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset.",https://en.wikipedia.org/wiki/Polynomial_interpolation
Merge sort,"In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.",https://en.wikipedia.org/wiki/Merge_sort
Pitman–Yor process,"In probability theory,  a Pitman–Yor process denoted PY(d, θ, G0), is a stochastic process whose sample path is a probability distribution. A random sample from this process is an infinite discrete probability distribution, consisting of an infinite set of atoms drawn from G0, with weights drawn from a two-parameter Poisson–Dirichlet distribution.  The process is named after Jim Pitman and Marc Yor.",https://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process
Clock synchronization,"Clock synchronization is a topic in computer science and engineering that aims to coordinate otherwise independent clocks. Even when initially set accurately, real clocks will differ after some amount of time due to clock drift, caused by clocks counting time at slightly different rates. There are several problems that occur as a result of clock rate differences and several solutions, some being more appropriate than others in certain contexts.",https://en.wikipedia.org/wiki/Clock_synchronization
Estimation theory,Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component.  The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.,https://en.wikipedia.org/wiki/Estimation_theory
Radon transform,"In mathematics, the Radon transform is the integral transform which takes a function f defined on the plane to a function Rf defined on the (two-dimensional) space of lines in the plane, whose value at a particular line is equal to the line integral of the function over that line. The transform was introduced in 1917 by Johann Radon, who also provided a formula for the inverse transform. Radon further included formulas for the transform in three dimensions, in which the integral is taken over planes (integrating over lines is known as the X-ray transform).  It was later generalized to higher-dimensional Euclidean spaces, and more broadly in the context of integral geometry.  The complex analog of the Radon transform is known as the Penrose transform. The Radon transform is widely applicable to tomography, the creation of an image from the projection data associated with cross-sectional scans of an object.",https://en.wikipedia.org/wiki/Radon_transform
Boyer–Moore string-search algorithm,"In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature. It was developed by Robert S. Boyer and J Strother Moore in 1977. The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer–Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.",https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm
Bailey–Borwein–Plouffe formula,"The Bailey–Borwein–Plouffe formula (BBP formula) is a formula for π. It was discovered in 1995 by Simon Plouffe and is named after the authors of the article in which it was published, David H. Bailey, Peter Borwein, and Plouffe. Before that, it had been published by Plouffe on his own site. The formula is",https://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula
Markov information source,"In mathematics, a Markov information source, or simply, a Markov source, is an information source whose underlying dynamics are given by a stationary finite Markov chain.",https://en.wikipedia.org/wiki/Markov_information_source
Pushmeet Kohli,"Pushmeet Kohli is a computer scientist at Google DeepMind where he heads the ""Robust and Reliable AI"" and ""AI for Science"" teams. Before joining DeepMind, he was partner scientist and director of research at Microsoft Research. Pushmeet conducts research in the field of machine learning and computer vision. However, he has also made contributions in game theory, discrete algorithms and psychometrics. He is the recipient of the BMVA Sullivan Prize. His papers have received awards at UAI 2018, CVPR 2015, WWW 2014, ISMAR 2011 and ECCV 2010.",https://en.wikipedia.org/wiki/Pushmeet_Kohli
Deterministic automaton,"In computer science, a deterministic automaton is a concept of automata theory in which the outcome of a transition from one state to another is determined by the input.",https://en.wikipedia.org/wiki/Deterministic_automaton
Cooley–Tukey FFT algorithm,,https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm
Sequence labeling,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values.  A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document.  Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence.  However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once.",https://en.wikipedia.org/wiki/Sequence_labeling
Broyden–Fletcher–Goldfarb–Shanno algorithm,"In numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.",https://en.wikipedia.org/wiki/BFGS_method
Template:Machine learning bar,,https://en.wikipedia.org/wiki/Template:Machine_learning_bar
Projection pursuit,"Projection pursuit (PP) is a type of statistical technique which involves finding the most ""interesting"" possible projections in multidimensional data. Often, projections which deviate more from a normal distribution are considered to be more interesting.  As each projection is found, the data are reduced by removing the component along that projection, and the process is repeated to find new projections; this is the ""pursuit"" aspect that motivated the technique known as matching pursuit.",https://en.wikipedia.org/wiki/Projection_pursuit
Variable kernel density estimation,"In statistics, adaptive or ""variable-bandwidth"" kernel density estimation is a form of kernel density estimation in which the size of the kernels used in the estimate are varieddepending upon either the location of the samples or the location of the test point.It is a particularly effective technique when the sample space is multi-dimensional.",https://en.wikipedia.org/wiki/Variable_kernel_density_estimation
Edge detection,"Edge detection includes a variety of mathematical methods that aim at identifying points in a digital image at which the image brightness changes sharply or, more formally, has discontinuities. The points at which image brightness changes sharply are typically organized into a set of curved line segments termed edges. The same problem of finding discontinuities in one-dimensional signals is known as step detection and the problem of finding signal discontinuities over time is known as change detection. Edge detection is a fundamental tool in image processing, machine vision and computer vision, particularly in the areas of feature detection and feature extraction.",https://en.wikipedia.org/wiki/Edge_detection
Partial differential equation,"In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.",https://en.wikipedia.org/wiki/Partial_differential_equation
Power iteration,"In mathematics, power iteration (also known as the power method) is an eigenvalue algorithm: given a diagonalizable matrix A, the algorithm will produce a number λ, which is the greatest (in absolute value) eigenvalue of A, and a nonzero vector v, which is a corresponding eigenvector of λ, that is, Av=λv.The algorithm is also known as the Von Mises iteration.",https://en.wikipedia.org/wiki/Power_iteration
Never-Ending Language Learning,"Never-Ending Language Learning system (NELL) is a semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, NSF, and CNPq with portions of the system running on a supercomputing cluster provided by Yahoo!.",https://en.wikipedia.org/wiki/Never-Ending_Language_Learning
Canonical correspondence analysis,"In applied statistics, canonical correspondence analysis (CCA) is a multivariate constrained ordination technique that extracts major gradients among combinations of explanatory variables in a dataset. The requirements of a CCA are that the samples are random and independent. Also, the data are categorical and that the independent variables are consistent within the sample site and error-free.",https://en.wikipedia.org/wiki/Canonical_correspondence_analysis
LZWL,LZWL is a syllable-based variant of the character-based LZW compression algorithm that  can work with syllables obtained by all algorithms of decomposition into syllables. The algorithm can be used for words too.,https://en.wikipedia.org/wiki/LZWL
Microcanonical ensemble,,https://en.wikipedia.org/wiki/Microcanonical_ensemble
Bully algorithm,"In distributed computing, the bully algorithm is a method for dynamically electing a coordinator or leader from a group of distributed computer processes. The process with the highest process ID number from amongst the non-failed processes is selected as the coordinator.",https://en.wikipedia.org/wiki/Bully_algorithm
LZJB,"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. The software is CDDL license licensed. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms.. The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator.",https://en.wikipedia.org/wiki/LZJB
Logical block addressing,"Logical block addressing (LBA) is a common scheme used for specifying the location of blocks of data stored on computer storage devices, generally secondary storage systems such as hard disk drives.  LBA is a particularly simple linear addressing scheme; blocks are located by an integer index, with the first block being LBA 0, the second LBA 1, and so on.",https://en.wikipedia.org/wiki/CHS_conversion
Minimum message length,"Minimum message length (MML) is a Bayesian information-theoretic method for statistical model comparison and selection. It provides a formal information theory restatement of Occam's Razor: even when models are equal in their measure of fit-accuracy to the observed data, the one generating the most concise explanation of data is more likely to be correct (where the explanation consists of the statement of the model, followed by the lossless encoding of the data using the stated model). MML was invented by Chris Wallace, first appearing in the seminal paper ""An information measure for classification"". MML is intended not just as a theoretical construct, but as a technique that may be deployed in practice. It differs from the related concept of Kolmogorov complexity in that it does not require use of a Turing-complete language to model data.",https://en.wikipedia.org/wiki/Minimum_message_length
General Problem Solver,"General Problem Solver or G.P.S. is a computer program created in 1959 by Herbert A. Simon, J. C. Shaw, and Allen Newell intended to work as a universal problem solver machine. Any problem that can be expressed as a set of well-formed formulas (WFFs) or Horn clauses, and that constitute a directed graph with one or more sources (that is, axioms) and sinks (that is, desired conclusions), can be solved, in principle, by GPS. Proofs in the predicate logic and Euclidean geometry problem spaces are prime examples of the domain the applicability of GPS. It was based on Simon and Newell's theoretical work on logic machines. GPS was the first computer program which separated its knowledge of problems (rules represented as input data) from its strategy of how to solve problems (a generic solver engine). GPS was implemented in the third-order programming language, IPL.",https://en.wikipedia.org/wiki/General_Problem_Solver
Evolution window,"It was observed in evolution strategies that significant progress toward the fitness/objective function's optimum, generally, can only happen in a narrow band of the mutation step size σ. That narrow band is called evolution window.",https://en.wikipedia.org/wiki/Evolution_window
Speech synthesis,"Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.",https://en.wikipedia.org/wiki/Speech_synthesis
Jackknife variance estimates for random forest,"In statistics, jackknife variance estimates for random forest are a way to estimate the variance in random forest models, in order to eliminate the bootstrap effects.",https://en.wikipedia.org/wiki/Jackknife_variance_estimates_for_random_forest
Distributed algorithm,"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.",https://en.wikipedia.org/wiki/Distributed_algorithm
String metric,"In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (""inverse similarity"") between two text strings for approximate string matching or comparison and in fuzzy string searching. A necessary requirement for a string metric (e.g. in contrast to string matching) is fulfillment of the triangle inequality. For example, the strings ""Sam"" and ""Samuel"" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.",https://en.wikipedia.org/wiki/String_metric
Amazon Web Services,,https://en.wikipedia.org/wiki/Amazon_Machine_Learning
Parsing expression grammar,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s.Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.",https://en.wikipedia.org/wiki/Parsing_expression_grammar
X-ray,"X-rays make up X-radiation, a form of high-energy electromagnetic radiation. Most X-rays have a wavelength ranging from 0.03 to 3 nanometres, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and energies in the range 100 eV to 200 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to as Röntgen radiation, after the German scientist Wilhelm Röntgen, who discovered it on November 8, 1895. He named it X-radiation to signify an unknown type of radiation. Spellings of X-ray(s) in English include the variants x-ray(s), xray(s), and X ray(s).",https://en.wikipedia.org/wiki/X-ray
Variable-order Bayesian network,"Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models. VOBN models are used in machine learning in general and have shown great potential in bioinformatics applications.These models extend the widely used position weight matrix (PWM) models, Markov models, and Bayesian network (BN) models.",https://en.wikipedia.org/wiki/Variable-order_Bayesian_network
Entropy,,https://en.wikipedia.org/wiki/Entropy
Smith–Waterman algorithm,"The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings of nucleic acid sequences or protein sequences. Instead of looking at the entire sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.",https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm
Chomsky normal form,"In formal language theory, a context-free grammar G is said to be in Chomsky normal form (first described by Noam Chomsky) if all of its production rules are of the form::92–93,106",https://en.wikipedia.org/wiki/Chomsky_normal_form
Sample exclusion dimension,"In computational learning theory, sample exclusion dimensions arise in the study of exact concept learning with queries.",https://en.wikipedia.org/wiki/Sample_exclusion_dimension
Methods of computing square roots,"Methods of computing square roots are numerical analysis algorithms for finding the principal, or non-negative, square root (usually denoted √S, 2√S, or S1/2) of a real number. Arithmetically, it means given S, a procedure for finding a number which when multiplied by itself, yields S; algebraically, it means a procedure for finding the non-negative root of the equation x2 - S = 0; geometrically, it means given the area of a square, a procedure for constructing a side of the square.",https://en.wikipedia.org/wiki/Methods_of_computing_square_roots
Interactive machine translation,"Interactive machine translation (IMT), is a specific sub-field of computer-aided translation. Under this translation paradigm, the computer software that assists the human translator attempts to predict the text the user is going to input by taking into account all the information it has available. Whenever such prediction is wrong and the user provides feedback to the system, a new prediction is performed considering the new information available. Such process is repeated until the translation provided matches the  user's expectations.",https://en.wikipedia.org/wiki/Interactive_machine_translation
Travelling salesman problem,,https://en.wikipedia.org/wiki/Traveling_salesman_problem
Halley's method,"In numerical analysis, Halley's method is a root-finding algorithm used for functions of one real variable with a continuous second derivative. It is named after its inventor Edmond Halley.",https://en.wikipedia.org/wiki/Halley%27s_method
Supervised learning,"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).",https://en.wikipedia.org/wiki/Supervised_learning
Inverted pendulum,"An inverted pendulum is a pendulum that has its center of mass above its pivot point.  It is unstable and without additional help will fall over. It can be suspended stably in this inverted position by using a control system to monitor the angle of the pole and move the pivot point horizontally back under the center of mass when it starts to fall over, keeping it balanced.  The inverted pendulum is a classic problem in dynamics and control theory and is used as a benchmark for testing control strategies.  It is often implemented with the pivot point mounted on a cart that can move horizontally under control of an electronic servo system as shown in the photo; this is called a cart and pole apparatus. Most applications limit the pendulum to 1 degree of freedom by affixing the pole to an axis of rotation. Whereas a normal pendulum is stable when hanging downwards, an inverted pendulum is inherently unstable, and must be actively balanced in order to remain upright; this can be done either by applying a torque at the pivot point, by moving the pivot point horizontally as part of a feedback system, changing the rate of rotation of a mass mounted on the pendulum on an axis parallel to the pivot axis and thereby generating a net torque on the pendulum, or by oscillating the pivot point vertically. A simple demonstration of moving the pivot point in a feedback system is achieved by balancing an upturned broomstick on the end of one's finger.",https://en.wikipedia.org/wiki/Inverted_pendulum
Boolean satisfiability problem,"In logic and computer science, the Boolean satisfiability problem (sometimes called propositional satisfiability problem and abbreviated SATISFIABILITY or SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. On the other hand, if no such assignment exists, the function expressed by the formula is FALSE for all possible variable assignments and the formula is unsatisfiable. For example, the formula ""a AND NOT b"" is satisfiable because one can find the values a = TRUE and b = FALSE, which make (a AND NOT b) = TRUE. In contrast, ""a AND NOT a"" is unsatisfiable.",https://en.wikipedia.org/wiki/CNF-SAT
Parity bit,"A parity bit, or check bit, is a bit added to a string of binary code to ensure that the total number of 1-bits in the string is even or odd. Parity bits are used as the simplest form of error detecting code.",https://en.wikipedia.org/wiki/Parity_bit
Holland's schema theorem,"Holland's schema theorem, also called the fundamental theorem of genetic algorithms, is an inequality that results from coarse-graining an equation for evolutionary dynamics.  The Schema Theorem says that short, low-order schemata with above-average fitness increase exponentially in frequency in successive generations. The theorem was proposed by John Holland in the 1970s. It was initially widely taken to be the foundation for explanations of the power of genetic algorithms. However, this interpretation of its implications has been criticized in several publications reviewed in , where the Schema Theorem is shown to be a special case of the Price equation with the schema indicator function as the macroscopic measurement.  ",https://en.wikipedia.org/wiki/Holland%27s_schema_theorem
Theory of conjoint measurement,"The theory of conjoint measurement (also known as conjoint measurement or additive conjoint measurement) is a general, formal theory of continuous quantity. It was independently discovered by the French economist Gérard Debreu (1960) and by the American mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey 1964).",https://en.wikipedia.org/wiki/Theory_of_conjoint_measurement
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/0-387-95284-5
Elliptic Curve Digital Signature Algorithm,"In cryptography, the Elliptic Curve Digital Signature Algorithm (ECDSA) offers a variant of the Digital Signature Algorithm (DSA) which uses elliptic curve cryptography.",https://en.wikipedia.org/wiki/ECDSA
NetMiner,"NetMiner is an application software for exploratory analysis and visualization of large network data based on SNA(Social Network Analysis). It can be used for general research and teaching in social networks. This tool allows researchers to explore their network data visually and interactively, helps them to detect underlying patterns and structures of the network.It features data transformation, network analysis, statistics, visualization of network data, chart, and a programming language based on the Python script language. Also, it enables users to import unstructured text data(e.g. news, articles, tweets, etc.) and extract words and network from text data. In addition, NetMiner SNS Data Collector, an extension of NetMiner, can collect some social networking service(SNS) data with a few clicks.",https://en.wikipedia.org/wiki/NetMiner
GrowCut algorithm,"GrowCut is an interactive segmentation algorithm. It uses Cellular Automaton as an image model. Automata evolution models segmentation process. Each cell of the automata has some label (in case of binary segmentation - 'object', 'background' and 'empty'). During automata evolution some cells capture their neighbours, replacing their labels. ",https://en.wikipedia.org/wiki/GrowCut_algorithm
Soft independent modelling of class analogies,Soft independent modelling by class analogy (SIMCA) is a statistical method for supervised classification of data. The method requires a training data set consisting of samples (or objects) with a set of attributes and their class membership. The term soft refers to the fact the classifier can identify samples as belonging to multiple classes and not necessarily producing a classification of samples into non-overlapping classes.,https://en.wikipedia.org/wiki/Soft_independent_modelling_of_class_analogies
Relationship square,"In statistics, the relationship square is a graphical representation for use in the factorial analysis of a table individuals x variables. This representation completes classical representations provided by principal component analysis (PCA) or multiple correspondence analysis (MCA), namely those of individuals, of quantitative variables (correlation circle) and of the categories of qualitative variables (at the centroid of the individuals who possess them). It is especially important in factor analysis of mixed data (FAMD) and in multiple factor analysis (MFA).",https://en.wikipedia.org/wiki/Relationship_square
Learning vector quantization,"In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.",https://en.wikipedia.org/wiki/Learning_vector_quantization
AC-3 algorithm,"The AC-3 algorithm (short for Arc Consistency Algorithm #3) is one of a series of algorithms used for the solution of constraint satisfaction problems (or CSP's). It was developed by Alan Mackworth in 1977. The earlier AC algorithms are often considered too inefficient, and many of the later ones are difficult to implement, and so AC-3 is the one most often taught and used in very simple constraint solvers.",https://en.wikipedia.org/wiki/AC-3_algorithm
Exponential-Golomb coding,An exponential-Golomb code (or just Exp-Golomb code) is a type of universal code.,https://en.wikipedia.org/wiki/Exponential-Golomb_coding
Statistical classification,"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of pattern recognition.",https://en.wikipedia.org/wiki/Statistical_classification
Karl Steinbuch,"Karl W. Steinbuch (June 15, 1917 in Stuttgart-Bad Cannstatt – June 4, 2005 in Ettlingen) was a German computer scientist,  cyberneticist, and electrical engineer.  He was an early and influential researcher of German computer science, and was the developer of the Lernmatrix, an early implementation of artificial neural networks. Steinbuch also wrote about the societal implications of modern media.",https://en.wikipedia.org/wiki/Karl_Steinbuch
RapidMiner,"RapidMiner is a data science software platform developed by the company of the same name that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics. It is used for business and commercial applications as well as for research, education, training, rapid prototyping, and application development and supports all steps of the machine learning process including data preparation, results visualization, model validation and optimization. RapidMiner is developed on an open core model. The RapidMiner Studio Free Edition, which is limited to 1 logical processor and 10,000 data rows is available under the AGPL license, by depending on various non-opensource components. Commercial pricing starts at $5,000 and is available from the developer.",https://en.wikipedia.org/wiki/RapidMiner
Regula falsi,"In mathematics, the regula falsi, method of false position, or false position method is a very old method for solving an equation in one unknown, that, in modified form, is still in use. In simple terms, the method is the trial and error technique of using test (""false"") values for the variable and then adjusting the test value according to the outcome. This is sometimes also referred to as ""guess and check"". Versions of the method predate the advent of algebra and the use of equations.",https://en.wikipedia.org/wiki/False_position_method
LZ77 and LZ78,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978.They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG and ZIP.",https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv
Algorithmic inference,"Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966).The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.",https://en.wikipedia.org/wiki/Algorithmic_inference
ID3 algorithm,"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.",https://en.wikipedia.org/wiki/ID3_algorithm
Stepwise regression,"In statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Usually, this takes the form of a sequence of F-tests or t-tests, but other techniques are possible, such as adjusted R2, Akaike information criterion, Bayesian information criterion, Mallows's Cp, PRESS, or false discovery rate.",https://en.wikipedia.org/wiki/Stepwise_regression
Google Nest,,https://en.wikipedia.org/wiki/Nest_Labs
Apache Flume,"Apache Flume is a distributed, reliable, and available software for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.",https://en.wikipedia.org/wiki/Apache_Flume
LZ77 and LZ78,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978.They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG and ZIP.",https://en.wikipedia.org/wiki/LZ77_and_LZ78
Vincenty's formulae,"Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid, developed by Thaddeus Vincenty (1975a). They are based on the assumption that the figure of the Earth is an oblate spheroid, and hence are more accurate than methods that assume a spherical Earth, such as great-circle distance.",https://en.wikipedia.org/wiki/Vincenty%27s_formulae
Predictive Model Markup Language,"The Predictive Model Markup Language (PMML) is an XML-based predictive model interchange format conceived by Dr. Robert Lee Grossman, then the director of the National Center for Data Mining at the University of Illinois at Chicago. PMML provides a way for analytic applications to describe and exchange predictive models produced by data mining and machine learning algorithms. It supports common models such as logistic regression and feedforward neural networks. Version 0.9 was published in 1998.  Subsequent versions have been developed by the Data Mining Group.",https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language
Text mining,"According to Hotho et al. (2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process. Text mining is ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources can be websites, books, emails, reviews, articles.Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).",https://en.wikipedia.org/wiki/Text_mining
One-class classification,"In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, tries to identify objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. This is different from and more difficult than the traditional classification problem, which tries to distinguish between two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal': In this scenario, there are few, if any, examples of catastrophic system states; only the statistics of normal operation are known.",https://en.wikipedia.org/wiki/One-class_classification
Conference on Artificial General Intelligence,"The Conference on Artificial General Intelligence is a meeting of researchers in the field of Artificial General Intelligence organized by the AGI Society and held annually since 2008. The conference was initiated by the 2006 Bethesda Artificial General Intelligence Workshop and has been hosted at the University of Memphis (sponsored by the AAAI); Arlington, Virginina (sponsored by the AAAI and Ray Kurzweil's KurzweilAI.net); Lugano, Switzerland (In Memoriam Ray Solomonoff and sponsored by the AAAI and KurzweilAI); Google headquarters in Mountain View, California (sponsored by Google, Inc., the AAAI, and KurzweilAI); the University of Oxford, United Kingdom (sponsored by the Future of Humanity Institute and KurzweilAI); and at Peking University, Beijing, China (sponsored by the Cognitive Science Society and the AAAI), Quebec City, Canada (sponsored by the Cognitive Science Society and the AAAI). The AGI-15 conference was held in Berlin, Germany.",https://en.wikipedia.org/wiki/Conference_on_Artificial_General_Intelligence
Fisher–Yates shuffle,"The Fisher–Yates shuffle is an algorithm for generating a random permutation of a finite sequence—in plain terms, the algorithm shuffles the sequence.  The algorithm effectively puts all the elements into a hat; it continually determines the next element by randomly drawing an element from the hat until no elements remain. The algorithm produces an unbiased permutation: every permutation is equally likely.  The modern version of the algorithm is efficient: it takes time proportional to the number of items being shuffled and shuffles them in place.",https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle
Adaptive neuro fuzzy inference system,"An adaptive neuro-fuzzy inference system or adaptive network-based fuzzy inference system (ANFIS) is a kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm. It has uses in intelligent situational aware energy management system.",https://en.wikipedia.org/wiki/Adaptive_neuro_fuzzy_inference_system
Loss function,"In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.",https://en.wikipedia.org/wiki/Loss_function
Layered hidden Markov model,"The layered hidden Markov model (LHMM) is a statistical model derived from the hidden Markov model (HMM). A layered hidden Markov model (LHMM) consists of N levels of HMMs, where the HMMs on level i + 1 correspond to observation symbols or probability generators at level i.Every level i of the LHMM consists of Ki HMMs running in parallel.",https://en.wikipedia.org/wiki/Layered_hidden_Markov_model
First-order inductive learner,"In machine learning, first-order inductive learner (FOIL) is a rule-based learning algorithm.",https://en.wikipedia.org/wiki/First-order_inductive_learner
Round-robin scheduling,"Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing.As the term is generally used, time slices (also known as time quanta) are assigned to each process in equal portions and in circular order, handling all processes without priority (also known as cyclic executive). Round-robin scheduling is simple, easy to implement, and starvation-free. Round-robin scheduling can also be applied to other scheduling problems, such as data packet scheduling in computer networks. It is an operating system concept.",https://en.wikipedia.org/wiki/Round-robin_scheduling
Naive Bayes classifier,"In machine learning, naïve Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.",https://en.wikipedia.org/wiki/Gaussian_Naive_Bayes
Margin classifier,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example.  For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.",https://en.wikipedia.org/wiki/Margin_classifier
Genetic memory (computer science),"In computer science, genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory. It can be used to predict weather patterns. Genetic memory and genetic algorithms have also gained an interest in the creation of artificial life.",https://en.wikipedia.org/wiki/Genetic_memory_(computer_science)
Shane Legg,"Shane Legg CBE is a machine learning researcher and cofounder of DeepMind Technologies, acquired by Google in 2014.",https://en.wikipedia.org/wiki/Shane_Legg
Trigram search,"Trigram search is a method of searching for text when the exact syntax or spelling of the target object is not precisely known. It finds objects which match the maximum number of three-character strings in the entered search terms, i.e., near matches. A threshold can be specified as a cutoff point, after which a result is no longer regarded as a match.",https://en.wikipedia.org/wiki/Trigram_search
SimRank,"SimRank is a general similarity measure, based on a simple and intuitive graph-theoretic model.SimRank is applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects.Effectively, SimRank is a measure that says ""two objects are considered to be similar if they are referenced by similar objects."" Although SimRank is widely adopted, it may output unreasonable similarity scores which are influenced by different factors, and can be solved in several ways, such as introducing an evidence weight factor, inserting additional terms that are neglected by SimRank or using PageRank-based alternatives.",https://en.wikipedia.org/wiki/SimRank
Averaged one-dependence estimators,Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique.  It was developed to address the attribute-independence problem of the popular naive Bayes classifier.  It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.,https://en.wikipedia.org/wiki/Averaged_One-Dependence_Estimators
Relevance vector machine,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.",https://en.wikipedia.org/wiki/Relevance_Vector_Machine
FLAME clustering,Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.,https://en.wikipedia.org/wiki/FLAME_clustering
Elementary function,"In mathematics, an elementary function is a function of a single variable composed of particular simple functions. ",https://en.wikipedia.org/wiki/Elementary_function_(differential_algebra)
Lagrange polynomial,"In numerical analysis, Lagrange polynomials are used for polynomial interpolation. For a given set of points (xj,yj),y_{j})} with no two xj values equal, the Lagrange polynomial is the polynomial of lowest degree that assumes at each value xj, so that the functions coincide at each point.",https://en.wikipedia.org/wiki/Lagrange_polynomial
Deep learning,"Deep learning  (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.",https://en.wikipedia.org/wiki/Deep_learning
Multilinear principal component analysis,"Multilinear principal component analysis (MPCA)  is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a ""data tensor"".  N-way arrays may be decomposed, analyzed, or modeled by ",https://en.wikipedia.org/wiki/Multilinear_principal_component_analysis
Partially ordered set,"In mathematics, especially order theory, a partially ordered set (also poset) formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a set. A poset consists of a set together with a binary relation indicating that, for certain pairs of elements in the set, one of the elements precedes the other in the ordering. The relation itself is called a ""partial order."" The word partial in the names ""partial order"" and ""partially ordered set"" is used as an indication that not every pair of elements needs to be comparable. That is, there may be pairs of elements for which neither element precedes the other in the poset. Partial orders thus generalize total orders, in which every pair is comparable.  ",https://en.wikipedia.org/wiki/Partial_ordering
Gaussian adaptation,"Gaussian adaptation (GA) (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an evolutionary algorithm designed for the maximization of manufacturing yield due to statistical deviation of component values of signal processing systems. In short, GA is a stochastic adaptive process where a number of samples of an n-dimensional vector x[xT = (x1, x2, ..., xn)] are taken from a multivariate Gaussian distribution, N(m, M), having mean m and moment matrix M. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are m* and M*.",https://en.wikipedia.org/wiki/Gaussian_adaptation
Soundex,"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, SQLite, Ingres, MS SQL Server and Oracle.) Improvements to Soundex are the basis for many modern phonetic algorithms.",https://en.wikipedia.org/wiki/Soundex
Weighted majority algorithm (machine learning),"In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used  to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts.The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well.",https://en.wikipedia.org/wiki/Weighted_majority_algorithm_(machine_learning)
Perplexity,"In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.",https://en.wikipedia.org/wiki/Perplexity
Kernel principal component analysis,"In the field of multivariate statistics, kernel principal component analysis (kernel PCA)is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.",https://en.wikipedia.org/wiki/Kernel_principal_component_analysis
Rabin–Karp algorithm,"In computer science, the Rabin–Karp algorithm or Karp–Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern.",https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_string_search_algorithm
Defining length,"In genetic algorithms and genetic programming defining length L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in schema H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H.",https://en.wikipedia.org/wiki/Defining_length
Text simplification,"Text simplification is an operation used in natural language processing to modify, enhance,  classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain large vocabularies and complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts.",https://en.wikipedia.org/wiki/Text_simplification
Wake-sleep algorithm,"The wake-sleep algorithm is an unsupervised learning algorithm for a stochastic multilayer[clarification needed] neural network. The algorithm adjusts the parameters so as to produce a good density estimator. There are two learning phases, the “wake” phase and the “sleep” phase, which are performed alternately. It was first designed as a model for brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It can be viewed as a way to train a Helmholtz Machine. It can also be used in Deep Belief Networks(DBN).",https://en.wikipedia.org/wiki/Wake-sleep_algorithm
Lesk algorithm,The Lesk algorithm is a classical algorithm for word sense disambiguation introduced by Michael E. Lesk in 1986.,https://en.wikipedia.org/wiki/Lesk_algorithm
Kernel eigenvoice,"Speaker adaptation is an important technology to fine-tune either features or speech models for mis-match due to inter-speaker variation. In the last decade, eigenvoice (EV) speaker adaptation has been developed. It makes use of the prior knowledge of training speakers to provide a fast adaptation algorithm (in other words, only a small amount of adaptation data is needed). Inspired by the kernel eigenface idea in face recognition, kernel eigenvoice (KEV) is proposed. KEV is a non-linear generalization to EV. This incorporates Kernel principal component analysis, a non-linear version of Principal Component Analysis, to capture higher order correlations in order to further explore the speaker space and enhance recognition performance.",https://en.wikipedia.org/wiki/Kernel_eigenvoice
Symmetric-key algorithm,"Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption (also known as asymmetric key encryption).",https://en.wikipedia.org/wiki/Symmetric_key_algorithm
Reinforcement learning,"Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.",https://en.wikipedia.org/wiki/Reinforcement_Learning
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/Division_algorithm
Katia Sycara,"Katia Sycara (Greek: Κάτια Συκαρά) is a professor in the Robotics Institute, School of Computer Science at Carnegie Mellon University internationally known for her research in artificial intelligence, particularly in the fields of negotiation, autonomous agents and multi-agent systems. She directs the Advanced Agent-Robotics Technology Lab at Robotics Institute, Carnegie Mellon University. She also serves as academic advisor for PhD students at both Robotics Institute and Tepper School of Business.",https://en.wikipedia.org/wiki/Katia_Sycara
Topological sorting,"In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.",https://en.wikipedia.org/wiki/Topological_sorting
Learning vector quantization,"In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.",https://en.wikipedia.org/wiki/Learning_Vector_Quantization
Nagle's algorithm,Nagle's algorithm is a  means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network. It was defined by John Nagle while working for Ford Aerospace. It was published in 1984 as a Request for Comments (RFC) with title Congestion Control in IP/TCP Internetworks (see RFC 896).,https://en.wikipedia.org/wiki/Nagle%27s_algorithm
Semantic folding,Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.,https://en.wikipedia.org/wiki/Semantic_folding
F1 score,"In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. ",https://en.wikipedia.org/wiki/F1_score
De Bruijn graph,"In graph theory, an n-dimensional De Bruijn graph of m symbols is a directed graph representing overlaps between sequences of symbols. It has mn vertices, consisting of all possible length-n sequences of the given symbols; the same symbol may appear multiple times in a sequence.",https://en.wikipedia.org/wiki/De_Bruijn_graph
Pell's equation,,https://en.wikipedia.org/wiki/Pell%27s_equation
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/Non-restoring_division
Neighbor joining,"In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987. Usually used for trees based on DNA or protein sequence data, the algorithm requires knowledge of the distance between each pair of taxa (e.g., species or sequences) to form the tree.",https://en.wikipedia.org/wiki/Neighbor_joining
Genetic operator,"A genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful. Genetic operators are used to create and maintain genetic diversity (mutation operator), combine existing solutions (also known as chromosomes) into new solutions (crossover) and select between solutions (selection). In his book discussing the use of genetic programming for the optimization of complex problems, computer scientist John Koza has also identified an 'inversion' or 'permutation' operator; however, the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed.",https://en.wikipedia.org/wiki/Genetic_operator
Correlation clustering,Clustering is the problem of partitioning data points into groups based on their similarity.  Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.,https://en.wikipedia.org/wiki/Correlation_clustering
Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",https://en.wikipedia.org/wiki/Ensemble_learning
Discipline (academia),"An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part), and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. It includes language, art and cultural studies and other scientific disciplines.",https://en.wikipedia.org/wiki/Academic_discipline
Latent class model,"In statistics, a latent class model (LCM) relates a set of observed (usually discrete) multivariate variables to a set of latent variables. It is a type of latent variable model. It is called a latent class model because the latent variable is discrete. A class is characterized by a pattern of conditional probabilities that indicate the chance that variables take on certain values.",https://en.wikipedia.org/wiki/Latent_class_model
Parity learning,"Parity learning is a problem in machine learning. An algorithm that solves this problem must find a function ƒ, given some samples (x, ƒ(x)) and the assurance that ƒ computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.",https://en.wikipedia.org/wiki/Parity_learning
Michael L. Littman,"Michael Lederman Littman (born August 30, 1966) is a computer scientist. He works mainly in reinforcement learning, but has done work in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science at Brown University.",https://en.wikipedia.org/wiki/Michael_L._Littman
Metaphone,"Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar-sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.",https://en.wikipedia.org/wiki/Metaphone
Outline of robotics,The following outline is provided as an overview of and topical guide to robotics:,https://en.wikipedia.org/wiki/Outline_of_robotics
Data exploration,"Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.",https://en.wikipedia.org/wiki/Data_exploration
Artificial development,"Artificial development,  also known as artificial embryogeny or machine intelligence or computational development, is an area of computer science and engineering concerned with computational models motivated by genotype-phenotype mappings in biological systems. Artificial development is often considered a sub-field of evolutionary computation, although the principles of artificial development have also been used within stand-alone computational models.",https://en.wikipedia.org/wiki/Artificial_development
Programming with Big Data in R,"Programming with Big Data in R (pbdR) is a series of R packages and an environment for statistical computing with big data by using high-performance statistical computation. The pbdR uses the same programming language as R with S3/S4 classes and methods which is used among statisticians and data miners for developing statistical software. The significant difference between pbdR and R code is that pbdR mainly focuses on distributed memory systems, where data are distributed across several processors and analyzed in a batch mode, while communications between processors are based on MPI that is easily used in large high-performance computing (HPC) systems. R system mainly focuses[citation needed] on single multi-core machines for data analysis via an interactive mode such as GUI interface.",https://en.wikipedia.org/wiki/Programming_with_Big_Data_in_R
Isotropic position,"In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix. ",https://en.wikipedia.org/wiki/Isotropic_position
Soft output Viterbi algorithm,The soft output Viterbi algorithm (SOVA) is a variant of the classical Viterbi algorithm.,https://en.wikipedia.org/wiki/Soft_output_Viterbi_algorithm
Genetic algorithm,,https://en.wikipedia.org/wiki/Genetic_algorithms
Sinkov statistic,"Sinkov statistics, also known as log-weight statistics, is a specialized field of statistics that was developed by Abraham Sinkov, while working for the small Signal Intelligence Service organization, the primary mission of which was to compile codes and ciphers for use by the U.S. Army. The mathematics involved include modular arithmetic, a bit of number theory, some linear algebra of two dimensions with matrices, some combinatorics, and a little statistics. ",https://en.wikipedia.org/wiki/Sinkov_statistic
Heikki Mannila,"Heikki Mannila (born 1960) is a Finnish computer scientist, the president of the Academy of Finland.",https://en.wikipedia.org/wiki/Heikki_Mannila
Shannon–Fano coding,"In the field of data compression, Shannon–Fano coding, named after Claude Shannon and Robert Fano, is a name given to two different but related techniques for constructing a prefix code based on a set of symbols and their probabilities (estimated or measured).  ",https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding
Prime number,,https://en.wikipedia.org/wiki/Prime_number
Permutation group,"In mathematics, a permutation group is a group G whose elements are permutations of a given set M and whose group operation is the composition of permutations in G (which are thought of as bijective functions from the set M to itself). The group of all permutations of a set M is the symmetric group of M, often written as Sym(M). The term permutation group thus means a subgroup of the symmetric group. If M = {1,2,...,n} then, Sym(M), the symmetric group on n letters is usually denoted by Sn.",https://en.wikipedia.org/wiki/Permutation_group
Mark-compact algorithm,"In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. Mark-compact algorithms can be regarded as a combination of the mark-sweep algorithm and Cheney's copying algorithm. First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. Compacting garbage collection is used by Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler.",https://en.wikipedia.org/wiki/Mark-compact_algorithm
Bradley–Terry model,"The Bradley–Terry model is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as",https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model
Radiosity (computer graphics),"In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely.  Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code ""LD*E"") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.",https://en.wikipedia.org/wiki/Radiosity_(3D_computer_graphics)
Determining the number of clusters in a data set,"Determining the number of clusters in a data set, a quantity often labelled k as in the k-means algorithm, is a frequent problem in data clustering, and is a distinct issue from the process of actually solving the clustering problem.",https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set
Pop music automation,"Pop music automation is a field of study among musicians and computer scientists with a goal of producing successful pop music algorithmically. It is often based on the premise that pop music is especially formulaic, unchanging, and easy to compose. The idea of automating pop music composition is related to many ideas in algorithmic music, Artificial Intelligence (AI) and computational creativity.",https://en.wikipedia.org/wiki/Pop_music_automation
Machine Learning (journal),"Machine Learning  is a peer-reviewed scientific journal, published since 1986.It should be distinguished from the journal Machine intelligence which was established in the mid-1960s.",https://en.wikipedia.org/wiki/Machine_Learning_(journal)
Line clipping,"In computer graphics, line clipping is the process of removing lines or portions of lines outside an area of interest. Typically, any line or part thereof which is outside of the viewing area is removed.",https://en.wikipedia.org/wiki/Fast_clipping
DeepDream,"DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.",https://en.wikipedia.org/wiki/DeepDream
Bisection method,"In mathematics, the bisection method is a root-finding method that applies to any continuous functions for which one knows two values with opposite signs. The method consists of repeatedly bisecting the interval defined by these values and then selecting the  subinterval in which the function changes sign, and therefore must contain a root. It is a very simple and robust method, but it is also relatively slow. Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods. The method is also called the interval halving method, the binary search method, or the dichotomy method.",https://en.wikipedia.org/wiki/Bisection_method
Cognitive robotics,Cognitive robotics  is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.,https://en.wikipedia.org/wiki/Cognitive_robotics
Low-energy adaptive clustering hierarchy,"Low-energy adaptive clustering hierarchy (""LEACH"") is a TDMA-based MAC protocol which is integrated with clustering and a simple routing protocol in wireless sensor networks (WSNs). The goal of LEACH is to lower the energy consumption required to create and maintain clusters in order to improve the life time of a wireless sensor network.",https://en.wikipedia.org/wiki/Low-energy_adaptive_clustering_hierarchy
TD-Gammon,"TD-Gammon is a computer backgammon program developed in 1992 by Gerald Tesauro at IBM's Thomas J. Watson Research Center. Its name comes from the fact that it is an artificial neural net trained by a form of temporal-difference learning, specifically TD-lambda.",https://en.wikipedia.org/wiki/TD-Gammon
Hermite interpolation,"In numerical analysis, Hermite interpolation, named after Charles Hermite, is a method of interpolating data points as a polynomial function.  The generated Hermite interpolating polynomial is closely related to the Newton polynomial, in that both are derived from the calculation of divided differences. However, the Hermite interpolating polynomial may also be computed without using divided differences, see Chinese remainder theorem § Hermite interpolation.",https://en.wikipedia.org/wiki/Hermite_interpolation
Alberto Broggi,Alberto Broggi is General Manager at VisLab srl (spinoff of the University of Parma acquired by Silicon-Valley company Ambarella Inc. on June 2015) and  a professor of Computer Engineering at the University of Parma in Italy.,https://en.wikipedia.org/wiki/Alberto_Broggi
Biplot,"Biplots are a type of exploratory graph used in statistics, a generalization of the simple two-variable scatterplot. A biplot allows information on both samples and variables of a data matrix to be displayed graphically. Samples are displayed as points while variables are displayed either as vectors, linear axes or nonlinear trajectories. In the case of categorical variables, category level points may be used to represent the levels of a categorical variable. A generalised biplot displays information on both continuous and categorical variables.",https://en.wikipedia.org/wiki/Biplot
Large margin nearest neighbor,"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.",https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor
Speech recognition,"Speech recognition is an interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.",https://en.wikipedia.org/wiki/Speech_recognition
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/Newton%E2%80%93Raphson_division
Minimum bounding box algorithms,"In computational geometry, the smallest enclosing box problem is that of finding the oriented minimum bounding box enclosing a set of points. It is a type of bounding volume. ""Smallest"" may refer to volume, area, perimeter, etc. of the box.",https://en.wikipedia.org/wiki/Minimum_bounding_box_algorithms
Introsort,"Introsort or introspective sort is a hybrid sorting algorithm that provides both fast average performance and (asymptotically) optimal worst-case performance. It begins with quicksort, it switches to heapsort when the recursion depth exceeds a level based on (the logarithm of) the number of elements being sorted and it switches to insertion sort when the number of elements is below some threshold. This combines the good parts of the three algorithms, with practical performance comparable to quicksort on typical data sets and worst-case O(n log n) runtime due to the heap sort. Since the three algorithms it uses are comparison sorts, it is also a comparison sort.",https://en.wikipedia.org/wiki/Introsort
Adaptive-additive algorithm,"In the studies of Fourier optics, sound synthesis, stellar interferometry, optical tweezers, and diffractive optical elements (DOEs) it is often important to know the spatial frequency phase of an observed wave source. In order to reconstruct this phase the Adaptive-Additive Algorithm (or AA algorithm), which derives from a group of adaptive (input-output) algorithms, can be used. The AA algorithm is an iterative algorithm that utilizes the Fourier Transform to calculate an unknown part of a propagating wave, normally the spatial frequency phase (k space). This can be done when given the phase’s known counterparts, usually an observed amplitude (position space) and an assumed starting amplitude (k space). To find the correct phase the algorithm uses error conversion, or the error between the desired and the theoretical intensities.",https://en.wikipedia.org/wiki/Adaptive-additive_algorithm
Symbolic regression,,https://en.wikipedia.org/wiki/Symbolic_regression
Jaccard index,"The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally given the French name coefficient de communauté by Paul Jaccard), is a statistic used for gauging the similarity and diversity of sample sets.",https://en.wikipedia.org/wiki/Jaccard_index
Double dabble,"In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift-and-add-3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency.",https://en.wikipedia.org/wiki/Double_dabble
ilastik,,https://en.wikipedia.org/wiki/Ilastik
Right to explanation,"In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to an explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be ""Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.""",https://en.wikipedia.org/wiki/Right_to_explanation
Principal component analysis,"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.",https://en.wikipedia.org/wiki/Principal_component_analysis
Ziggurat algorithm,"The ziggurat algorithm is an algorithm for pseudo-random number sampling.  Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables.  The algorithm is used to generate values from a monotone decreasing probability distribution.  It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from.  It was developed by George Marsaglia and others in the 1960s.",https://en.wikipedia.org/wiki/Ziggurat_algorithm
GloVe (machine learning),"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is developed as an open-source project at Stanford. As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods.",https://en.wikipedia.org/wiki/GloVe_(machine_learning)
Category:Search algorithms,"This category has the following 5 subcategories, out of 5 total.",https://en.wikipedia.org/wiki/Category:Search_algorithms
Rule-based machine learning,"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.  The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.  This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]",https://en.wikipedia.org/wiki/Rule-based_machine_learning
Science,,https://en.wikipedia.org/wiki/Science
Binary classification,Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.,https://en.wikipedia.org/wiki/Binary_classifier
Evolver (software),"Evolver is a software package that allows users to solve a wide variety of optimization problems using a genetic algorithm. Launched in 1989, it was the first commercially available genetic algorithm package for personal computers. The program was originally developed by Axcelis, Inc. and is now owned by Palisade Corporation.",https://en.wikipedia.org/wiki/Evolver_(software)
Margin (machine learning),In machine learning the margin of a single data point is defined to be the distance from the data point to a decision boundary.  Note that there are many distances and decision boundaries that may be appropriate for certain datasets and goals.  A margin classifier is a classifier that explicitly utilizes the margin of each example while learning a classifier.  There are theoretical justifications (based on the VC dimension) as to why maximizing the margin (under some suitable constraints) may be beneficial for machine learning and statistical inferences algorithms.,https://en.wikipedia.org/wiki/Margin_(machine_learning)
"Training, validation, and test sets","In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.","https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets"
Expectation–maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",https://en.wikipedia.org/wiki/Expectation-maximization_algorithm
Bucket sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort,  a generalization of pigeonhole sort, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.",https://en.wikipedia.org/wiki/Postman_sort
Bézier curve,"A Bézier curve (/ˈbɛz.i.eɪ/ BEH-zee-ay) is a parametric curve used in computer graphics and related fields. The curve, which is related to the Bernstein polynomial, is named after Pierre Bézier, who used it in the 1960s for designing curves for the bodywork of Renault cars. Other uses include the design of computer fonts and animation. Bézier curves can be combined to form a Bézier spline, or generalized to higher dimensions to form Bézier surfaces. The Bézier triangle is a special case of the latter.",https://en.wikipedia.org/wiki/B%C3%A9zier_curve
Probably approximately correct learning,"In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.",https://en.wikipedia.org/wiki/Probably_approximately_correct_learning
SUBCLU,"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.",https://en.wikipedia.org/wiki/SUBCLU
Fürer's algorithm,"Fürer's algorithm is an integer multiplication algorithm for extremely large integers with very low asymptotic complexity. It was published in 2007 by the Swiss mathematician Martin Fürer of Pennsylvania State University as an asymptotically faster algorithm when analysed on a multitape Turing machine than its predecessor, the Schönhage–Strassen algorithm.",https://en.wikipedia.org/wiki/F%C3%BCrer%27s_algorithm
Antiderivative,"In calculus, an antiderivative, primitive function, primitive integral or indefinite integral[Note 1] of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as F′=f. The process of solving for antiderivatives is called antidifferentiation (or indefinite integration) and its opposite operation is called differentiation, which is the process of finding a derivative.",https://en.wikipedia.org/wiki/Antiderivatives
Top-down parsing,Top-down parsing in computer science is a parsing strategy where one first looks at the highest level of the parse tree and works down the parse tree by using the rewriting rules of a formal grammar. LL parsers are a type of parser that uses a top-down parsing strategy.,https://en.wikipedia.org/wiki/Top-down_parsing
Sum of absolute transformed differences,"The sum of absolute transformed differences (SATD) is a block matching criterion widely used in fractional motion estimation for video compression.  It works by taking a frequency transform, usually a Hadamard transform, of the differences between the pixels in the original block and the corresponding pixels in the block being used for comparison.  The transform itself is often of a small block rather than the entire macroblock.  For example, in x264, a series of 4×4 blocks are transformed rather than doing the more processor-intensive 16×16 transform.",https://en.wikipedia.org/wiki/Sum_of_absolute_transformed_differences
Integer factorization,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these factors are further restricted to prime numbers, the process is called prime factorization.",https://en.wikipedia.org/wiki/Integer_factorization
Functional principal component analysis,"Functional principal component analysis (FPCA) is a statistical method for investigating the dominant modes of variation of functional data. Using this method,  a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space L2 that consists of the  eigenfunctions of the autocovariance operator. FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions, the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, or in functional regression and classification.",https://en.wikipedia.org/wiki/Functional_principal_component_analysis
Importance sampling,"In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest.  It is related to umbrella sampling in computational physics. Depending on the application, the term may refer to the process of sampling from this alternative distribution, the process of inference, or both.",https://en.wikipedia.org/wiki/Importance_sampling
Rewriting,"In mathematics, computer science, and logic, rewriting covers a wide range of (potentially non-deterministic) methods of replacing subterms of a formula with other terms. The objects of focus for this article include rewriting systems (also known as rewrite systems, rewrite engines or reduction systems). In their most basic form, they consist of a set of objects, plus relations on how to transform those objects.",https://en.wikipedia.org/wiki/Rewriting
Floyd–Steinberg dithering,"Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors.",https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering
Comparison of deep-learning software,"The following table compares notable software frameworks, libraries and computer programs for deep learning.",https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software
Natural Language Toolkit,,https://en.wikipedia.org/wiki/Natural_Language_Toolkit
Fringe search,"In computer science, fringe search is a graph search algorithm that finds the least-cost path from a given initial node to one goal node.",https://en.wikipedia.org/wiki/Fringe_search
Ordinary least squares,"In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function.",https://en.wikipedia.org/wiki/Ordinary_least_squares_regression
Lossless compression,"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data.  By contrast, lossy compression permits reconstruction only of an approximation of the original data, though usually with greatly improved compression rates (and therefore reduced media sizes).",https://en.wikipedia.org/wiki/Lossless_data_compression
Feature engineering,Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself .,https://en.wikipedia.org/wiki/Feature_engineering
Jubatus,"Jubatus is an open-source online machine learning and distributed computing framework that is developed at Nippon Telegraph and Telephone and Preferred Infrastructure. Jubatus has many features like classification, recommendation, regression, anomaly detection, and graph mining.It supports many client languages C++, Java, Ruby, and Python.Jubatus uses Iterative Parameter Mixture for distributed machine learning.",https://en.wikipedia.org/wiki/Jubatus
Heart failure,,https://en.wikipedia.org/wiki/ESC_algorithm
Kernel perceptron,"In machine learning, the kernel perceptron is a variant of the popular perceptron learning algorithm that can learn kernel machines, i.e. non-linear classifiers that employ a kernel function to compute the similarity of unseen samples to training samples. The algorithm was invented in 1964, making it the first kernel classification learner.",https://en.wikipedia.org/wiki/Kernel_perceptron
Gradient boosting,"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.",https://en.wikipedia.org/wiki/Gradient_boosted_decision_tree
Clique (graph theory),"In the mathematical area of graph theory, a clique (/ˈkliːk/ or /ˈklɪk/) is a subset of vertices of an undirected graph such that  every two distinct vertices in the clique are adjacent; that is, its induced subgraph is complete. Cliques are one of the basic concepts of graph theory and are used in many other mathematical problems and constructions on graphs. Cliques have also been studied in computer science: the task of finding whether there is a clique of a given size in a graph (the clique problem) is NP-complete, but despite this hardness result, many algorithms for finding cliques have been studied.",https://en.wikipedia.org/wiki/Maximum_clique
Preference learning,"Preference learning is a subfield in machine learning, which is a classification method based on observed preference information . In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.",https://en.wikipedia.org/wiki/Preference_learning
Bernoulli scheme,,https://en.wikipedia.org/wiki/Bernoulli_scheme
LanguageWare,"LanguageWare is a natural language processing (NLP) technology developed by IBM, which allows applications to process natural language text. It comprises a set of Java libraries which provide a range of NLP functions: language identification, text segmentation/tokenization, normalization, entity and relationship extraction, and semantic analysis and disambiguation. The analysis engine uses Finite State Machine approach at multiple levels, which aids its performance characteristics, while maintaining a reasonably small footprint.",https://en.wikipedia.org/wiki/LanguageWare
Schönhage–Strassen algorithm,"The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Schönhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation, O(n⋅log⁡n⋅log⁡log⁡n)  for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 2n+1 elements, a specific type of number theoretic transform.",https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm
Stanford University,,https://en.wikipedia.org/wiki/Stanford_University
Ian Goodfellow,"Ian J. Goodfellow (born 1985 or 1986) is a researcher working in machine learning, currently employed at Apple Inc. as its director of machine learning in the Special Projects Group. He was previously employed as a research scientist at Google Brain. He has made several contributions to the field of deep learning.",https://en.wikipedia.org/wiki/Ian_Goodfellow
Branch and bound,"Branch and bound (BB, B&B, or BnB) is an algorithm  design paradigm for discrete and combinatorial optimization problems, as well as mathematical optimization. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.",https://en.wikipedia.org/wiki/Branch_and_bound
Neural Computation (journal),"Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies). According to the Journal Citation Reports, the journal has a 2014 impact factor of 2.207.",https://en.wikipedia.org/wiki/Neural_Computation_(journal)
Sort-merge join,The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.,https://en.wikipedia.org/wiki/Sort-Merge_Join
Nearest neighbor search,"Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.",https://en.wikipedia.org/wiki/Nearest_neighbor_search
Jerome H. Friedman,"Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.",https://en.wikipedia.org/wiki/Jerome_H._Friedman
k q-flats,"In data mining and machine learning, k-flat, where q is a given integer.",https://en.wikipedia.org/wiki/K_q-flats
Ukkonen's algorithm,"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.",https://en.wikipedia.org/wiki/Ukkonen%27s_algorithm
Tree traversal,"In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well.",https://en.wikipedia.org/wiki/Tree_traversal
Shortest common supersequence problem,"In computer science, the shortest common supersequence of two sequences X and Y is the shortest sequence which has X and Y as subsequences. This is a problem closely related to the longest common subsequence problem. Given two sequences X = < x1,...,xm > and Y = < y1,...,yn >, a sequence U = < u1,...,uk > is a common supersequence of X and Y if items can be removed from U to produce X or Y.",https://en.wikipedia.org/wiki/Shortest_common_supersequence_problem
String kernel,"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be.",https://en.wikipedia.org/wiki/String_kernel
Package-merge algorithm,"The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L.  It is a greedy algorithm, and a generalization of Huffman's original algorithm.  Package-merge works by reducing the code construction problem to the binary coin collector's problem.",https://en.wikipedia.org/wiki/Package-merge_algorithm
Formal concept analysis,"Formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s.",https://en.wikipedia.org/wiki/Formal_concept_analysis
Constraint satisfaction,"In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution to a set of constraints that impose conditions that the variables must satisfy.  A solution is therefore a set of values for the variables that satisfies all constraints—that is, a point in the feasible region.",https://en.wikipedia.org/wiki/Constraint_satisfaction
Switching Kalman filter,"The switching Kalman filtering (SKF) method is a variant of the Kalman filter. In its generalised form, it is often attributed to Kevin P. Murphy, but related switching state-space models have been in use.",https://en.wikipedia.org/wiki/Switching_Kalman_filter
Dlib,"Dlib is a general purpose cross-platform software library written in the programming language C++. Its design is heavily influenced by ideas from design by contract and component-based software engineering. Thus it is, first and foremost, a set of independent software components. It is open-source software released under a Boost Software License.",https://en.wikipedia.org/wiki/Dlib
Fatigue (material),"In materials science, fatigue is the weakening of a material caused by cyclic loading that results in progressive and localized structural damage and the growth of cracks. Once a fatigue crack has initiated, each loading cycle will grow the crack a small amount, typically producing striations on some parts of the fracture surface. The crack will continue to grow until it reaches a critical size, which occurs when the stress intensity factor of the crack exceeds the fracture toughness of the material, producing rapid propagation and typically complete fracture of the structure.",https://en.wikipedia.org/wiki/Fatigue_(material)
Maximum flow problem,"In optimization theory, maximum flow problems involve finding a feasible flow through a flow network that obtains the maximum possible flow rate.",https://en.wikipedia.org/wiki/Maximum_flow
Nested sampling algorithm,The nested sampling algorithm is a computational approach to the Bayesian statistics problems of comparing models and generating samples from posterior distributions. It was developed in 2004 by physicist John Skilling.,https://en.wikipedia.org/wiki/Nested_sampling_algorithm
Damm algorithm,"In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. It was presented by H. Michael Damm in 2004.",https://en.wikipedia.org/wiki/Damm_algorithm
NTRUEncrypt,"The NTRUEncrypt public key cryptosystem, also known as the NTRU encryption algorithm, is a lattice-based alternative to RSA and ECC and is based on the shortest vector problem in a lattice (which is not known to be breakable using quantum computers).",https://en.wikipedia.org/wiki/NTRUEncrypt
Factor analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ""error"" terms. Factor analysis aims to find independent latent variables.",https://en.wikipedia.org/wiki/Higher-order_factor_analysis
Stochastic matrix,"In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain.  Each of its entries is a nonnegative real number representing a probability.:9-11   It is also called a probability matrix, transition matrix, substitution matrix, or Markov matrix.:9-11",https://en.wikipedia.org/wiki/Stochastic_matrix
A-law algorithm,"An A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, i.e. modify, the dynamic range of an analog signal for digitizing. It is one of two versions of the G.711 standard from ITU-T, the other version being the similar μ-law, used in North America and Japan.",https://en.wikipedia.org/wiki/A-law_algorithm
Clique (graph theory),"In the mathematical area of graph theory, a clique (/ˈkliːk/ or /ˈklɪk/) is a subset of vertices of an undirected graph such that  every two distinct vertices in the clique are adjacent; that is, its induced subgraph is complete. Cliques are one of the basic concepts of graph theory and are used in many other mathematical problems and constructions on graphs. Cliques have also been studied in computer science: the task of finding whether there is a clique of a given size in a graph (the clique problem) is NP-complete, but despite this hardness result, many algorithms for finding cliques have been studied.",https://en.wikipedia.org/wiki/Clique_(graph_theory)
Confusion matrix,"Sources: Fawcett (2006), Powers (2011), Ting (2011), and CAWCR Chicco & Jurman (2020).",https://en.wikipedia.org/wiki/Confusion_matrix
Geohash,"Geohash is a public domain geocode system  invented in 2008 by Gustavo Niemeyer and (similar work in 1966) G.M. Morton, which encodes a geographic location into a short string of letters and digits. It is a hierarchical spatial data structure which subdivides space into buckets of grid shape, which is one of the many applications of what is known as a Z-order curve, and generally space-filling curves.",https://en.wikipedia.org/wiki/Geohash
Hierarchical clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters.",https://en.wikipedia.org/wiki/Hierarchical_clustering
Linear classifier,"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.",https://en.wikipedia.org/wiki/Linear_classifier
Arnoldi iteration,"In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method.  Arnoldi finds an approximation to the eigenvalues and eigenvectors of general (possibly non-Hermitian) matrices by constructing an orthonormal basis of the Krylov subspace, which makes it particularly useful when dealing with large sparse matrices.",https://en.wikipedia.org/wiki/Arnoldi_iteration
Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",https://en.wikipedia.org/wiki/Ensembles_of_classifiers
Heapsort,,https://en.wikipedia.org/wiki/Heapsort
Knight's tour,"A knight's tour is a sequence of moves of a knight on a chessboard such that the knight visits every square exactly once. If the knight ends on a square that is one knight's move from the beginning square (so that it could tour the board again immediately, following the same path), the tour is closed; otherwise, it is open.",https://en.wikipedia.org/wiki/Warnsdorff%27s_rule
Thomas G. Dietterich,,https://en.wikipedia.org/wiki/Thomas_G._Dietterich
PyTorch,"PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab (FAIR). It is free and open-source software released under the Modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.",https://en.wikipedia.org/wiki/PyTorch
Happened-before,"In computer science, the happened-before relation (denoted: →) is a relation between the result of two events, such that if one event should happen before another event, the result must reflect that, even if those events are in reality executed out of order (usually to optimize program flow). This involves ordering events based on the potential causal relationship of pairs of events in a concurrent system, especially asynchronous distributed systems. It was formulated by Leslie Lamport. In Java specifically, a happens-before relationship is a guarantee that memory written to by statement A is visible to statement B, that is, that statement A completes its write before statement B starts its read.",https://en.wikipedia.org/wiki/Lamport_ordering
Q methodology,"Q Methodology is a research method used in psychology and in social sciences to study people's ""subjectivity""—that is, their viewpoint. Q was developed by psychologist William Stephenson. It has been used both in clinical settings for assessing a patient's progress over time (intra-rater comparison), as well as in research settings to examine how people think about a topic (inter-rater comparisons).",https://en.wikipedia.org/wiki/Q_methodology
tf–idf,,https://en.wikipedia.org/wiki/Term_frequency%E2%80%93inverse_document_frequency
John D. Lafferty,"John D. Lafferty is an American scientist, Professor at Yale University and leading researcher in machine learning. He is best known for proposing the Conditional Random Fields with Andrew McCallum and Fernando C.N. Pereira.",https://en.wikipedia.org/wiki/John_D._Lafferty
European Conference on Artificial Intelligence,"The biennial European Conference on Artificial Intelligence (ECAI) is the leading conference in the field of Artificial Intelligence in Europe, and is commonly listed together with IJCAI and AAAI as one of the three major general AI conferences worldwide. The conference series has been held without interruption since 1974, originally under the name AISB.",https://en.wikipedia.org/wiki/European_Conference_on_Artificial_Intelligence
Bitap algorithm,"The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm. The algorithm tells whether a given text contains a substring which is ""approximately equal"" to a given pattern, where approximate equality is defined in terms of Levenshtein distance –  if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. The algorithm  begins by precomputing a set of bitmasks containing one bit for each element of the pattern. Then it is able to do most of the work with bitwise operations, which are extremely fast.",https://en.wikipedia.org/wiki/Bitap_algorithm
Voronoi diagram,"In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. In the simplest case, these objects are just finitely many points in the plane (called seeds, sites, or generators). For each seed there is a corresponding region consisting of all points of the plane closer to that seed than to any other. These regions are called Voronoi cells. The Voronoi diagram of a set of points is dual to its Delaunay triangulation.",https://en.wikipedia.org/wiki/Voronoi_diagram
Dinic's algorithm,"Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz. The algorithm runs in O(V2E) time and is similar to the Edmonds–Karp algorithm, which runs in O(VE2) time, in that it uses shortest augmenting paths. The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance.",https://en.wikipedia.org/wiki/Dinic%27s_algorithm
Probit model,"In statistics, a probit model is a type of regression where the dependent variable can take only two values, for example married or not married. The word is a portmanteau, coming from probability + unit. The purpose of the model is to estimate the probability that an observation with particular characteristics will fall into a specific one of the categories; moreover, classifying observations based on their predicted probabilities is a type of binary classification model.",https://en.wikipedia.org/wiki/Probit_model
Forward–backward algorithm,"The forward–backward algorithm is an  inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions o1:T:=o1,…,oT,\dots ,o_{T}}, i.e. it computes, for all hidden state variables Xt∈{X1,…,XT},\dots ,X_{T}\}}, the distribution P(Xt | o1:T). This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.",https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm
Smoothsort,"In computer science, smoothsort is a comparison-based sorting algorithm. A variant of heapsort, it was invented and published by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort.[self-published source?] The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.",https://en.wikipedia.org/wiki/Smoothsort
Ordered dithering,"Ordered dithering is an image dithering algorithm. It is commonly used to display a continuous image on a display of smaller color depth. For example, Microsoft Windows uses it in 16-color graphics modes. The algorithm is characterized by noticeable crosshatch patterns in the result.",https://en.wikipedia.org/wiki/Ordered_dithering
Activation function,"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes.",https://en.wikipedia.org/wiki/Activation_function
KNIME,"KNIME (/naɪm/), the Konstanz Information Miner, is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.",https://en.wikipedia.org/wiki/KNIME
Averaged one-dependence estimators,Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique.  It was developed to address the attribute-independence problem of the popular naive Bayes classifier.  It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.,https://en.wikipedia.org/wiki/Averaged_one-dependence_estimators
Fibonacci coding,"In mathematics and computing, Fibonacci coding is a universal code[citation needed] which encodes positive integers into binary code words. It is one example of representations of integers based on Fibonacci numbers. Each code word ends with ""11"" and contains no other instances of ""11"" before the end.",https://en.wikipedia.org/wiki/Fibonacci_coding
List scheduling,"The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following steps until a valid schedule is obtained :",https://en.wikipedia.org/wiki/List_scheduling
DPLL algorithm,"In logic and computer science, the Davis–Putnam–Logemann–Loveland (DPLL) algorithm is a complete, backtracking-based search algorithm for deciding the satisfiability of propositional logic formulae in conjunctive normal form, i.e. for solving the CNF-SAT problem.",https://en.wikipedia.org/wiki/DPLL_algorithm
Knuth–Bendix completion algorithm,"The Knuth–Bendix completion algorithm (named after Donald Knuth and Peter Bendix) is a semi-decision algorithm for transforming a set of equations (over terms) into a confluent term rewriting system.  When the algorithm succeeds, it effectively solves the word problem for the specified algebra.",https://en.wikipedia.org/wiki/Knuth%E2%80%93Bendix_completion_algorithm
Pollard's rho algorithm,"Pollard's rho algorithm is an algorithm for integer factorization. It was invented by John Pollard in 1975. It uses only a small amount of space, and its expected running time is proportional to the square root of the size of the smallest prime factor of the composite number being factorized.",https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm
Cyclic redundancy check,,https://en.wikipedia.org/wiki/Cyclic_redundancy_check
Parity benchmark,Parity problems are widely used as benchmark problems in genetic programming but inherited from the artificial neural network community. Parity is calculated by summing all the binary inputs and reporting if the sum is odd or even.,https://en.wikipedia.org/wiki/Parity_benchmark
TrustRank,TrustRank is an algorithm that conducts link analysis to separate useful webpages from spam and helps search engine rank pages in SERPs (Search Engine Results Pages). It is semi-automated process which means that it needs some human assistance in order to function properly. Search engines have many different algorithms and ranking factors that they use when measuring the quality of webpages. TrustRank is one of them. ,https://en.wikipedia.org/wiki/TrustRank
Andrew Ng,,https://en.wikipedia.org/wiki/Andrew_Ng
Truncated binary encoding,Truncated binary encoding is an entropy encoding typically used for uniform probability distributions with a finite alphabet. It is parameterized by an alphabet with total size of number n. It is a slightly more general form of binary encoding when n is not a power of two.,https://en.wikipedia.org/wiki/Truncated_binary_encoding
Shor's algorithm,"Shor's algorithm is a polynomial-time quantum computer algorithm for integer factorization. Informally, it solves the following problem: Given an integer N, find its prime factors. It was invented in 1994 by the American mathematician Peter Shor.",https://en.wikipedia.org/wiki/Shor%27s_algorithm
Inauthentic text,"An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless.  Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs.  They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.",https://en.wikipedia.org/wiki/Inauthentic_text
Matrix chain multiplication,"Matrix chain multiplication (or Matrix Chain Ordering Problem, MCOP) is an optimization problem that can be solved using dynamic programming.  Given a sequence of matrices, the goal is to find the most efficient way to multiply these matrices. The problem is not actually to perform the multiplications, but merely to decide the sequence of the matrix multiplications involved.",https://en.wikipedia.org/wiki/Chain_matrix_multiplication
Knuth's Algorithm X,"""Algorithm X"" is the name Donald Knuth used in his paper ""Dancing Links"" to refer to ""the most obvious trial-and-error approach"" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm. While Algorithm X is generally useful as a succinct explanation of how the exact cover problem may be solved, Knuth's intent in presenting it was merely to demonstrate the utility of the dancing links technique via an efficient implementation he called DLX.",https://en.wikipedia.org/wiki/Algorithm_X
Introselect,"In computer science, introselect (short for ""introspective selection"") is a selection algorithm that is a hybrid of quickselect and median of medians which has fast average performance and optimal worst-case performance. Introselect is related to the introsort sorting algorithm: these are analogous refinements of the basic quickselect and quicksort algorithms, in that they both start with the quick algorithm, which has good average performance and low overhead, but fall back to an optimal worst-case algorithm (with higher overhead) if the quick algorithm does not progress rapidly enough. Both algorithms were introduced by David Musser in (Musser 1997), with the purpose of providing generic algorithms for the C++ Standard Library that have both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened. However, in most C++ Standard Library implementations that use introselect, another ""introselect"" algorithm is used, which combines quickselect and heapselect, and has a worst-case running time of O(n log n).",https://en.wikipedia.org/wiki/Introselect
scrypt,,https://en.wikipedia.org/wiki/Scrypt
Social engineering (security),,https://en.wikipedia.org/wiki/Social_engineering_(security)
Prisma (app),Prisma is a photo-editing mobile application that uses neural networks and artificial intelligence to apply artistic effects to transform images.,https://en.wikipedia.org/wiki/Prisma_(app)
Learnable function class,"In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.",https://en.wikipedia.org/wiki/Learnable_function_class
Rada Mihalcea,"Rada Mihalcea is a professor of computer science and engineering at the University of Michigan. Her research focuses on natural language processing, multimodal processing, and computational social science.",https://en.wikipedia.org/wiki/Rada_Mihalcea
Trigram tagger,"In computational linguistics, a trigram tagger is a statistical method for automatically identifying words as being nouns, verbs, adjectives, adverbs, etc. based on second order Markov models that consider triples of consecutive words.  It is trained on a text corpus as a method to predict the next word, taking the product of the probabilities of unigram, bigram and trigram. In speech recognition, algorithms utilizing trigram-tagger score better than those algorithms utilizing IIMM tagger but less well than Net tagger.",https://en.wikipedia.org/wiki/Trigram_tagger
SPSS Modeler,"IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming. One of its main aims from the outset was to get rid of unnecessary complexity in data transformations, and to make complex predictive models very easy to use. The first version incorporated decision trees (ID3), and neural networks (backprop), which could both be trained without underlying knowledge of how those techniques worked.",https://en.wikipedia.org/wiki/SPSS_Modeler
Probabilistic soft logic,"Probabilistic soft logic (PSL) is a SRL framework for collective, probabilistic reasoning in relational domains.PSL uses first order logic rules as a template language for graphical models over random variables with soft truth values from the interval [0,1].",https://en.wikipedia.org/wiki/Probabilistic_soft_logic
Feature scaling,"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.",https://en.wikipedia.org/wiki/Feature_scaling
Local tangent space alignment,"Local tangent space alignment (LTSA) is a method for manifold learning, which can efficiently learn a nonlinear embedding into low-dimensional coordinates from high-dimensional data, and can also reconstruct high-dimensional coordinates from embedding coordinates.  It is based on the intuition that when a manifold is correctly unfolded, all of the tangent hyperplanes to the manifold will become aligned.  It begins by computing the k-nearest neighbors of every point.  It computes the tangent space at every point by computing the d-first principal components in each local neighborhood.  It then optimizes to find an embedding that aligns the tangent spaces, but it ignores the label information conveyed by data samples, and thus can not be used for classification directly.",https://en.wikipedia.org/wiki/Local_tangent_space_alignment
"Quick, Draw!","Quick, Draw! is an online game developed by Google that challenges players to draw a picture of an object or idea and then uses a neural network artificial intelligence to guess what the drawings represent. The AI learns from each drawing, increasing its ability to guess correctly in the future. The game is similar to Pictionary in that the player only has a limited time to draw (20 seconds). The concepts that it guesses can be simple, like 'foot', or more complicated, like 'animal migration'. This game is one of many simple games created by Google that are AI based as part of a project known as 'A.I. Experiments'.","https://en.wikipedia.org/wiki/Quick,_Draw!"
Girvan–Newman algorithm,The Girvan–Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems.,https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm
Powerset construction,"In the theory of computation and automata theory, the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language. It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to 2n states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs.",https://en.wikipedia.org/wiki/Powerset_construction
Stochastic cellular automaton,"Stochastic cellular automata or probabilistic cellular automata (PCA) or random cellular automata or locally interacting Markov chains are an important extension of cellular automaton. Cellular automata are a discrete-time dynamical system of interacting entities, whose state is discrete.",https://en.wikipedia.org/wiki/Stochastic_cellular_automaton
Neuroevolution,"Neuroevolution, or neuro-evolution, is a form of  artificial intelligence that uses evolutionary algorithms to generate artificial neural networks (ANN), parameters, topology and rules. It is most commonly applied in artificial life, general game playing and evolutionary robotics. The main benefit is that neuroevolution can be applied more widely than supervised learning algorithms, which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies. Neuroevolution is commonly used as part of the reinforcement learning paradigm, and it can be contrasted with conventional deep learning techniques that use gradient descent on a neural network with a fixed topology.",https://en.wikipedia.org/wiki/Neuroevolution
Gremlin (programming language),"Gremlin is a graph traversal language and virtual machine developed by Apache TinkerPop of the Apache Software Foundation. Gremlin works for both OLTP-based graph databases as well as OLAP-based graph processors. Gremlin's automata and functional language foundation enable Gremlin to naturally support imperative and declarative querying, host language agnosticism, user-defined domain specific languages, an extensible compiler/optimizer, single- and multi-machine execution models, hybrid depth- and breadth-first evaluation, as well as Turing Completeness.",https://en.wikipedia.org/wiki/Gremlin_(programming_language)
Template talk:Machine learning bar,"This section title and contents seem pretty much random to me. How are contents chosen? One regression, one random clustering algorithm, 4 standard classificators; but no decision tree; which is probably the grandfather of all classificators. --Chire (talk) 12:41, 22 October 2013 (UTC)",https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar
Structural equation modeling,"Structural equation modeling (SEM) includes a diverse set of mathematical models, computer algorithms, and statistical methods that fit networks of constructs to data. SEM includes confirmatory factor analysis, confirmatory composite analysis, path analysis, partial least squares path modeling, and latent growth modeling. The concept should not be confused with the related concept of structural models in econometrics, nor with structural models in economics. Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables. The links between constructs of a structural equation model may be estimated with independent regression equations or through more involved approaches such as those employed in LISREL.",https://en.wikipedia.org/wiki/Structural_equation_modeling
Leave-one-out error,"∀i∈{1,...,m},PS{supz∈Z|V(fS,zi)−V(fS|i,zi)|≤βCV}≥1−δCV{\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|\leq \beta _{CV}\}\geq 1-\delta _{CV}}",https://en.wikipedia.org/wiki/Leave-one-out_error
Lagrange polynomial,"In numerical analysis, Lagrange polynomials are used for polynomial interpolation. For a given set of points (xj,yj),y_{j})} with no two xj values equal, the Lagrange polynomial is the polynomial of lowest degree that assumes at each value xj, so that the functions coincide at each point.",https://en.wikipedia.org/wiki/Lagrange_interpolation
Gift wrapping algorithm,"In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points.",https://en.wikipedia.org/wiki/Gift_wrapping_algorithm
Jacobi eigenvalue algorithm,"In numerical linear algebra, the Jacobi eigenvalue algorithm is an iterative method for the calculation of the eigenvalues and eigenvectors of a real symmetric matrix (a process known as diagonalization). It is named after Carl Gustav Jacob Jacobi, who first proposed the method in 1846, but only became widely used in the 1950s with the advent of computers.",https://en.wikipedia.org/wiki/Jacobi_eigenvalue_algorithm
Common-method variance,"In applied statistics, (e.g., applied to the social sciences and psychometrics), common-method variance  (CMV) is the spurious ""variance that is attributable to the measurement method rather than to the constructs the measures are assumed to represent"" or equivalently as ""systematic error variance shared among variables measured with and introduced as a function of the same method and/or source"". For example, an electronic survey method might influence results for those who might be unfamiliar with an electronic survey interface differently than for those who might be familiar. If measures are affected by CMV or common-method bias, the intercorrelations among them can be inflated or deflated depending upon several factors. Although it is sometimes assumed that CMV affects all variables, evidence suggests that whether or not the correlation between two variables is affected by CMV is a function of both the method and the particular constructs being measured.",https://en.wikipedia.org/wiki/Common-method_variance
Representer theorem,"In statistical learning theory, a representer theorem is any of several related results stating that a minimizer f∗ of a regularized empirical risk functional defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.",https://en.wikipedia.org/wiki/Representer_theorem
Fitness proportionate selection,"Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.",https://en.wikipedia.org/wiki/Fitness_proportionate_selection
Linear interpolation,"In mathematics, linear interpolation is a method of curve fitting using linear polynomials to construct new data points within the range of a discrete set of known data points.",https://en.wikipedia.org/wiki/Linear_interpolation
Breadth-first search,"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.",https://en.wikipedia.org/wiki/Breadth-first_search
Bayesian network,"A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.",https://en.wikipedia.org/wiki/Bayesian_network
Template talk:Graph search algorithm,"Since all of the algorithms in this box can be applied (with slight modifications) to general graphs, and are often used in this way, I move that this box be renamed and retitled ""Graph search algorithms"" and that a summary article be created at Graph search algorithm. Tree traversal discusses some of these, but I think a more general treatment that discusses commonalities of depth-first search, breadth-first search, A* search, etc. in a central place is in order. The existence of this box suggests the need for such a ""main article."" What do you think? Deco 00:20, 20 April 2006 (UTC)",https://en.wikipedia.org/wiki/Template_talk:Graph_search_algorithm
Discrete logarithm,"In the mathematics of the real numbers, the logarithm logb a is a number x such that bx = a, for given numbers a and b. Analogously, in any group G, powers bk can be defined for all integers k, and the discrete logarithm logb a is an integer k such that bk = a. In number theory, the more commonly used term is index: we can write x = indr a (mod m) (read the index of a to the base r modulo m) for rx ≡ a (mod m) if r is a primitive root of m and gcd(a,m) = 1.",https://en.wikipedia.org/wiki/Discrete_logarithm
Concept learning,"Concept learning, also known as category learning, concept attainment, and concept formation, is defined by Bruner, Goodnow, & Austin (1967) as ""the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories"". More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features. ",https://en.wikipedia.org/wiki/Concept_learning
Taguchi loss function,"The Taguchi loss function is  graphical depiction of loss developed by the Japanese business statistician Genichi Taguchi to describe a phenomenon affecting the value of products produced by a company.  Praised by Dr. W. Edwards Deming (the business guru of the 1980s American quality movement), it made clear the concept that quality does not suddenly plummet when, for instance, a machinist exceeds a rigid blueprint tolerance.  Instead 'loss' in value progressively increases as variation increases from the intended condition.  This was considered a breakthrough in describing quality, and helped fuel the continuous improvement movement that since has become known as lean manufacturing.",https://en.wikipedia.org/wiki/Taguchi_loss_function
Tiny Encryption Algorithm,"In cryptography, the Tiny Encryption Algorithm (TEA) is a block cipher notable for its simplicity of description and implementation, typically a few lines of code. It was designed by David Wheeler and Roger Needham of the Cambridge Computer Laboratory; it was first  presented at the Fast Software Encryption workshop in Leuven in 1994, and first published in the proceedings of that workshop.",https://en.wikipedia.org/wiki/Tiny_Encryption_Algorithm
Dixon's factorization method,"In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method. Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.",https://en.wikipedia.org/wiki/Dixon%27s_algorithm
Sepp Hochreiter,"Sepp Hochreiter (born Josef Hochreiter in 1967) is a German computer scientist. Since 2018 he has led the Institute for Machine Learning at the Johannes Kepler University of Linz after having led the Institute of Bioinformatics from 2006 to 2018. In 2017 he became the head of the Linz Institute of Technology (LIT) AI Lab which focuses on advancing research on artificial intelligence. Previously, he was at the Technical University of Berlin, at the University of Colorado at Boulder, and at the Technical University of Munich.",https://en.wikipedia.org/wiki/Sepp_Hochreiter
Radial basis function kernel,"In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification.",https://en.wikipedia.org/wiki/Radial_basis_function_kernel
Pancake sorting,,https://en.wikipedia.org/wiki/Pancake_sorting
Trevor Hastie,"Trevor John Hastie (born 27 June 1953) is a South African and American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University. Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.",https://en.wikipedia.org/wiki/Trevor_Hastie
Bio-inspired computing,"Bio-inspired computing, short for biologically inspired computing, is a field of study which seeks to solve computer science problems using models of biology. It relates to connectionism, social behavior, and emergence. Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. Bio-inspired computing is a major subset of natural computation.",https://en.wikipedia.org/wiki/Bio-inspired_computing
Open Neural Network Exchange,The Open Neural Network Exchange (ONNX) is an open-source artificial intelligence ecosystem. ONNX is available on GitHub.,https://en.wikipedia.org/wiki/Onnx
Simultaneous localization and mapping,"In navigation, robotic mapping and odometry for virtual reality or augmented reality, simultaneous localization and mapping (SLAM) is the computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it. While this initially appears to be a chicken-and-egg problem there are several algorithms known for solving it, at least approximately, in tractable time for certain environments. Popular approximate solution methods include the particle filter, extended Kalman filter, Covariance intersection, and GraphSLAM.",https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping
Biconjugate gradient method,"In mathematics, more specifically in numerical linear algebra, the biconjugate gradient method is an algorithm to solve systems of linear equations",https://en.wikipedia.org/wiki/Biconjugate_gradient_method
Forward error correction,"In telecommunication, information theory, and coding theory, forward error correction  (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels.The central idea is the sender encodes the message in a redundant way, most often by using an error-correcting code (ECC).",https://en.wikipedia.org/wiki/Forward_error_correction
Bionics,,https://en.wikipedia.org/wiki/Bionics
Gaussian elimination,"Gaussian elimination, also known as row reduction, is an algorithm in linear algebra for solving a system of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855). Some special cases of the method - albeit presented without proof - were known to Chinese mathematicians as early as circa 179 AD.",https://en.wikipedia.org/wiki/Gaussian_elimination
Young tableau,"In mathematics, a Young tableau (/tæˈbloʊ, ˈtæbloʊ/; plural: tableaux) is a combinatorial object useful in representation theory and Schubert calculus. It provides a convenient way to describe the group representations of the symmetric and general linear groups and to study their properties. Young tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of the symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians, including Percy MacMahon, W. V. D. Hodge, G. de B. Robinson, Gian-Carlo Rota, Alain Lascoux, Marcel-Paul Schützenberger and Richard P. Stanley.",https://en.wikipedia.org/wiki/Young_tableau
Sørensen–Dice coefficient,"The Sørensen–Dice coefficient (see below for other names) is a statistic used to gauge the similarity of two samples. It was independently developed by the botanists Thorvald Sørensen and Lee Raymond Dice, who published in 1948 and 1945 respectively.",https://en.wikipedia.org/wiki/Dice%27s_coefficient
Learning automaton,A learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used.,https://en.wikipedia.org/wiki/Learning_automata
Correspondence analysis,"Correspondence analysis (CA) or reciprocal averaging is a multivariate statistical technique proposed by Herman Otto Hartley (Hirschfeld) and later developed by Jean-Paul Benzécri. It is conceptually similar to principal component analysis, but applies to categorical rather than continuous data. In a similar manner to principal component analysis, it provides a means of displaying or summarising a set of data in two-dimensional graphical form.",https://en.wikipedia.org/wiki/Correspondence_analysis
Bentley–Ottmann algorithm,"In computational geometry, the Bentley–Ottmann algorithm is a sweep line algorithm for listing all crossings in a set of line segments, i.e. it finds the intersection points (or, simply, intersections) of line segments. It extends the Shamos–Hoey algorithm, a similar previous algorithm for testing whether or not a set of line segments has any crossings. For an input consisting of n crossings (or intersections), the Bentley–Ottmann algorithm takes time O((n+k)log⁡n). In cases where k=o(n2log⁡n), this is an improvement on a naïve algorithm that tests every pair of segments, which takes Θ(n2).",https://en.wikipedia.org/wiki/Bentley%E2%80%93Ottmann_algorithm
Barnes–Hut simulation,The Barnes–Hut simulation (named after Josh Barnes and Piet Hut) is an approximation algorithm for performing an n-body simulation. It is notable for having order O(n log n) compared to a direct-sum algorithm which would be O(n2).,https://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation
Averaged one-dependence estimators,Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique.  It was developed to address the attribute-independence problem of the popular naive Bayes classifier.  It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.,https://en.wikipedia.org/wiki/AODE
Spigot algorithm,"A spigot algorithm is an algorithm for computing the value of a mathematical constant (such as π or e) that generates output digits in some base (usually 2 or a power of 2) from left to right, with limited intermediate storage. The name comes from the sense of the word ""spigot"" for a tap or valve controlling the flow of a liquid.",https://en.wikipedia.org/wiki/Spigot_algorithm
Inheritance (genetic algorithm),"In genetic algorithms, inheritance is the ability of modeled objects to mate, mutate (similar to biological mutation), and propagate their problem solving genes to the next generation, in order to produce an evolved solution to a particular problem. The selection of objects that will be inherited from in each successive generation is determined by a fitness function, which varies depending upon the problem being addressed.",https://en.wikipedia.org/wiki/Inheritance_(genetic_algorithm)
Block Truncation Coding,"Block Truncation Coding (BTC) is a type of lossy image compression technique for greyscale images. It divides the original images into blocks and then uses a quantizer to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. It is an early predecessor of the popular hardware DXTC technique, although BTC compression method was first adapted to color long before DXTC using a very similar approach called Color Cell Compression. BTC has also been adapted to video compression.",https://en.wikipedia.org/wiki/Block_Truncation_Coding
Chi-square automatic interaction detection,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing). The technique was developed in South Africa and was published in 1980 by Gordon V. Kass, who had completed a PhD thesis on this topic. CHAID can be used for prediction (in a similar fashion to regression analysis, this version of CHAID being originally known as XAID) as well as classification, and for detection of interaction between variables. CHAID is based on a formal extension of the United States' AID (Automatic Interaction Detection) and THAID (THeta Automatic Interaction Detection) procedures of the 1960s and 1970s, which in turn were extensions of earlier research, including that performed in the UK in the 1950s.",https://en.wikipedia.org/wiki/Chi-square_automatic_interaction_detection
Imperialist competitive algorithm,"In computer science, imperialist competitive algorithm is a computational method that is used to solve optimization problems of different types.  Like most of the methods in the area of evolutionary computation, ICA does not need the gradient of the function in its optimization process. From a specific point of view, ICA can be thought of as the social counterpart of genetic algorithms (GAs). ICA is the mathematical model and the computer simulation of human social evolution, while GAs are based on the biological evolution of species.",https://en.wikipedia.org/wiki/Imperialist_competitive_algorithm
Image-based lighting,"Image-based lighting (IBL) is a 3D rendering technique which involves capturing an omnidirectional representation of real-world light information as an image, typically using a specialized camera. This image is then projected onto a dome or sphere analogously to environment mapping, and this is used to simulate the lighting for the objects in the scene. This allows highly detailed real-world lighting to be used to light a scene, instead of trying to accurately model illumination using an existing rendering technique.",https://en.wikipedia.org/wiki/Image-based_lighting
Band matrix,"In mathematics, particularly matrix theory, a band matrix is a sparse matrix whose non-zero entries are confined to a diagonal band, comprising the main diagonal and zero or more diagonals on either side.",https://en.wikipedia.org/wiki/Bandwidth_(matrix_theory)
Cognitive computer,"A cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain. It generally adopts a Neuromorphic engineering approach. An example of a cognitive computer implemented by using neural networks and deep learning is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in ""Loihi"", which will be available to university and research labs in 2018. Intel, Qualcomm, and others are improving neuromorphic processors steadily, Intel with its Pohoiki Beach and Springs systems ",https://en.wikipedia.org/wiki/Cognitive_computer
Any-angle path planning,"Any-angle path planning algorithms are a subset of pathfinding algorithms that search for a path between two points in space and allow the turns in the path to have any angle. The result is a path that goes directly toward the goal and has relatively few turns. Other pathfinding algorithms such as A* constrain the paths to a grid, which produces jagged, indirect paths. Any-angle algorithms are able to find optimal or near-optimal paths by incorporating the any-angle search into the algorithm itself. Algorithms such as A* Post Smoothing that smooth a path found by a grid constrained algorithm are unable to reliably find optimal paths since they cannot change what side of a blocked cell is traversed.",https://en.wikipedia.org/wiki/Any-angle_path_planning
Approximations of π,,https://en.wikipedia.org/wiki/Computing_%CF%80
Exact cover,"In mathematics, given a collection S of subsets of a set X, an exact cover is a subcollection S* of S such that each element in X is contained in exactly one subset in S*.One says that each element in X is covered by exactly one subset in S*.An exact cover is a kind of cover.",https://en.wikipedia.org/wiki/Exact_cover
Decision tree,"A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.",https://en.wikipedia.org/wiki/Decision_tree
Genetic algorithm scheduling,The genetic algorithm is an operational research method that may be used to solve scheduling problems in production planning.,https://en.wikipedia.org/wiki/Genetic_algorithm_scheduling
Message authentication code,"In cryptography, a message authentication code (MAC), sometimes known as a tag, is a short piece of information used to authenticate a message—in other words, to confirm that the message came from the stated sender (its authenticity) and has not been changed. The MAC value protects both a message's data integrity as well as its authenticity, by allowing verifiers (who also possess the secret key) to detect any changes to the message content.",https://en.wikipedia.org/wiki/Message_authentication_code
Distribution learning theory,"The distributional learning theory or learning of probability distribution is a framework in computational learning theory. It has been proposed from Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert Schapire and Linda Sellie in 1994  and it was inspired from the PAC-framework introduced by Leslie Valiant.",https://en.wikipedia.org/wiki/Distribution_learning_theory
Base rate,"In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were ""medical professionals"", and 99% of the public were not ""medical professionals"", then the base rate of medical professionals is simply 1%.",https://en.wikipedia.org/wiki/Base_rate
Predictive analytics,"Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.",https://en.wikipedia.org/wiki/Predictive_analytics
Bicubic interpolation,"In mathematics, bicubic interpolation is an extension of cubic interpolation for interpolating data points on a two-dimensional regular grid. The interpolated surface is smoother than corresponding surfaces obtained by bilinear interpolation or nearest-neighbor interpolation. Bicubic interpolation can be accomplished using either Lagrange polynomials, cubic splines, or cubic convolution algorithm.",https://en.wikipedia.org/wiki/Bicubic_interpolation
XGBoost,"XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java,Python,R,Julia,Perl, and Scala.It works on Linux,Windows, andmacOS. From the project description, it aims to provide a ""Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library"". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, and Apache Flink.It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.",https://en.wikipedia.org/wiki/Xgboost
Growth function,"The growth function, also called the shatter coefficient or the shattering number, measures the richness of a set family. It is especially used in the context of statistical learning theory, where it measures the complexity of a hypothesis class.The term 'growth function' was coined by Vapnik and Chervonenkis in their 1968 paper, where they also proved many of its properties.It is a basic concept in machine learning.",https://en.wikipedia.org/wiki/Growth_function
International Data Encryption Algorithm,"In cryptography, the International Data Encryption Algorithm (IDEA), originally called Improved Proposed Encryption Standard (IPES), is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991.  The algorithm was intended as a replacement for the Data Encryption Standard (DES). IDEA is a minor revision of an earlier cipher Proposed Encryption Standard (PES).",https://en.wikipedia.org/wiki/International_Data_Encryption_Algorithm
Bees algorithm,"In computer science and operations research, the bees algorithm is a population-based search algorithm which was developed by Pham, Ghanbarzadeh et al. in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.",https://en.wikipedia.org/wiki/Bees_algorithm
Cross-entropy method,"The cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. It is applicable to both combinatorial and continuous problems, with either a static or noisy objective.",https://en.wikipedia.org/wiki/Cross-entropy_method
Discrete Fourier transform,"In mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence.  An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies.  It has the same sample-values as the original input sequence.  The DFT is therefore said to be a frequency domain representation of the original input sequence.  If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle.  If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.",https://en.wikipedia.org/wiki/Discrete_Fourier_transform
Recursive descent parser,"In computer science, a recursive descent parser is a kind of top-down parser built from a set of mutually recursive procedures (or a non-recursive equivalent) where each such procedure implements one of the nonterminals of the grammar. Thus the structure of the resulting program closely mirrors that of the grammar it recognizes.",https://en.wikipedia.org/wiki/Recursive_descent_parser
Adversarial machine learning,"Adversarial machine learning is a technique employed in the field of machine learning which attempts to fool models through malicious input. This technique can be applied for a variety of reasons, the most common being to attack or cause a malfunction in standard machine learning models.",https://en.wikipedia.org/wiki/Adversarial_machine_learning
Neuro Laboratory,Neuro Laboratory is a shareware scientific computing software for Windows and Linux platforms developed by Scientific Software. The current version is 1.1.,https://en.wikipedia.org/wiki/Neuro_Laboratory
Neural network software,"Neural network software is used to simulate, research, develop, and apply artificial neural networks, software concepts adapted from biological neural networks, and in some cases, a wider array of adaptive systems such as artificial intelligence and machine learning.",https://en.wikipedia.org/wiki/Neural_network_software
Jump search,"In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items Lkm, where k∈N and m is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].",https://en.wikipedia.org/wiki/Jump_search
Markov chain geostatistics,"Markov chain geostatistics uses Markov chain spatial models, simulation algorithms and associated spatial correlation measures (e.g., transiogram) based on the Markov chain random field theory, which extends a single Markov chain into a multi-dimensional random field for geostatistical modeling. A Markov chain random field is still a single spatial Markov chain. The spatial Markov chain moves or jumps in a space and decides its state at any unobserved location through interactions with its nearest known neighbors in different directions. The data interaction process can be well explained as a local sequential Bayesian updating process within a neighborhood. Because single-step transition probability matrices are difficult to estimate from sparse sample data and are impractical in representing the complex spatial heterogeneity of states, the transiogram, which is defined as a transition probability function over the distance lag, is proposed as the accompanying spatial measure of Markov chain random fields.",https://en.wikipedia.org/wiki/Markov_chain_geostatistics
LZX,LZX is an LZ77 family compression algorithm. It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen in 1990s.,https://en.wikipedia.org/wiki/LZX
Feature detection (computer vision),"In computer vision and image processing feature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.",https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)
Modular arithmetic,"In mathematics, modular arithmetic is a system of arithmetic for integers, where numbers ""wrap around"" when reaching a certain value, called the modulus. The modern approach to modular arithmetic was developed by Carl Friedrich Gauss in his book Disquisitiones Arithmeticae, published in 1801.",https://en.wikipedia.org/wiki/Modular_arithmetic
Generalized iterative scaling,"In statistics, generalized iterative scaling (GIS) and improved iterative scaling (IIS) are two early algorithms used to fit log-linear models, notably multinomial logistic regression (MaxEnt) classifiers and extensions of it such as MaxEnt Markov models and conditional random fields. These algorithms have been largely surpassed by gradient-based methods such as L-BFGS and coordinate descent algorithms.",https://en.wikipedia.org/wiki/Generalized_iterative_scaling
Analysis of variance,,https://en.wikipedia.org/wiki/ANOVA
Page replacement algorithm,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated. Page replacement happens when a requested page is not in memory (page fault) and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.",https://en.wikipedia.org/wiki/Clock_with_Adaptive_Replacement
Stooge sort,"Stooge sort is a recursive sorting algorithm. It is notable for its exceptionally bad time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...).The running time of the algorithm is thus slower comparedto reasonable sorting algorithms, and is slower than Bubble sort, a canonical example of a fairly inefficient sort.It is however more efficient than Slowsort.The name comes from The Three Stooges.",https://en.wikipedia.org/wiki/Stooge_sort
Writer invariant,"Writer invariant, also called authorial invariant or author's invariant, is a property of a text which is invariant of its author, that is, it will be similar in all texts of a given author and different in texts of different authors. It can be used to find plagiarism or discover who is real author of anonymously published text. Writer invariant is also an author's pattern of writing a letter in handwritten text recognition.",https://en.wikipedia.org/wiki/Writer_invariant
Telephone exchange,A telephone exchange or telephone switch is a telecommunications system used in the public switched telephone network or in large enterprises. It interconnects telephone subscriber lines or virtual circuits of digital systems to establish telephone calls between subscribers.,https://en.wikipedia.org/wiki/Telephone_exchange
Statistical learning theory,"Statistical learning theory is a framework for machine learningdrawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.",https://en.wikipedia.org/wiki/Statistical_learning_theory
Elevator algorithm,The elevator algorithm (also SCAN) is a disk-scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.,https://en.wikipedia.org/wiki/Elevator_algorithm
Page replacement algorithm,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated. Page replacement happens when a requested page is not in memory (page fault) and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.",https://en.wikipedia.org/wiki/Page_replacement_algorithms
Boosting (machine learning),"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): ""Can a set of weak learners create a single strong learner?"" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.",https://en.wikipedia.org/wiki/Boosting_(meta-algorithm)
Document classification,"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.",https://en.wikipedia.org/wiki/Document_classification
Machine learning,"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to perform the task.:2 Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.",https://en.wikipedia.org/wiki/Machine_learning
Huber loss,"In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used.",https://en.wikipedia.org/wiki/Huber_loss
Graphical model,"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.",https://en.wikipedia.org/wiki/Graphical_model
Hamiltonian mechanics,"Hamiltonian mechanics is an equivalent but more abstract reformulation of classical mechanic theory. Historically, it contributed to the formulation of statistical mechanics and quantum mechanics.",https://en.wikipedia.org/wiki/Hamiltonian_dynamics
Fitness function,"A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims. Fitness functions are used in genetic programming and genetic algorithms to guide simulations towards optimal design solutions.",https://en.wikipedia.org/wiki/Fitness_function
Data Mining Extensions,Data Mining Extensions (DMX) is a query language for data mining models supported by Microsoft's SQL Server Analysis Services product.,https://en.wikipedia.org/wiki/Data_Mining_Extensions
Concept drift,"In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.",https://en.wikipedia.org/wiki/Concept_drift
Scoring algorithm,"Scoring algorithm, also known as Fisher's scoring, is a form of Newton's method used in statistics to solve maximum likelihood equations numerically, named after Ronald Fisher.",https://en.wikipedia.org/wiki/Scoring_algorithm
Beam search,"In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. It is thus a greedy algorithm.",https://en.wikipedia.org/wiki/Beam_search
Needleman–Wunsch algorithm,"The Needleman–Wunsch algorithm is an algorithm used in  bioinformatics to align protein or nucleotide sequences. It was one of the first applications of dynamic programming to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970. The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems, and it uses the solutions to the smaller problems to find an optimal solution to the larger problem. It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance. The algorithm assigns a score to every possible alignment, and the purpose of the algorithm is to find all possible alignments having the highest score.",https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm
Deeplearning4j,"Eclipse Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.",https://en.wikipedia.org/wiki/Deeplearning4j
Ilya Sutskever,Ilya Sutskever is a computer scientist working in machine learning and currently serving as the Chief scientist of OpenAI.,https://en.wikipedia.org/wiki/Ilya_Sutskever
Evolution strategy,"In computer science, an evolution strategy (ES) is an optimization technique based on ideas of evolution. It belongs to the general class of evolutionary computation or artificial evolution methodologies.",https://en.wikipedia.org/wiki/Evolution_strategy
k-medians clustering,"In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the squared 2-norm distance metric (which k-means does.)",https://en.wikipedia.org/wiki/K-medians
Karn's algorithm,"Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking. The algorithm,  also sometimes termed as the Karn-Partridge algorithm was proposed in a paper by Phil Karn and Craig Partridge in 1987.",https://en.wikipedia.org/wiki/Karn%27s_algorithm
k-means++,"In data mining, k-means++ is an algorithm for choosing the initial values (or ""seeds"") for the k-means clustering algorithm.  It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in  independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)",https://en.wikipedia.org/wiki/K-means%2B%2B
International Standard Book Number,,https://en.wikipedia.org/wiki/International_Standard_Book_Number
Proximal gradient methods for learning,Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is ℓ1 regularization (also known as Lasso) of the form,https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning
Artificial immune system,"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.",https://en.wikipedia.org/wiki/Artificial_immune_system
Snakes and Ladders,"Snakes and Ladders, known originally as Moksha Patam, is an ancient Indian board game regarded today as a worldwide classic. It is played between two or more players on a gameboard having numbered, gridded squares. A number of ""ladders"" and ""snakes"" are pictured on the board, each connecting two specific board squares. The object of the game is to navigate one's game piece, according to die  rolls, from the start (bottom square) to the finish (top square), helped or hindered by ladders and snakes, respectively.",https://en.wikipedia.org/wiki/Snakes_and_Ladders
HUMANT (HUManoid ANT) algorithm,"HUMANT (HUManoid ANT) algorithm belongs to Ant colony optimization algorithms. It is a Multi-Objective Ant Colony Optimization (MOACO) with a priori approach to Multi-Objective Optimization (MOO), based on Max-Min Ant System (MMAS) and multi-criteria decision-making PROMETHEE method.",https://en.wikipedia.org/wiki/HUMANT_(HUManoid_ANT)_algorithm
Computational learning theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.",https://en.wikipedia.org/wiki/Computational_learning_theory
MIMIC (immunology),"MIMIC, or modular immune in vitro construct, is an artificial system imitating the human immune system.  It has applications in vaccine development.",https://en.wikipedia.org/wiki/MIMIC_(immunology)
Arithmetic coding,,https://en.wikipedia.org/wiki/Arithmetic_coding
Decision list,"Decision lists are a representation for Boolean functions which can be easily learnable from examples.  Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.",https://en.wikipedia.org/wiki/Decision_list
Patience sorting,"In computer science, patience sorting is a sorting algorithm inspired by, and named after, the card game patience. A variant of the algorithm efficiently computes the length of a longest increasing subsequence in a given array.",https://en.wikipedia.org/wiki/Patience_sorting
Repeated incremental pruning to produce error reduction (RIPPER),"In machine learning, repeated incremental pruning to produce error reduction (RIPPER) is a propositional rule learner proposed by William W. Cohen as an optimized version of IREP.",https://en.wikipedia.org/wiki/Repeated_incremental_pruning_to_produce_error_reduction_(RIPPER)
Push–relabel maximum flow algorithm,"In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows in a flow network. The name ""push–relabel"" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a ""preflow"" and gradually converts it into a maximum flow by moving flow locally between neighboring nodes using push operations under the guidance of an admissible network maintained by relabel operations. In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.",https://en.wikipedia.org/wiki/Push%E2%80%93relabel_algorithm
Probability distribution,"In probability theory and statistics, a probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. In more technical terms, the probability distribution is a description of a random phenomenon in terms of the probabilities of events. For instance, if the random variable X is used to denote the outcome of a coin toss (""the experiment""), then the probability distribution of X would take the value 0.5 for X = heads, and 0.5 for X = tails (assuming the coin is fair).  Examples of random phenomena can include the results of an experiment or survey.",https://en.wikipedia.org/wiki/Probability_distribution
Elastic map,"Elastic maps provide a tool for nonlinear dimensionality reduction. By their construction, they are a system of elastic springs  embedded in the dataspace. This system approximates a low-dimensional manifold. The elastic coefficients of this system allow the switch from completely unstructured k-means clustering (zero elasticity) to the estimators located closely to linear PCA manifolds (for high bending and low stretching modules). With some intermediate values of the elasticity coefficients, this system effectively approximates non-linear principal manifolds. This approach is based on a mechanical analogy between principal manifolds, that are passing through ""the middle"" of the data distribution, and elastic membranes and plates. The method was developed by A.N. Gorban, A.Y. Zinovyev and A.A. Pitenko in 1996–1998.",https://en.wikipedia.org/wiki/Elastic_net
Evolutionary computation,"In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.",https://en.wikipedia.org/wiki/Evolutionary_computation
RSA (cryptosystem),"RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the ""factoring problem"". The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1977. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997.",https://en.wikipedia.org/wiki/RSA_(cryptosystem)
Level of measurement,"Level of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines. Other classifications include those by Mosteller and Tukey, and by Chrisman.",https://en.wikipedia.org/wiki/Ordinal_classification
EDLUT,EDLUT (Event-Driven LookUp Table) is a computer application for simulating networks of spiking neurons.It was developed in the University of Granada and source code was released under GNU GPL version 3.,https://en.wikipedia.org/wiki/EDLUT
Huang's algorithm,Huang's algorithm is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.,https://en.wikipedia.org/wiki/Huang%27s_algorithm
Borůvka's algorithm,"Borůvka's algorithm is a greedy algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct,or a minimum spanning forest in the case of a graph that is not connected.",https://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm
Firefly algorithm,"In mathematical optimization, the firefly algorithm is a metaheuristic proposed by Xin-She Yang and inspired by the flashing behavior of fireflies.",https://en.wikipedia.org/wiki/Firefly_algorithm
Predictive learning,"Predictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by Piaget's account of how children construct knowledge of the world by interacting with it. Gary Drescher's book 'Made-up Minds' was seminal for the area.",https://en.wikipedia.org/wiki/Predictive_learning
LALR parser,"In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language. (""LR"" means left-to-right, rightmost derivation.)",https://en.wikipedia.org/wiki/Look-ahead_LR_parser
Apache OpenNLP,"The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as language detection, tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing and coreference resolution. These tasks are usually required to build more advanced text processing services.",https://en.wikipedia.org/wiki/OpenNLP
Lasso (statistics),"In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  It was originally introduced in geophysics literature in 1986, and later independently rediscovered and popularized in 1996 by Robert Tibshirani, who coined the term and provided further insights into the observed performance. ",https://en.wikipedia.org/wiki/Least_Absolute_Shrinkage_and_Selection_Operator
Cycle sort,"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.",https://en.wikipedia.org/wiki/Cycle_sort
bcrypt,,https://en.wikipedia.org/wiki/Bcrypt
Chandy–Lamport algorithm,The Chandy–Lamport algorithm is a snapshot algorithm that is used in distributed systems for recording a consistent global state of an asynchronous system. It was developed by and named after Leslie Lamport and K. Mani Chandy.,https://en.wikipedia.org/wiki/Chandy-Lamport_algorithm
Fermat primality test,The Fermat primality test is a probabilistic test to determine whether a number is a probable prime.,https://en.wikipedia.org/wiki/Fermat_primality_test
Collostructional analysis,"Collostructional analysis is a family of methods developed by (in alphabetical order) Stefan Th. Gries (University of California, Santa Barbara) and Anatol Stefanowitsch (Free University of Berlin). Collostructional analysis aims at measuring the degree of attraction or repulsion that words exhibit to constructions, where the notion of construction has so far been that of Goldberg's construction grammar.",https://en.wikipedia.org/wiki/Collostructional_analysis
Parsing expression grammar,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s.Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.",https://en.wikipedia.org/wiki/Packrat_parser
VIGRA,"VIGRA is the abbreviation for ""Vision with Generic Algorithms"". It is a free open-source computer vision library which focuses on customizable algorithms and data structures. VIGRA component can be easily adapted to specific needs of target application without compromising execution speed, by using template techniques similar to those in the C++ Standard Template Library.",https://en.wikipedia.org/wiki/VIGRA
Lanczos algorithm,"The Lanczos algorithm is a direct algorithm devised by Cornelius Lanczos that is an adaptation of power methods to find the m Hermitian matrix, where m. Although computationally efficient in principle, the method as initially formulated was not useful, due to its numerical instability.",https://en.wikipedia.org/wiki/Lanczos_iteration
Index calculus algorithm,"In computational number theory, the index calculus algorithm is a probabilistic algorithm  for computing discrete logarithms.Dedicated to the discrete logarithm in (Z/qZ)∗ is a prime, index calculus leads to a family of algorithms adapted to finite fields and to some families of elliptic curves. The algorithm collects relations among the discrete logarithms of small primes, computes them by a linear algebra procedure and finally expresses the desired discrete logarithm with respect to the discrete logarithms of small primes.",https://en.wikipedia.org/wiki/Index_calculus_algorithm
Marcus Hutter,,https://en.wikipedia.org/wiki/Marcus_Hutter
Parallel metaheuristic,"Parallel metaheuristic is a class of techniques that are capable of reducing both the numerical effort[clarification needed] and the run time of a metaheuristic. To this end, concepts and technologies from the field of parallelism in computer science are used to enhance and even completely modify the behavior of existing metaheuristics.  Just as it exists a long list of metaheuristics like evolutionary algorithms, particle swarm, ant colony optimization, simulated annealing, etc. it also exists a large set of different techniques strongly or loosely based in these ones, whose behavior encompasses the multiple parallel execution of algorithm components that cooperate in some way to solve a problem on a given parallel hardware platform.",https://en.wikipedia.org/wiki/Parallel_metaheuristic
Expectation propagation,Expectation propagation (EP) is a technique in Bayesian machine learning.,https://en.wikipedia.org/wiki/Expectation_propagation
Verlet integration,"Verlet integration (French pronunciation: ​) is a numerical method used to integrate Newton's equations of motion. It is frequently used to calculate trajectories of particles in molecular dynamics simulations and computer graphics. The algorithm was first used in 1791 by Delambre and has been rediscovered many times since then, most recently by Loup Verlet in the 1960s for use in molecular dynamics. It was also used by Cowell and Crommelin in 1909 to compute the orbit of Halley's Comet, and by Carl Størmer in 1907 to study the trajectories of electrical particles in a magnetic field (hence it is also called Störmer's method).The Verlet integrator provides good numerical stability, as well as other properties that are important in physical systems such as time reversibility and preservation of the symplectic form on phase space, at no significant additional computational cost over the simple Euler method.",https://en.wikipedia.org/wiki/Verlet_integration
Gröbner basis,,https://en.wikipedia.org/wiki/Gr%C3%B6bner_basis
GrabCut,GrabCut is an image segmentation method based on graph cuts.,https://en.wikipedia.org/wiki/Grabcut
PageRank,"PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}",https://en.wikipedia.org/wiki/PageRank
Renjin,Renjin is an implementation of the R programming language atop the Java Virtual Machine. It is free software released under  the GPL. Renjin is tightly integrated with Java to allow the embedding of the interpreter into any Java application with full two-way access between the Java and R code.,https://en.wikipedia.org/wiki/Renjin
Backpropagation,"In machine learning, specifically deep learning, backpropagation (backprop, BP) is a widely used algorithm in training feedforward neural networks for supervised learning. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally – a class of algorithms referred to generically as ""backpropagation"". In deep learning, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.",https://en.wikipedia.org/wiki/Backpropagation
Structural risk minimization,"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.",https://en.wikipedia.org/wiki/Structural_risk_minimization
Reinforcement learning,"Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.",https://en.wikipedia.org/wiki/Reinforcement_learning
Support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.",https://en.wikipedia.org/wiki/Support-vector_machine
Texas Medication Algorithm Project,"The Texas Medication Algorithm Project (TMAP) is a controversial decision-tree medical algorithm, the design of which was based on the expert opinions of mental health specialists. It has provided and rolled out a set of psychiatric management guidelines for doctors treating certain mental disorders within Texas' publicly funded mental health care system, along with manuals relating to each of them.  The algorithms commence after diagnosis and cover pharmacological treatment (hence ""Medication Algorithm"").",https://en.wikipedia.org/wiki/Texas_Medication_Algorithm_Project
Jürgen Schmidhuber,,https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber
Stochastic grammar,A stochastic grammar (statistical grammar) is a grammar framework with a probabilistic notion of grammaticality:,https://en.wikipedia.org/wiki/Stochastic_grammar
Multiple kernel learning,"Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b) combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source.",https://en.wikipedia.org/wiki/Multiple_kernel_learning
Lempel–Ziv–Markov chain algorithm,"The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. It has been under development since either 1996 or 1998 by Igor Pavlov and was first used in the 7z format of the 7-Zip archiver. This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 and features a high compression ratio (generally higher than bzip2) and a variable compression-dictionary size (up to 4 GB), while still maintaining decompression speed similar to other commonly used compression algorithms.",https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm
Piranha (software),"Piranha is a text mining system developed for the United States Department of Energy (DOE) by Oak Ridge National Laboratory (ORNL).  The software processes large volumes of unrelated free-text documents and shows relationships amongst them, a technique valuable across numerous scientific and data domains, from health care fraud to national security.  The results are presented in clusters of prioritized relevance to business and government analysts. Piranha uses the term frequency/inverse corpus frequency term weighting method which provides strong parallel processing of textual information, thus the ability to analyze very large document sets.Piranha has six main strengths: Collecting and Extracting: Millions of documents from numerous sources such as databases and social media can be collected and text extracted from hundreds of file formats; This info. can then be translated to any number of languages.Storing and indexing: Documents in search servers, relational databases, etc. can be stored and indexed at will.Recommending: Recommending the most valuable information for particular users.Categorizing: Grouping items via supervised and semi-supervised machine learning methods and targeted search lists.Clustering: Similarity is used to create a hierarchical group of documents.Visualizing: Showing relationships among documents so that users can quickly recognize connections.",https://en.wikipedia.org/wiki/Piranha_(software)
Network congestion,"Network congestion in data networking and queueing theory is the reduced quality of service that occurs when a network node or link is carrying more data than it can handle. Typical effects include queueing delay, packet loss or the blocking of new connections. A consequence of congestion is that an incremental increase in offered load leads either only to a small increase or even a decrease in network throughput.",https://en.wikipedia.org/wiki/Network_congestion
Template talk:Outline footer,Links from this article with broken #section links :[[Portal:Contents/Outline of Knowledge#Technology and applied sciences|Technology and applied sciences]],https://en.wikipedia.org/wiki/Template_talk:Outline_footer
Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.",https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma
Quantum machine learning,"Quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning increases such capabilities intelligently, by creating opportunities to conduct analysis on quantum states and systems. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster with the assistance of quantum devices. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term ""quantum machine learning"" is often associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning quantum phase transitions or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Finally, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as ""quantum learning theory"".",https://en.wikipedia.org/wiki/Quantum_machine_learning
Metaheuristic,"In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a set of solutions which is too large to be completely sampled. Metaheuristics may make few assumptions about the optimization problem being solved, and so they may be usable for a variety of problems.",https://en.wikipedia.org/wiki/Metaheuristic
Anne O'Tate,"Anne O'Tate  is a free, web-based application  that analyses sets of records identified on PubMed, the bibliographic database of articles from over 5,500 biomedical journals worldwide. While PubMed has its own wide range of search options to identify sets of records relevant to a researchers query it lacks the ability to analyse these sets of records further, a process for which the terms text mining and  drill down have been used. Anne O'Tate is able to perform such analysis and can process sets of up to 25,000 PubMed records.",https://en.wikipedia.org/wiki/Anne_O%27Tate
Multivariate interpolation,"In numerical analysis, multivariate interpolation or spatial interpolation is interpolation on functions of more than one variable.",https://en.wikipedia.org/wiki/Multivariate_interpolation
Matthews correlation coefficient,"The Matthews correlation coefficient (MCC) is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. Although the MCC is equivalent to Karl Pearson's phi coefficient,, which was developed decades earlier, the term MCC is widely used in the field of bioinformatics. ",https://en.wikipedia.org/wiki/Matthews_correlation_coefficient
Bootstrap aggregating,"Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.",https://en.wikipedia.org/wiki/Bootstrap_aggregating
Reward-based selection,"Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination. The probability of being selected for an individual is proportional to the cumulative reward, obtained by the individual. The cumulative reward can be computed as a sum of the individual reward and the reward, inherited from parents.",https://en.wikipedia.org/wiki/Reward-based_selection
Pedro Domingos,Pedro Domingos is a Professor at University of Washington. He is a researcher in machine learning known for Markov logic network enabling uncertain inference.,https://en.wikipedia.org/wiki/Pedro_Domingos
Prefrontal cortex basal ganglia working memory,"Prefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia. It can be compared to long short-term memory (LSTM) in functionality, but is more biologically explainable.[third-party source needed]",https://en.wikipedia.org/wiki/Prefrontal_cortex_basal_ganglia_working_memory
Lexicographic breadth-first search,"In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from a breadth-first search, but it produces an ordering that is consistent with breadth-first search.",https://en.wikipedia.org/wiki/Lexicographic_breadth-first_search
Random sample consensus,"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates.  Therefore, it also can be interpreted as an outlier detection method. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed.  The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.",https://en.wikipedia.org/wiki/RANSAC
Template:Outline footer,Initial visibility: currently defaults to autocollapse,https://en.wikipedia.org/wiki/Template:Outline_footer
Regression analysis,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared distances between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).",https://en.wikipedia.org/wiki/Regression_analysis
Mean squared error,"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.",https://en.wikipedia.org/wiki/Mean_squared_error
Peter Flach,,https://en.wikipedia.org/wiki/Peter_Flach
Subgraph isomorphism problem,"In theoretical computer science, the subgraph isomorphism problem is a computational task in which two graphs G and H are given as input, and one must determine whether G contains a subgraph that is isomorphic to H.Subgraph isomorphism is a generalization of both the maximum clique problem and the problem of testing whether a graph contains a Hamiltonian cycle, and is therefore NP-complete. However certain other cases of subgraph isomorphism may be solved in polynomial time.",https://en.wikipedia.org/wiki/Subgraph_isomorphism_problem
Almeida–Pineda recurrent backpropagation,"Almeida–Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks. It is a type of supervised learning. It was described somewhat cryptically in Richard Feynman's senior thesis, and rediscovered independently in the context of artificial neural networks by both Fernando Pineda and Luis B. Almeida.",https://en.wikipedia.org/wiki/Almeida%E2%80%93Pineda_recurrent_backpropagation
Bat algorithm,"The Bat algorithm is a metaheuristic algorithm for global optimization. It was inspired by the echolocation behaviour of microbats, with varying pulse rates of emission and loudness. The Bat algorithm was developed by Xin-She Yang in 2010.",https://en.wikipedia.org/wiki/Bat_algorithm
Topic model,"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ""dog"" and ""bone"" will appear more often in documents about dogs, ""cat"" and ""meow"" will appear in documents about cats, and ""the"" and ""is"" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The ""topics"" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.",https://en.wikipedia.org/wiki/Topic_model
Leabra,"Leabra stands for local, error-driven and associative, biologically realistic algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics.  This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in emergent (successor of PDP++) when making a new project, and is extensively used in various simulations.",https://en.wikipedia.org/wiki/Leabra
Computational complexity theory,"Computational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.",https://en.wikipedia.org/wiki/Computational_complexity_theory
Variational message passing,"Variational message passing (VMP) is an approximate inference technique for continuous- or discrete-valued Bayesian networks, with conjugate-exponential parents, developed by John Winn. VMP was developed as a means of generalizing the approximate variational methods used by such techniques as Latent Dirichlet allocation and works by updating an approximate distribution at each node through messages in the node's Markov blanket.",https://en.wikipedia.org/wiki/Variational_message_passing
Web search engine,"A web search engine or Internet search engine is a software system that is designed to carry out web search (Internet search), which means to search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories.  Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler.Internet content that is not capable of being searched by a web search engine is generally described as the deep web.",https://en.wikipedia.org/wiki/Search_engine
Multiple correspondence analysis,"In statistics, multiple correspondence analysis (MCA) is a data analysis technique for nominal categorical data, used to detect and represent underlying structures in a data set. It does this by representing data as points in a low-dimensional Euclidean space. The procedure thus appears to be the counterpart of principal component analysis for categorical data. MCA can be viewed as an extension of simple correspondence analysis (CA) in that it is applicable to a large set of categorical variables.",https://en.wikipedia.org/wiki/Multiple_correspondence_analysis
Clustering illusion,"The clustering illusion is the tendency to erroneously consider the inevitable ""streaks"" or ""clusters"" arising in small samples from random distributions to be non-random. The illusion is caused by a human tendency to underpredict the amount of variability likely to appear in a small sample of random or semi-random data.",https://en.wikipedia.org/wiki/Clustering_illusion
Sparse PCA,"Sparse principal component analysis (sparse PCA) is a specialised technique used in statistical analysis and, in particular, in the analysis of multivariate data sets. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by introducing sparsity structures to the input variables.",https://en.wikipedia.org/wiki/Sparse_PCA
Local outlier factor,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.",https://en.wikipedia.org/wiki/Local_outlier_factor
International Joint Conference on Artificial Intelligence,"The International Joint Conference on Artificial Intelligence (IJCAI) is a gathering of artificial intelligence researchers and practitioners. It is organized by the IJCAI, Inc. . It was held biennially in odd-numbered years from 1969 to 2015. Starting 2016, IJCAI is held annually. IJCAI is a highly selective conference. For instance, only 17% of the papers submitted to the conference were accepted in 2011, and in previous years never more than 26%. This makes it a more selective publication than many AI journals.",https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence
Julia (programming language),,https://en.wikipedia.org/wiki/Julia_(programming_language)
Kernel methods for vector output,Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.,https://en.wikipedia.org/wiki/Kernel_methods_for_vector_output
t-distributed stochastic neighbor embedding,,https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
Time complexity,"In computer science, the time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.",https://en.wikipedia.org/wiki/Strongly_polynomial
Newell's algorithm,"Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.",https://en.wikipedia.org/wiki/Newell%27s_algorithm
Longitudinal redundancy check,"In telecommunication, a longitudinal redundancy check (LRC), or horizontal redundancy check, is a form of redundancy check that is applied independently to each of a parallel group of bit streams.  The data must be divided into transmission blocks, to which the additional check data is added.",https://en.wikipedia.org/wiki/Longitudinal_redundancy_check
Tree sort,"A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order.",https://en.wikipedia.org/wiki/Tree_sort
Natural language processing,"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.",https://en.wikipedia.org/wiki/Natural_language_processing
Generalized canonical correlation,"In statistics, the generalized canonical correlation analysis (gCCA), is a way of making sense of cross-correlation matrices between the sets of random variables when there are more than two sets. While a conventional CCA generalizes principal component analysis (PCA) to two sets of random variables, a gCCA  generalizes PCA to more than two sets of random variables. The canonical variables represent those common factors that can be found by a large PCA of all of the transformed random variables after each set underwent its own PCA.",https://en.wikipedia.org/wiki/Generalized_canonical_correlation
ElGamal encryption,"In cryptography, the ElGamal encryption system is an asymmetric key encryption algorithm for public-key cryptography which is based on the Diffie–Hellman key exchange. It was described by Taher Elgamal in 1985. ElGamal encryption is used in the free GNU Privacy Guard software, recent versions of PGP, and other cryptosystems. The Digital Signature Algorithm (DSA) is a variant of the ElGamal signature scheme, which should not be confused with ElGamal encryption.",https://en.wikipedia.org/wiki/ElGamal_encryption
Convex polygon,"A convex polygon is a simple polygon (not self-intersecting) in which no line segment between two points on the boundary ever goes outside the polygon. Equivalently, it is a simple polygon whose interior is a convex set.  In a convex polygon, all interior angles are less than or equal to 180 degrees, while in a strictly convex polygon all interior angles are strictly less than 180 degrees.",https://en.wikipedia.org/wiki/Convex_polygon
Kernel method,"In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.",https://en.wikipedia.org/wiki/Kernel_method
Pohlig–Hellman algorithm,"In group theory, the Pohlig–Hellman algorithm, sometimes credited as the Silver–Pohlig–Hellman algorithm, is a special-purpose algorithm for computing discrete logarithms in a finite abelian group whose order is a smooth integer.",https://en.wikipedia.org/wiki/Pohlig%E2%80%93Hellman_algorithm
Katz's back-off model,"Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It accomplishes this estimation by backing off through progressively shorter history models under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results.",https://en.wikipedia.org/wiki/Katz%27s_back-off_model
Adaptive Huffman coding,"Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding. It permits building the code as the symbols are being transmitted, having no initial knowledge of source distribution, that allows one-pass encoding and adaptation to changing conditions in data.",https://en.wikipedia.org/wiki/Adaptive_Huffman_coding
AdaBoost,"AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.",https://en.wikipedia.org/wiki/AdaBoost
Miller–Rabin primality test,"The Miller–Rabin primality test or Rabin–Miller primality test is a primality test: an algorithm which determines whether a given number is prime, similar to the Fermat primality test and the Solovay–Strassen primality test. It was first discovered by Russian mathematician M. M. Artjuhov in 1967. Gary L. Miller rediscovered it in 1976; Miller's version of the test is deterministic, but its correctness relies on the unproven extended Riemann hypothesis. Michael O. Rabin modified it to obtain an unconditional probabilistic algorithm in 1980.",https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test
GLR parser,"A GLR parser (GLR standing for ""Generalized LR"", where L stands for ""left-to-right"" and R stands for ""rightmost (derivation)"") is an extension of an LR parser algorithm to handle non-deterministic and ambiguous grammars. The theoretical foundation was provided in a 1974 paper by Bernard Lang (along with other general Context-Free parsers such as GLL). It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, complexity with respect to grammar classes, and optimization techniques. The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a ""parallel parser"". Tomita presented five stages in his original work, though in practice it is the second stage that is recognized as the GLR parser.",https://en.wikipedia.org/wiki/GLR_parser
CellCognition,"CellCognition is a free open-source computational framework for quantitative analysis of high-throughput fluorescence microscopy (time-lapse) images in the field of bioimage informatics and systems microscopy. The CellCognition framework uses image processing, computer vision and machine learning techniques for single-cell tracking and classification of cell morphologies. This enables measurements of temporal progression of cell phases, modeling of cellular dynamics and generation of phenotype map.",https://en.wikipedia.org/wiki/CellCognition
Outline of computer vision,The following outline is provided as an overview of and topical guide to computer vision:,https://en.wikipedia.org/wiki/Outline_of_computer_vision
Apache SystemML,Apache SystemML is a flexible machine learning system that automatically scales to Spark and Hadoop clusters.,https://en.wikipedia.org/wiki/Apache_SystemML
Integer factorization,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these factors are further restricted to prime numbers, the process is called prime factorization.",https://en.wikipedia.org/wiki/Prime_factorization_algorithm
Emergent (software),"Emergent (formerly PDP++) is neural simulation software that is primarily intended for creating models of the brain and cognitive processes. Development initially began in 1995 at Carnegie Mellon University, and as of 2014, continues at the University of Colorado at Boulder. The 3.x release of the software, which was known as PDP++, is featured in the textbook Computational Explorations in Cognitive Neuroscience.",https://en.wikipedia.org/wiki/Emergent_(software)
Magnitude (mathematics),"In mathematics, magnitude is the size of a mathematical object, a property which determines whether the object is larger or smaller than other objects of the same kind. More formally, an object's magnitude is the displayed result of an ordering (or ranking) of the class of objects to which it belongs.",https://en.wikipedia.org/wiki/Magnitude_(mathematics)
Seam carving,"Seam carving (or liquid rescaling) is an algorithm for content-aware image resizing, developed by Shai Avidan, of Mitsubishi Electric Research Laboratories (MERL), and Ariel Shamir, of the Interdisciplinary Center and MERL. It functions by establishing a number of seams (paths of least importance) in an image and automatically removes seams to reduce image size or inserts seams to extend it. Seam carving also allows manually defining areas in which pixels may not be modified, and features the ability to remove whole objects from photographs.",https://en.wikipedia.org/wiki/Seam_carving
Raymond's algorithm,"Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system.  It imposes a logical structure (a K-ary tree) on distributed resources.  As defined, each node has only a single parent, to which all requests to attain the token are made.",https://en.wikipedia.org/wiki/Raymond%27s_Algorithm
CURE algorithm,CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.,https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm
Information fuzzy networks,"Information fuzzy networks (IFN) is a greedy machine learning algorithm for supervised learning.The data structure produced by the learning algorithm is also called Info Fuzzy Network.IFN construction is quite similar to decision trees' construction.However, IFN constructs a directed graph and not a tree.IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.",https://en.wikipedia.org/wiki/Information_fuzzy_networks
Alpha–beta pruning,"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.",https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning
Hungarian algorithm,"The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time and which anticipated later primal-dual methods. It was developed and published in 1955 by Harold Kuhn, who gave the name ""Hungarian method"" because the algorithm was largely based on the earlier works of two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry.",https://en.wikipedia.org/wiki/Hungarian_method
Karatsuba algorithm,"The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatoly Karatsuba in 1960 and published in 1962. It reduces the multiplication of two n-digit numbers to at most nlog2⁡3≈n1.58\approx n^{1.58}} single-digit multiplications in general (and exactly nlog2⁡3 when n is a power of 2). It is therefore faster than the classical algorithm, which requires n2 single-digit products. For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210), whereas the classical algorithm requires (210)2 = 1,048,576 (a speedup of 17.75 times).",https://en.wikipedia.org/wiki/Karatsuba_algorithm
Prüfer sequence,"In combinatorial mathematics, the Prüfer sequence (also Prüfer code or Prüfer numbers) of a labeled tree is a unique sequence associated with the tree.  The sequence for a tree on n vertices has length n − 2, and can be generated by a simple iterative algorithm.  Prüfer sequences were first used by Heinz Prüfer to prove Cayley's formula in 1918.",https://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence
Ground state,"The ground state of a quantum-mechanical system is its lowest-energy state; the energy of the ground state is known as the zero-point energy of the system. An excited state is any state with energy greater than the ground state. In quantum field theory, the ground state is usually called the vacuum state or the vacuum.",https://en.wikipedia.org/wiki/Ground_state
Michael Kearns (computer scientist),"Michael Kearns is an American computer scientist, professor and National Center Chair at the University of Pennsylvania, the founding director of Penn's Singh Program in Networked & Social Systems Engineering (NETS), the founding director of Warren Center for Network and Data Sciences , and also holds secondary appointments in Penn's Wharton School and department of Economics. He is a leading researcher in computational learning theory and algorithmic game theory, and interested in machine learning, artificial intelligence, computational finance, algorithmic trading, computational social science and social networks. He leads the Advisory and Research function in Morgan Stanley's Artificial Intelligence Center of Excellence team.",https://en.wikipedia.org/wiki/Michael_Kearns_(computer_scientist)
Universal portfolio algorithm,The universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late Stanford University information theorist Thomas M. Cover.,https://en.wikipedia.org/wiki/Universal_portfolio_algorithm
Automatic taxonomy construction,"Automatic taxonomy construction (ATC) is the use of software programs to generate taxonomical classifications from a body of texts called a corpus. ATC is a branch of natural language processing, which in turn is a branch of artificial intelligence.",https://en.wikipedia.org/wiki/Automatic_taxonomy_construction
Ball tree,"In computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as ""balls"". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.",https://en.wikipedia.org/wiki/Ball_tree
Yasuo Matsuyama,"Yasuo Matsuyama (born March 23, 1947) is a Japanese researcher in machine learning and human-aware information processing.",https://en.wikipedia.org/wiki/Yasuo_Matsuyama
μ-law algorithm,,https://en.wikipedia.org/wiki/Mu-law_algorithm
Booth's multiplication algorithm,"Booth's multiplication algorithm is a multiplication algorithm that multiplies two signed binary numbers in two's complement notation. The algorithm was invented by Andrew Donald Booth in 1950 while doing research on crystallography at Birkbeck College in Bloomsbury, London. Booth's algorithm is of interest in the study of computer architecture.",https://en.wikipedia.org/wiki/Booth%27s_multiplication_algorithm
Min-conflicts algorithm,"In computer science, the min conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems (CSP).",https://en.wikipedia.org/wiki/Min_conflicts_algorithm
Hamming code,"In telecommunication, Hamming codes are a family of linear error-correcting codes. Hamming codes can detect up to two-bit errors or correct one-bit errors without detection of uncorrected errors. By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three.Richard W. Hamming invented Hamming codes in 1950 as a way of automatically correcting errors introduced by punched card readers. In his original paper, Hamming elaborated his general idea, but specifically focused on the Hamming(7,4) code which adds three parity bits to four bits of data.",https://en.wikipedia.org/wiki/Hamming_code
Least-squares support-vector machine,"Least-squares support-vector machines (LS-SVM) are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis.  In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs. Least-squares SVM classifiers were proposed by Suykens and Vandewalle. LS-SVMs are a class of kernel-based learning methods.",https://en.wikipedia.org/wiki/Least_squares_support_vector_machine
Deep belief network,"In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (""hidden units""), with connections between the layers but not between units within each layer.",https://en.wikipedia.org/wiki/Deep_belief_network
Darkforest,"Darkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with  Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.",https://en.wikipedia.org/wiki/Darkforest
International Conference on Machine Learning,"The International Conference on Machine Learning (ICML) is an international academic conference on machine learning. Along with the Conference on Neural Information Processing Systems, it is one of the two primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the International Machine Learning Society.",https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning
Waffles (machine learning),"Waffles is a collection of command-line tools for performing machine learning operations developed at Brigham Young University. These tools are written in C++, and are available under the GNU Lesser General Public License.",https://en.wikipedia.org/wiki/Waffles_(machine_learning)
Quadratic sieve,"The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve.",https://en.wikipedia.org/wiki/Quadratic_sieve
Evolutionary programming,"Evolutionary programming is one of the four major evolutionary algorithm paradigms.  It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.",https://en.wikipedia.org/wiki/Evolutionary_programming
DFA minimization,"In automata theory (a branch of theoretical computer science), DFA minimization is the task of transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Here, two DFAs are called equivalent if they recognize the same regular language. Several different algorithms accomplishing this task are known and described in standard textbooks on automata theory.",https://en.wikipedia.org/wiki/DFA_minimization
Flow network,"In graph theory, a flow network (also known as a transportation network) is a directed graph where each edge has a capacity and each edge receives a flow. The amount of flow on an edge cannot exceed the capacity of the edge. Often in operations research, a directed graph is called a network, the vertices are called nodes and the edges are called arcs.  A flow must satisfy the restriction that the amount of flow into a node equals the amount of flow out of it, unless it is a source, which has only outgoing flow, or sink, which has only incoming flow.  A network can be used to model traffic in a computer network, circulation with demands, fluids in pipes, currents in an electrical circuit, or anything similar in which something travels through a network of nodes.",https://en.wikipedia.org/wiki/Flow_network
Mersenne Twister,The Mersenne Twister is a pseudorandom number generator (PRNG). It is by far the most widely used general-purpose PRNG. Its name derives from the fact that its period length is chosen to be a Mersenne prime.,https://en.wikipedia.org/wiki/Mersenne_Twister
Matching wildcards,"In computer science, an algorithm for matching wildcards (also known as globbing) is useful in comparing text strings that may contain wildcard syntax. Common uses of these algorithms include command-line interfaces, e.g. the Bourne shell or Microsoft Windows command-line or text editor or file manager, as well as the interfaces for some search engines and databases. Wildcard matching is a subset of the problem of matching regular expressions and string matching in general.",https://en.wikipedia.org/wiki/Matching_wildcards
Gaussian process,"In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.",https://en.wikipedia.org/wiki/Gaussian_process
Factored language model,"The factored language model (FLM) is an extension of a conventional language model introduced by Jeff Bilmes and Katrin Kirchoff in 2003.  In an FLM, each word is viewed as a vector of k factors: wi={fi1,...,fik}.,...,f_{i}^{k}\}.}  An FLM provides the probabilistic model P(f|f1,...,fN),...,f_{N})} where the prediction of a factor f parents {f1,...,fN},...,f_{N}\}}.  For example, if w represents a Part of speech tag for English, the expression P(wi|wi−2,wi−1,ti−1),w_{i-1},t_{i-1})} gives a model for predicting current word token based on a traditional Ngram model as well as the Part of speech tag of the previous word.",https://en.wikipedia.org/wiki/Factored_language_model
Support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.",https://en.wikipedia.org/wiki/Support_Vector_Machines
Fast Fourier transform,,https://en.wikipedia.org/wiki/Fast_Fourier_transform
Euler method,"In mathematics and computational science, the Euler method (also called forward Euler method) is a first-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value. It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–1870).",https://en.wikipedia.org/wiki/Euler_integration
Jump-and-Walk algorithm,"Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.",https://en.wikipedia.org/wiki/Jump-and-Walk_algorithm
Multiple sequence alignment,"A multiple sequence alignment (MSA) is a sequence alignment of three or more biological sequences, generally protein, DNA, or RNA. In many cases, the input set of query sequences are assumed to have an evolutionary relationship by which they share a linkage and are descended from a common ancestor. From the resulting MSA, sequence homology can be inferred and phylogenetic analysis can be conducted to assess the sequences' shared evolutionary origins. Visual depictions of the alignment as in the image at right illustrate mutation events such as point mutations (single amino acid or nucleotide changes) that appear as differing characters in a single alignment column, and insertion or deletion mutations (indels or gaps) that appear as hyphens in one or more of the sequences in the alignment. Multiple sequence alignment is often used to assess sequence conservation of protein domains, tertiary and secondary structures, and even individual amino acids or nucleotides.",https://en.wikipedia.org/wiki/Multiple_sequence_alignment
Multiplication algorithm,"A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are used. Efficient multiplication algorithms have existed since the advent of the decimal system.",https://en.wikipedia.org/wiki/Multiplication_algorithm
Simple precedence parser,"In computer science, a simple precedence parser is a type of bottom-up parser for context-free grammars that can be used only by simple precedence grammars.",https://en.wikipedia.org/wiki/Simple_precedence_parser
Oracle Data Mining,"Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.",https://en.wikipedia.org/wiki/Oracle_Data_Mining
Ternary search,"A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two thirds. A ternary search is an example of a divide and conquer algorithm (see search algorithm).",https://en.wikipedia.org/wiki/Ternary_search
Zhu–Takaoka string matching algorithm,"In computer science, the Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm. It uses two consecutive text characters to compute the bad character shift. It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase.",https://en.wikipedia.org/wiki/Zhu%E2%80%93Takaoka_string_matching_algorithm
NOMINATE (scaling method),"NOMINATE (an acronym for Nominal Three-Step Estimation) is a multidimensional scaling application developed by political scientists Keith T. Poole and Howard Rosenthal in the early 1980s to analyzepreferential and choice data, such as legislative roll-call voting behavior. As computing capabilities grew, Poole and Rosenthal developed multiple iterations of their NOMINATE procedure: the original D-NOMINATE method, W-NOMINATE, and most recently DW-NOMINATE (for dynamic, weighted NOMINATE). In 2009, Poole and Rosenthal were named the first recipients of the Society for Political Methodology's Best Statistical Software Award for their development of NOMINATE, a recognition conferred to ""individual(s) for developing statistical software that makes a significant research contribution"". In 2016, Keith T. Poole was awarded the Society for Political Methodology's Career Achievement Award. The citation for this award reads, in part, ""One can say perfectly correctly, and without any hyperbole: the modern study of the U.S. Congress would be simply unthinkable without NOMINATE legislative roll call voting scores. NOMINATE has produced data that entire bodies of our discipline—and many in the press—have relied on to understand the U.S. Congress.""",https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)
Generative model,"In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling.",https://en.wikipedia.org/wiki/Generative_model
Mean shift,"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.",https://en.wikipedia.org/wiki/Mean_shift
Ugly duckling theorem,"The Ugly Duckling theorem is an argument showing that classification is not really possible without some sort of bias. More particularly, it assumes finitely many properties combinable by logical connectives, and finitely many objects; it asserts that any two different objects share the same number of (extensional) properties. The theorem is named after Hans Christian Andersen's story ""The Ugly Duckling"", because it shows that a duckling is just as similar to a swan as two duckling are to each other. It was proposed by Satosi Watanabe in 1969.:376–377",https://en.wikipedia.org/wiki/Ugly_duckling_theorem
Genetic algorithm,,https://en.wikipedia.org/wiki/GATTO
Phong shading,"Phong shading is an interpolation technique for surface shading in 3D computer graphics. It is also called Phong interpolation, or normal-vector interpolation shading. It interpolates surface normals across rasterized polygons and computes pixel colors based on the interpolated normals and a reflection model. Phong shading may also refer to the specific combination of Phong interpolation and the Phong reflection model.",https://en.wikipedia.org/wiki/Phong_shading
Sparse matrix,"In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix). Using those definitions, a matrix will be sparse when its sparsity is greater than 0.5.",https://en.wikipedia.org/wiki/Symmetric_sparse_matrix
Chien search,"In abstract algebra, the Chien search, named after Robert Tienwen Chien, is a fast algorithm for determining roots of polynomials defined over a finite field. Chien search is commonly used to find the roots of error-locator polynomials encountered in decoding Reed-Solomon codes and BCH codes.",https://en.wikipedia.org/wiki/Chien_search
Goertzel algorithm,"The Goertzel algorithm is a technique in digital signal processing (DSP) for efficient evaluation of the individual terms of the discrete Fourier transform (DFT). It is useful in certain practical applications, such as recognition of dual-tone multi-frequency signaling (DTMF) tones produced by the push buttons of the keypad of a traditional analog telephone. The algorithm was first described by Gerald Goertzel in 1958.",https://en.wikipedia.org/wiki/Goertzel_algorithm
Factor regression model,"The factor regression model, or hybrid factor model, is a special multivariate model with the following form:",https://en.wikipedia.org/wiki/Factor_regression_model
Torch (machine learning),"Torch is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation. As of 2018, Torch is no longer in active development. However, PyTorch is actively developed as of August 2019.",https://en.wikipedia.org/wiki/Torch_(machine_learning)
Premature convergence,"In genetic algorithms, the term of premature convergence means that a population for an optimization problem converged too early, resulting in being suboptimal. In this context, the parental solutions, through the aid of genetic operators, are not able to generate offspring that are superior to, or outperform, their parents. Premature convergence is a common problem found in genetic algorithms, as it leads to a loss, or convergence of, a large number of alleles, subsequently making it very difficult to search for a specific gene in which the alleles were present. An allele is considered lost if, in a population, a gene is present, where all individuals are sharing the same value for that particular gene. An allele is, as defined by De Jong, considered to be a converged allele, when 95% of a population share the same value for a certain gene (see also convergence).",https://en.wikipedia.org/wiki/Premature_convergence
Longest path problem,"In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. A path is called simple if it does not have any repeated vertices; the length of a path may either be measured by its number of edges, or (in weighted graphs) by the sum of the weights of its edges. In contrast to the shortest path problem, which can be solved in polynomial time in graphs without negative-weight cycles, the longest path problem is NP-hard and the decision version of the problem, which asks whether a path exists of at least some given length, is NP-complete.  This means that the decision problem cannot be solved in polynomial time for arbitrary graphs unless P = NP. Stronger hardness results are also known showing that it is difficult to approximate. However, it has a linear time solution for directed acyclic graphs, which has important applications in finding the critical path in scheduling problems.",https://en.wikipedia.org/wiki/Longest_path_problem
Lingyun Gu,Lingyun Gu (born in 1976) is one of the experts in AI technologies.,https://en.wikipedia.org/wiki/Lingyun_Gu
Knight's tour,"A knight's tour is a sequence of moves of a knight on a chessboard such that the knight visits every square exactly once. If the knight ends on a square that is one knight's move from the beginning square (so that it could tour the board again immediately, following the same path), the tour is closed; otherwise, it is open.",https://en.wikipedia.org/wiki/Knight%27s_tour
Berkeley algorithm,"The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. It was developed by Gusella and Zatti at the University of California, Berkeley in 1989. Like Cristian's algorithm, it is intended for use within intranets.",https://en.wikipedia.org/wiki/Berkeley_algorithm
Leslie P. Kaelbling,"Leslie Pack Kaelbling is an American roboticist and the Panasonic Professor of Computer Science and Engineering at the Massachusetts Institute of Technology. She is widely recognized for adapting partially observable Markov decision process from operations research for application in artificial intelligence and robotics. Kaelbling received the IJCAI Computers and Thought Award in 1997 for applying reinforcement learning to embedded control systems and developing programming tools for robot navigation. In 2000, she was elected as a Fellow of the Association for the Advancement of Artificial Intelligence.",https://en.wikipedia.org/wiki/Leslie_P._Kaelbling
Calibration (statistics),"There are two main uses of the term calibration in statistics that denote special types of statistical inference problems. ""Calibration"" can mean",https://en.wikipedia.org/wiki/Calibration_(statistics)
Birkhoff interpolation,"In mathematics, Birkhoff interpolation is an extension of polynomial interpolation.",https://en.wikipedia.org/wiki/Birkhoff_interpolation
Earliest deadline first scheduling,"Earliest deadline first (EDF) or least time to go is a dynamic priority scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (task finishes, new task released, etc.) the queue will be searched for the process closest to its deadline. This process is the next to be scheduled for execution.",https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling
Ellipsoid method,"In mathematical optimization, the ellipsoid method is an iterative method for minimizing convex functions. When specialized to solving feasible linear optimization problems with rational data, the ellipsoid method is an  algorithm which finds an optimal solution in a finite number of steps.",https://en.wikipedia.org/wiki/Ellipsoid_method
Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",https://en.wikipedia.org/wiki/Stacked_Generalization
Markov random field,"In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.",https://en.wikipedia.org/wiki/Markov_random_field
Unrestricted algorithm,An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.,https://en.wikipedia.org/wiki/Unrestricted_algorithm
Longest increasing subsequence,"In computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. This subsequence is not necessarily contiguous, or unique.Longest increasing subsequences are studied in the context of various disciplines related to mathematics, including algorithmics, random matrix theory, representation theory, and physics. The longest increasing subsequence problem is solvable in time O(n log n), where n denotes the length of the input sequence.",https://en.wikipedia.org/wiki/Longest_increasing_subsequence_problem
Cycle detection,"In computer science, cycle detection or cycle finding is the algorithmic problem of finding a cycle in a sequence of iterated function values.",https://en.wikipedia.org/wiki/Floyd%27s_cycle-finding_algorithm
Suffix tree,"In computer science, a suffix tree (also called PAT tree or, in an earlier form, position tree) is a compressed trie containing all the suffixes of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations.",https://en.wikipedia.org/wiki/Suffix_tree
Association rule learning,Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.,https://en.wikipedia.org/wiki/Eclat_algorithm
Curse of dimensionality,The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.  The expression was coined by Richard E. Bellman when considering problems in dynamic programming.,https://en.wikipedia.org/wiki/Curse_of_dimensionality
Reference counting,,https://en.wikipedia.org/wiki/Reference_counting
Out-of-bag error,"Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.",https://en.wikipedia.org/wiki/Out-of-bag_error
Conditional random field,"Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighboring"" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.",https://en.wikipedia.org/wiki/Conditional_Random_Field
Exploratory factor analysis,"In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables. It is commonly used by researchers when developing a scale (a scale is a collection of questions used to measure a particular research topic) and serves to identify a set of latent constructs underlying a battery of measured variables. It should be used when the researcher has no a priori hypothesis about factors or patterns of measured variables. Measured variables are any one of several attributes of people that may be observed and measured. Examples of measured variables could be the physical height, weight, and pulse rate of a human being. Usually, researchers would have a large number of measured variables, which are assumed to be related to a smaller number of ""unobserved"" factors. Researchers must carefully consider the number of measured variables to include in the analysis. EFA procedures are more accurate when each factor is represented by multiple measured variables in the analysis.",https://en.wikipedia.org/wiki/Exploratory_factor_analysis
Visual cortex,The visual cortex of the brain is that part of the cerebral cortex which processes visual information. It is located in the occipital lobe. Visual nerves run straight from the eye to the primary visual cortex to the visual association cortex.,https://en.wikipedia.org/wiki/Visual_cortex
David J. C. MacKay,,https://en.wikipedia.org/wiki/David_J._C._MacKay
Karger's algorithm,"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.",https://en.wikipedia.org/wiki/Karger%27s_algorithm
Pipeline Pilot,"Pipeline Pilot is a desktop software program sold by Dassault Systèmes for processing and analyzing data. Originally used in the natural sciences, the product's basic ETL (Extract, transform, load) and analytics capabilities have been broadened. The product is now used for data science, ETL, reporting, prediction and analytics in a number of sectors. The main feature of the product is the ability to design data workflows using a graphical user interface. The program is an example of visual and dataflow programming. It has use in a variety of settings, such as  cheminformatics and QSAR, Next Generation Sequencing, image analysis, and text analytics.",https://en.wikipedia.org/wiki/Pipeline_Pilot
Mehryar Mohri,"Mehryar Mohri is a professor of computer science at the Courant Institute of Mathematical Sciences at New York University known for his work in machine learning, automata theory and algorithms, speech recognition and natural language processing.",https://en.wikipedia.org/wiki/Mehryar_Mohri
MATLAB,,https://en.wikipedia.org/wiki/MATLAB
Hindley–Milner type system,A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.,https://en.wikipedia.org/wiki/Hindley-Milner_type_inference
Inductive bias,The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.,https://en.wikipedia.org/wiki/Inductive_bias
Best-first search,Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.,https://en.wikipedia.org/wiki/Best-first_search
DADiSP,"DADiSP (Data Analysis and Display, pronounced day-disp) is a numerical computing environment developed by DSP Development Corporation which allows one to display and manipulate data series, matrices and images with an interface similar to a spreadsheet. DADiSP is used in the study of signal processing, numerical analysis, statistical and physiological data processing.",https://en.wikipedia.org/wiki/DADiSP
Probabilistic classification,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles.",https://en.wikipedia.org/wiki/Probabilistic_classifier
Column generation,Column generation or delayed column generation is an efficient algorithm for solving larger linear programs.,https://en.wikipedia.org/wiki/Delayed_column_generation
BCH code,"In coding theory, the BCH codes or Bose–Chaudhuri–Hocquenghem codes form a class of cyclic error-correcting codes that are constructed using polynomials over a finite field (also called Galois field). BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Bose and D. K. Ray-Chaudhuri. The name Bose–Chaudhuri–Hocquenghem (and the acronym BCH) arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).",https://en.wikipedia.org/wiki/Peterson%E2%80%93Gorenstein%E2%80%93Zierler_algorithm
Odds algorithm,"The odds-algorithm  is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems.  Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below. ",https://en.wikipedia.org/wiki/Bruss_algorithm
Native-language identification,"Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.",https://en.wikipedia.org/wiki/Native-language_identification
Public-key cryptography,,https://en.wikipedia.org/wiki/Asymmetric_key_algorithm
Cone algorithm,"In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles.  Its applications include computational surface science and computational nano science. The cone algorithm was first described in a publication about nanogold in 2005.",https://en.wikipedia.org/wiki/Cone_algorithm
Lior Ron (business executive),"Lior Ron (born March 16, 1977) is an Israeli-born businessman. He served in the Israel Defense Forces from 1997 to 2004, before attending Stanford to pursue a MBA. In 2016 he co-founded Otto, a self-driving truck company, with Anthony Levandowski, Claire Delaunay and Don Burnette. Prior to Otto he was the Product Lead for Google Maps and then the Product Lead for Motorola Mobility, which was acquired by Google in 2011.",https://en.wikipedia.org/wiki/Lior_Ron_(business_executive)
Flood fill,"Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in the ""bucket"" fill tool of paint programs to fill connected, similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared.",https://en.wikipedia.org/wiki/Flood_fill
Memetic algorithm,"In computer science and operations research, a memetic algorithm (MA) is an extension of the traditional genetic algorithm. It uses a local search technique to reduce the likelihood of the premature convergence.",https://en.wikipedia.org/wiki/Memetic_algorithm
Ridders' method,"In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a continuous functionf(x) . The method is due to C. Ridders.",https://en.wikipedia.org/wiki/Ridder%27s_method
Statistical relational learning,"Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s.",https://en.wikipedia.org/wiki/Statistical_relational_learning
Marr–Hildreth algorithm,"In computer vision, the Marr–Hildreth algorithm is a method of detecting edges in digital images, that is, continuous curves where there are strong and rapid variations in image brightness. The Marr–Hildreth edge detection method is simple and operates by convolving the image with the Laplacian of the Gaussian function, or, as a fast approximation by difference of Gaussians. Then, zero crossings are detected in the filtered result to obtain the edges. The Laplacian-of-Gaussian image operator is sometimes also referred to as the Mexican hat wavelet due to its visual shape when turned upside-down.  David Marr and Ellen C. Hildreth are two of the inventors.",https://en.wikipedia.org/wiki/Marr%E2%80%93Hildreth_algorithm
Hoshen–Kopelman algorithm,"The Hoshen–Kopelman algorithm is a simple and efficient algorithm for labeling clusters on a grid, where the grid is a regular network of cells, with the cells being either occupied or unoccupied. This algorithm is based on a well-known union-finding algorithm. The algorithm was originally described by Joseph Hoshen and Raoul Kopelman in their 1976 paper ""Percolation and Cluster Distribution. I. Cluster Multiple Labeling Technique and Critical Concentration Algorithm"".",https://en.wikipedia.org/wiki/Hoshen%E2%80%93Kopelman_algorithm
Constructing skill trees,"Constructing skill trees (CST) is a hierarchical reinforcement learning algorithm which can build  skill trees from a set of sample solution trajectories obtained from demonstration.  CST uses an incremental MAP(maximum a posteriori ) change point detection algorithm to segment each demonstration trajectory into skills and integrate the results into a skill tree.  CST was introduced by George Konidaris, Scott Kuindersma, Andrew Barto and Roderic Grupen in 2010.",https://en.wikipedia.org/wiki/Constructing_skill_trees
Empirical risk minimization,"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk). ",https://en.wikipedia.org/wiki/Empirical_risk_minimization
Detrended correspondence analysis,"Detrended correspondence analysis (DCA) is a multivariate statistical technique widely used by ecologists to find the main factors or gradients in large, species-rich but usually sparse data matrices that typify ecological community data. DCA is frequently used to suppress artifacts inherent in most other multivariate analyses when applied to gradient data.",https://en.wikipedia.org/wiki/Detrended_correspondence_analysis
Set (mathematics),"In mathematics, a set is a well-defined collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2, 4, 6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived.",https://en.wikipedia.org/wiki/Set_(mathematics)
Hartmut Neven,"Hartmut Neven (born 1964) is a scientist working in quantum computing, computer vision, robotics and computational neuroscience. He is best known for his work in face and object recognition and his contributions to quantum machine learning. He is currently Director of Engineering and Distinguished Scientist at Google where he is leading the Quantum Artificial Intelligence Lab which he founded in 2012.",https://en.wikipedia.org/wiki/Hartmut_Neven
Encog,"Encog is a machine learning framework available for Java and .Net.Encog supports different learning algorithms such as Bayesian Networks, Hidden Markov Models and Support Vector Machines.However, its main strength lies in its neural network algorithms. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using many different techniques.  Multithreading is used to allow optimal training performance on multicore machines.",https://en.wikipedia.org/wiki/Encog
Item response theory,"In psychometrics, item response theory (IRT) (also known as latent trait theory, strong true score theory, or modern mental test theory) is a paradigm for the design, analysis, and scoring of tests, questionnaires, and similar instruments measuring abilities, attitudes, or other variables. It is a theory of testing based on the relationship between individuals' performances on a test item and the test takers' levels of performance on an overall measure of the ability that item was designed to measure. Several different statistical models are used to represent both item and test taker characteristics. Unlike simpler alternatives for creating scales and evaluating questionnaire responses, it does not assume that each item is equally difficult. This distinguishes IRT from, for instance, Likert scaling, in which ""All items are assumed to be replications of each other or in other words items are considered to be parallel instruments"" (p. 197). By contrast, item response theory treats the difficulty of each item (the item characteristic curves, or ICCs) as information to be incorporated in scaling items. ",https://en.wikipedia.org/wiki/Item_response_theory
Lexical analysis,"In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.",https://en.wikipedia.org/wiki/Lexical_analysis
DEFLATE,"In computing, Deflate is a lossless data compression file format that uses a combination of LZSS and Huffman coding. It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. Deflate was later specified in RFC 1951 (1996).",https://en.wikipedia.org/wiki/DEFLATE_(algorithm)
Truncation selection,"In animal and plant breeding, truncation selection is a standard method in selective breeding in selecting animals to be bred for the next generation. Animals are ranked by their phenotypic value on some trait such as milk production, and the top percentage is reproduced. The effects of truncation selection for a continuous trait can be modeled by the standard breeder's equation  by using heritability and truncated normal distributions; on a binary trait, it can be modeled easily using the liability threshold model. It is considered an easy and efficient method of breeding.",https://en.wikipedia.org/wiki/Truncation_selection
BKM algorithm,"The BKM algorithm is a shift-and-add algorithm for computing elementary functions, first published in 1994 by Jean-Claude Bajard, Sylvanus Kla, and Jean-Michel Muller.  BKM is based on computing complex logarithms (L-mode) and exponentials (E-mode) using a method similar to the algorithm Henry Briggs used to compute logarithms.  By using a precomputed table of logarithms of negative powers of two, the BKM algorithm computes elementary functions using only integer add, shift, and compare operations.",https://en.wikipedia.org/wiki/BKM_algorithm
Non-negative matrix factorization,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.",https://en.wikipedia.org/wiki/Non-negative_matrix_factorization
Structured kNN,"Structured k-Nearest Neighbours is a machine learning algorithm that generalizes the k-Nearest Neighbors (kNN) classifier.Whereas the kNN classifier supports binary classification, multiclass classification and regression, the Structured kNN (SkNN) allows training of a classifier for general structured output labels.",https://en.wikipedia.org/wiki/Structured_kNN
Language model,"A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability P(w1,…,wm),\ldots ,w_{m})} to the whole sequence.",https://en.wikipedia.org/wiki/Language_model
Metropolis–Hastings algorithm,"In statistics and statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value).  Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high.  For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods.",https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm
Path-based strong component algorithm,"In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra's version was the first to achieve linear time.",https://en.wikipedia.org/wiki/Path-based_strong_component_algorithm
Partial least squares regression,"Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical.",https://en.wikipedia.org/wiki/Partial_least_squares_regression
XOR swap algorithm,"In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. ""Distinct"" means that the variables are stored at different, non-overlapping, memory addresses as the algorithm would set a single aliased value to zero; the actual values of the variables do not have to be different.",https://en.wikipedia.org/wiki/Xor_swap_algorithm
Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.",https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
Time complexity,"In computer science, the time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.",https://en.wikipedia.org/wiki/Polynomial_time
Daitch–Mokotoff Soundex,Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch.  It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.,https://en.wikipedia.org/wiki/Daitch%E2%80%93Mokotoff_Soundex
Quicksort,,https://en.wikipedia.org/wiki/Quicksort
Stability (learning theory),"Stability, also known as algorithmic stability, is a notion in computational learning theory of how a  machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (""A"" to ""Z"") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.",https://en.wikipedia.org/wiki/Stability_(learning_theory)
Information gain in decision trees,"In information theory and machine learning, information gain is a synonym for Kullback–Leibler divergence; the amount of information gained about a random variable or signal from observing another random variable.  However, in the context of decision trees, the term is sometimes used synonymously with mutual information, which is the conditional expected value of the Kullback–Leibler divergence of the univariate probability distribution of one variable from the conditional distribution of this variable given the other one.",https://en.wikipedia.org/wiki/Information_gain_in_decision_trees
Scale-invariant feature transform,"The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in images. It was patented in Canada by the University of British Columbia and published by David Lowe in 1999.Applications include object recognition, robotic mapping and navigation, image stitching, 3D modeling, gesture recognition, video tracking, individual identification of wildlife and match moving.",https://en.wikipedia.org/wiki/Scale-invariant_feature_transform
Ruppert's algorithm,"In mesh generation, Ruppert's algorithm, also known as Delaunay refinement, is an algorithm for creating quality Delaunay triangulations.  The algorithm takes a planar straight-line graph (or in dimension higher than two a piecewise linear system) and returns a conforming Delaunay triangulation of only quality triangles.  A triangle is considered poor-quality if it has a circumradius to shortest edge ratio larger than some prescribed threshold.  Discovered by Jim Ruppert in the early 1990s,""Ruppert's algorithm for two-dimensional quality mesh generation is perhaps the first theoretically guaranteed meshing algorithm to be truly satisfactory in practice.""",https://en.wikipedia.org/wiki/Ruppert%27s_algorithm
Binary classification,Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.,https://en.wikipedia.org/wiki/Binary_classification
Dorothea Wagner,"Dorothea Wagner (born 1957) is a German computer scientist, known for her research in graph drawing, route planning, and social network analysis. She heads the Institute of Theoretical Informatics at the Karlsruhe Institute of Technology.",https://en.wikipedia.org/wiki/Dorothea_Wagner
Metadata,,https://en.wikipedia.org/wiki/Metadata
Fortuna (PRNG),"Fortuna is a cryptographically secure pseudorandom number generator (PRNG) devised by Bruce Schneier and Niels Ferguson and published in 2003. It is named after Fortuna, the Roman goddess of chance.",https://en.wikipedia.org/wiki/Fortuna_(PRNG)
Exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.",https://en.wikipedia.org/wiki/Exponential_backoff
Grover's algorithm,"Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N) evaluations of the function, where N is the size of the function's domain. It was devised by Lov Grover in 1996.",https://en.wikipedia.org/wiki/Grover%27s_algorithm
X-ray crystallography,"X-ray crystallography (XRC) is the experimental science determining the atomic and molecular structure of a crystal, in which the crystalline structure causes a beam of incident X-rays to diffract into many specific directions. By measuring the angles and intensities of these diffracted beams, a crystallographer can produce a three-dimensional picture of the density of electrons within the crystal. From this electron density, the mean positions of the atoms in the crystal can be determined, as well as their chemical bonds, their crystallographic disorder, and various other information.",https://en.wikipedia.org/wiki/X-ray_crystallography
Algorithms for calculating variance,"Algorithms for calculating variance play a major role in computational statistics. A key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.",https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.",https://en.wikipedia.org/wiki/Restoring_division
RC4,,https://en.wikipedia.org/wiki/RC4_(cipher)
Language identification in the limit,"Language identification in the limit is a formal model for inductive inference of formal languages, mainly by computers (see machine learning). It was introduced by E. Mark Gold in a technical report and a journal article with the same title.",https://en.wikipedia.org/wiki/Language_identification_in_the_limit
Elastic matching,"Elastic matching is one of the pattern recognition techniques in computer science. Elastic matching (EM) is also known as deformable template, flexible matching, or nonlinear template matching.",https://en.wikipedia.org/wiki/Elastic_matching
Gauss–Seidel method,"In numerical linear algebra, the Gauss–Seidel method, also known as the Liebmann method or the method of successive displacement,  is an iterative method used to solve a linear system of equations. It is named after the German mathematicians Carl Friedrich Gauss and Philipp Ludwig von Seidel, and is similar to the Jacobi method. Though it can be applied to any matrix with non-zero elements on the diagonals, convergence is only guaranteed if the matrix is either diagonally dominant, or symmetric and positive definite. It was only mentioned in a private letter from Gauss to his student Gerling in 1823. A publication was not delivered before 1874 by Seidel.",https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method
Chaitin's algorithm,"Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. It is named after its designer, Gregory Chaitin. Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling. ",https://en.wikipedia.org/wiki/Chaitin%27s_algorithm
Cache replacement policies,,https://en.wikipedia.org/wiki/Cache_algorithms
Hidden Markov model,,https://en.wikipedia.org/wiki/Hidden_Markov_model
Christopher Bishop,,https://en.wikipedia.org/wiki/Christopher_Bishop
Vision processing unit,"A vision processing unit (VPU) is (as of 2018) an emerging class of microprocessor; it is a specific type of AI accelerator,[failed verification] designed to accelerate machine vision tasks.",https://en.wikipedia.org/wiki/Vision_processing_unit
Unary coding,"Unary coding,[nb 1]  or the unary numeral system and also sometimes called thermometer code, is an entropy encoding that represents a natural number, n, with n ones followed by a zero (if natural number is understood as non-negative integer) or with n − 1 ones followed by a zero (if natural number is understood as strictly positive integer).  For example 5 is represented as 111110 or 11110. Some representations use n or n − 1 zeros followed by a one. The ones and zeros are interchangeable without loss of generality. Unary coding is both a prefix-free code and a self-synchronizing code.",https://en.wikipedia.org/wiki/Unary_coding
Long short-term memory,"Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDS's (intrusion detection systems).",https://en.wikipedia.org/wiki/Long_short-term_memory
Hard disk drive performance characteristics,Higher performance in hard disk drives comes from devices which have better performance characteristics. These performance characteristics can be grouped into two categories: access time and data transfer time (or rate).,https://en.wikipedia.org/wiki/Seek_time
Geworkbench,"geWorkbench (genomics Workbench) is an open-source software platform for integrated genomic data analysis.  It is a desktop application written in the programming language Java. geWorkbench uses a component architecture. As of 2016, there are more than 70 plug-ins available, providing for the visualization and analysis of gene expression, sequence, and structure data.",https://en.wikipedia.org/wiki/Geworkbench
Feedforward neural network,"A feedforward neural network is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from its  descendant: recurrent neural networks.",https://en.wikipedia.org/wiki/Feedforward_neural_network
Beam tracing,"Beam tracing is an algorithm to simulate wave propagation.It was developed in the context of computer graphics to render 3D scenes,but it has been also used in other similar areas such as acoustics andelectromagnetism simulations.",https://en.wikipedia.org/wiki/Beam_tracing
Greatest common divisor,,https://en.wikipedia.org/wiki/Greatest_common_divisor
Frrole,"Frrole, Inc. is a Palo Alto based social intelligence company founded in Jan 2014.  It provides contextual topical and people insights to brands, media and technology companies by analyzing social data in real-time. In addition to its standard API provision, Frrole also provides custom built command centers for its clients. The bulk of Frrole’s operations is carried out from its office in Bangalore, India.",https://en.wikipedia.org/wiki/Frrole
Labeled data,"Labeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.",https://en.wikipedia.org/wiki/Labeled_data
Point set registration,"In computer vision, pattern recognition, and robotics, point set registration, also known as point cloud registration or scan matching, is the process of finding a spatial transformation (e.g., scaling, rotation and translation) that aligns two point clouds. The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model (or coordinate frame), and mapping a new measurement to a known data set to identify features or to estimate its pose. Raw 3D point cloud data are typically obtained from Lidars and RGB-D cameras. 3D point clouds can also be generated from computer vision algorithms such as triangulation, bundle adjustment, and more recently, monocular image depth estimation using deep learning. For 2D point set registration used in image processing and feature-based image registration, a point set may be 2D pixel coordinates obtained by feature extraction from an image, for example corner detection. Point cloud registration has extensive applications in autonomous driving, motion estimation and 3D reconstruction, object detection and pose estimation, robotic manipulation, simultaneous localization and mapping (SLAM), panorama stitching, virtual and augmented reality, and medical imaging.",https://en.wikipedia.org/wiki/Point_set_registration
Vector clock,"A vector clock is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, interprocess messages contain the state of the sending process's logical clock.",https://en.wikipedia.org/wiki/Vector_clocks
Halftone,"Halftone is the reprographic technique that simulates continuous-tone imagery through the use of dots, varying either in size or in spacing, thus generating a gradient-like effect. ""Halftone"" can also be used to refer specifically to the image that is produced by this process.",https://en.wikipedia.org/wiki/Half-toning
Aphelion (software),"The Aphelion Imaging Software Suite is a software suite that includes three base products -  Aphelion Lab, Aphelion Dev, and Aphelion SDK for addressing image processing and image analysis applications. The suite also includes a set of extension programs to implement specific vertical applications that benefit from imaging techniques.",https://en.wikipedia.org/wiki/Aphelion_(software)
Logistic regression,"In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.  This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.  Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.",https://en.wikipedia.org/wiki/Logistic_regression
Logistic model tree,"In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning.",https://en.wikipedia.org/wiki/Logistic_model_tree
Whirlpool (hash function),"In computer science and cryptography, Whirlpool (sometimes styled WHIRLPOOL) is a cryptographic hash function. It was designed by Vincent Rijmen (co-creator of  the Advanced Encryption Standard) and Paulo S. L. M. Barreto, who first described it in 2000. ",https://en.wikipedia.org/wiki/WHIRLPOOL
Confirmatory factor analysis,"In statistics, confirmatory factor analysis (CFA) is a special form of factor analysis, most commonly used in social research. It is used to test whether measures of a construct are consistent with a researcher's understanding of the nature of that construct (or factor). As such, the objective of confirmatory factor analysis is to test whether the data fit a hypothesized measurement model. This hypothesized model is based on theory and/or previous analytic research. CFA was first developed by Jöreskog and has built upon and replaced older methods of analyzing construct validity such as the MTMM Matrix as described in Campbell & Fiske (1959).",https://en.wikipedia.org/wiki/Confirmatory_factor_analysis
ID3 algorithm,"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.",https://en.wikipedia.org/wiki/Iterative_Dichotomiser_3
Persian Speech Corpus,"The Persian Speech Corpus is a Modern Persian speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of about 2.5 hours of Persian speech aligned with recorded speech on the phoneme level, including annotations of word boundaries. Previous spoken corpora of Persian include FARSDAT, which consists of read aloud speech from newspaper texts from 100 Persian speakers and the Telephone FARsi Spoken language DATabase (TFARSDAT) which comprises seven hours of read and spontaneous speech produced by 60 native speakers of Persian from ten regions of Iran.",https://en.wikipedia.org/wiki/Persian_Speech_Corpus
Burst error,"In telecommunication, a burst error or error burst is a contiguous sequence of symbols, received over a communication channel, such that the first and last symbols are in error and there exists no contiguous subsequence of m correctly received symbols within the error burst.",https://en.wikipedia.org/wiki/Burst_error
Metropolis light transport,Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis–Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.,https://en.wikipedia.org/wiki/Metropolis_light_transport
Bayesian statistics,"Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses a degree of belief in an event. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. This differs from a number of other interpretations of probability, such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials.",https://en.wikipedia.org/wiki/Bayesian_statistics
Weasel program,The weasel program or Dawkins' weasel is a thought experiment and a variety of computer simulations illustrating it.  Their aim is to demonstrate that the process that drives evolutionary systems—random variation combined with non-random cumulative selection—is different from pure chance.,https://en.wikipedia.org/wiki/Weasel_program
Decision stump,"A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules.",https://en.wikipedia.org/wiki/Decision_stump
R (programming language),,https://en.wikipedia.org/wiki/R_(programming_language)
Random subspace method,"In machine learning the random subspace method, also called attribute bagging or feature bagging, is an ensemble learning method that attempts to reduce the correlation between estimators in an ensemble by training them on random samples of features instead of the entire feature set.",https://en.wikipedia.org/wiki/Random_subspace_method
Tonelli–Shanks algorithm,"The Tonelli–Shanks algorithm (referred to by Shanks as the RESSOL algorithm) is used in modular arithmetic to solve for r in a congruence of the form r2 ≡ n (mod p), where p is a prime: that is, to find a square root of n modulo p.",https://en.wikipedia.org/wiki/Tonelli%E2%80%93Shanks_algorithm
Repertory grid,The repertory grid is an interviewing technique which uses nonparametric factor analysis to determine an idiographic measure of personality. It was devised by George Kelly in around 1955 and is based on his personal construct theory of personality.,https://en.wikipedia.org/wiki/Repertory_grid
Kahan summation algorithm,"In numerical analysis, the Kahan summation algorithm, also known as compensated summation, significantly reduces the numerical error in the total obtained by adding a sequence of finite-precision floating-point numbers, compared to the obvious approach. This is done by keeping a separate running compensation (a variable to accumulate small errors).",https://en.wikipedia.org/wiki/Kahan_summation_algorithm
D*,"D* (pronounced ""D star"") is any one of the following three related incremental search algorithms:",https://en.wikipedia.org/wiki/D*
Health informatics,"Health informatics (also called health care informatics, healthcare informatics, medical informatics, nursing informatics,  clinical informatics, or biomedical informatics) is information engineering applied to the field of health care, essentially the management and use of patient health care information. It is a multidisciplinary field that uses health information technology (HIT) to improve health care via any combination of higher quality, higher efficiency (spurring lower cost and thus greater availability), and new opportunities. The disciplines involved include information science, computer science, social science, behavioral science, management science, and others. The United States National Library of Medicine (NLM) defines health informatics as ""the interdisciplinary study of the design, development, adoption and application of IT-based innovations in health care services delivery, management and planning"". It deals with the resources, devices, and methods required to optimize the acquisition, storage, retrieval, and use of information in health and bio-medicine. Health informatics tools include computers, clinical guidelines, formal medical terminologies, and information and communication systems, among others. It is applied to the areas of nursing, clinical medicine, dentistry, pharmacy, public health, occupational therapy, physical therapy, biomedical research, and alternative medicine,[unreliable medical source?] all of which are designed to improve the overall of effectiveness of patient care delivery by ensuring that the data generated is of a high quality.",https://en.wikipedia.org/wiki/Biomedical_informatics
Hyperparameter (machine learning),"In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.",https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)
String-searching algorithm,"In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.",https://en.wikipedia.org/wiki/Substring_search
Integer programming,"An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.",https://en.wikipedia.org/wiki/Integer_linear_programming
k-nearest neighbors algorithm,"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.",https://en.wikipedia.org/wiki/K-nearest_neighbors_classification
Relevance vector machine,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.",https://en.wikipedia.org/wiki/Relevance_vector_machine
Database transaction,"A database transaction symbolizes a unit of work performed within a database management system (or similar system) against a database, and treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in a database.",https://en.wikipedia.org/wiki/Transaction_(database)
Constrained Delaunay triangulation,"In computational geometry, a constrained Delaunay triangulation is a generalization of the Delaunay triangulation that forces certain required segments into the triangulation. Because a Delaunay triangulation is almost always unique, often a constrained Delaunay triangulation contains edges that do not satisfy the Delaunay condition. Thus a constrained Delaunay triangulation often is not a Delaunay triangulation itself.",https://en.wikipedia.org/wiki/Constrained_Delaunay_triangulation
Explanation-based learning,"Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory in order to make generalizations or form concepts from training examples.",https://en.wikipedia.org/wiki/Explanation-based_learning
Inverse iteration,"In numerical analysis, inverse iteration (also known as the inverse power method) is an iterative eigenvalue algorithm. It allows one to find an approximateeigenvector when an approximation to a corresponding eigenvalue is already known.The method is conceptually similar to  the power method.It appears to have originally been developed to compute resonance frequencies in the field of structural mechanics.",https://en.wikipedia.org/wiki/Inverse_iteration
Steve Omohundro,"Stephen M. Omohundro (born 1959) is an American computer scientist whose areas of research include Hamiltonian physics, dynamical systems, programming languages, machine learning, machine vision, and the social implications of artificial intelligence. His current work uses rational economics to develop safe and beneficial intelligent technologies for better collaborative modeling, understanding, innovation, and decision making.",https://en.wikipedia.org/wiki/Steve_Omohundro
Machine translation,,https://en.wikipedia.org/wiki/Machine_translation
Simple LR parser,"In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm.  As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse in a single left-to-right scan over the input stream, without guesswork or backtracking.  The parser is mechanically generated from a formal grammar for the language.",https://en.wikipedia.org/wiki/Simple_LR_parser
Adaptive resonance theory,"Adaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.",https://en.wikipedia.org/wiki/Adaptive_resonance_theory
Self-organizing map,"A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.",https://en.wikipedia.org/wiki/Self-organizing_map
LZRW,"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques.  This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991. ",https://en.wikipedia.org/wiki/LZRW
Special number field sieve,"In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm. The general number field sieve (GNFS) was derived from it.",https://en.wikipedia.org/wiki/Special_number_field_sieve
Motion planning,Motion planning (also known as the navigation problem or the piano mover's problem) is a term used in robotics is to find a sequence of valid configurations that moves the robot from the source to destination.,https://en.wikipedia.org/wiki/Motion_planning
Fish School Search,"Fish School Search (FSS), proposed by Bastos Filho and Lima Neto in 2007 is, in its basic version, an unimodal optimization algorithm inspired on the collective behavior of fish schools. The mechanisms of feeding and coordinated movement were used as inspiration to create the search operators. The core idea is to make the fishes “swim” toward the positive gradient in order to “eat” and “gain weight”. Collectively, the heavier fishes are more influent in the search process as a whole, what makes the barycenter of the fish school moves toward better places in the search space over the iterations.",https://en.wikipedia.org/wiki/Fish_School_Search
Linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.",https://en.wikipedia.org/wiki/Linear_discriminant_analysis
Arabic Speech Corpus,The Arabic Speech Corpus is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes.,https://en.wikipedia.org/wiki/Arabic_Speech_Corpus
Prediction by partial matching,Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream.  PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis.,https://en.wikipedia.org/wiki/PPM_compression_algorithm
Probabilistic latent semantic analysis,"Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.",https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis
Zoubin Ghahramani,,https://en.wikipedia.org/wiki/Zoubin_Ghahramani
Randomized weighted majority algorithm,The randomized weighted majority algorithm is an algorithm in machine learning theory.It improves the mistake bound of the weighted majority algorithm.,https://en.wikipedia.org/wiki/Randomized_weighted_majority_algorithm
Coupled pattern learner,Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.,https://en.wikipedia.org/wiki/Coupled_pattern_learner
Partially ordered set,"In mathematics, especially order theory, a partially ordered set (also poset) formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a set. A poset consists of a set together with a binary relation indicating that, for certain pairs of elements in the set, one of the elements precedes the other in the ordering. The relation itself is called a ""partial order."" The word partial in the names ""partial order"" and ""partially ordered set"" is used as an indication that not every pair of elements needs to be comparable. That is, there may be pairs of elements for which neither element precedes the other in the poset. Partial orders thus generalize total orders, in which every pair is comparable.  ",https://en.wikipedia.org/wiki/Partial_order
Approximate counting algorithm,"The approximate counting algorithm allows the counting of a large number of events using a small amount of memory.  Invented in 1977 by Robert Morris (cryptographer) of Bell Labs, it uses probabilistic techniques to increment the counter.  It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, who coined the name approximate counting, and strongly contributed to its recognition among the research community.  The algorithm is considered one of the precursors of streaming algorithms, and the more general problem of determining the frequency moments of a data stream has been central to the field.",https://en.wikipedia.org/wiki/Approximate_counting_algorithm
Spike-and-slab regression,"In statistics, spike-and-slab regression is a Bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations.",https://en.wikipedia.org/wiki/Spike-and-slab_variable_selection
Expectation–maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm
Induction of regular languages,"In computational learning theory, induction of regular languages refers to the task of learning a formal description (e.g. grammar) of a regular language from a given set of example strings. Although Mark E. Gold has shown that not every regular language can be learned this way (see language identification in the limit), approaches have been investigated for a variety of subclasses. They are sketched in this article. For learning of more general grammars, see Grammar induction.",https://en.wikipedia.org/wiki/Induction_of_regular_languages
Dancing Links,"In computer science, dancing links is a technique for reverting the operation of deleting a node from a circular doubly linked list. It is particularly useful for efficiently implementing backtracking algorithms, such as Donald Knuth's Algorithm X for the exact cover problem. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.",https://en.wikipedia.org/wiki/Dancing_Links
Lattice Miner,"Lattice Miner  is a formal concept analysis software tool for the construction, visualization and manipulation of concept lattices. It allows the generation of formal concepts and association rules as well as the transformation of formal contexts via apposition, subposition, reduction and object/attribute generalization, and the manipulation of concept lattices via approximation, projection and selection. Lattice Miner allows also the drawing of nested line diagrams.",https://en.wikipedia.org/wiki/Lattice_Miner
Rayleigh quotient iteration,Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates.,https://en.wikipedia.org/wiki/Rayleigh_quotient_iteration
Nonblocking minimal spanning switch,"A nonblocking minimal spanning switch is a device that can connect N inputs to N outputs in any combination.  The most familiar use of switches of this type is in a telephone exchange.  The term ""non-blocking"" means that if it is not defective, it can always make the connection.  The term ""minimal"" means that it has the fewest possible components, and therefore the minimal expense.",https://en.wikipedia.org/wiki/Nonblocking_minimal_spanning_switch
Andrew McCallum,"Andrew McCallum is a professor and researcher in the computer science department at University of Massachusetts Amherst. His primary specialties are in machine learning, natural language processing, information extraction, information integration, and social network analysis.",https://en.wikipedia.org/wiki/Andrew_McCallum
Aho–Corasick algorithm,"In computer science, the Aho–Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the ""dictionary"") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = .mw-parser-output .monospaced{font-family:monospace,monospace}a, aa, aaa, aaaa and input string is aaaa).",https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/0-471-05669-3
Models of DNA evolution,"A number of different Markov models of DNA sequence evolution have been proposed.  These substitution models differ in terms of the parameters used to describe the rates at which one nucleotide replaces another during evolution. These models are frequently used in molecular phylogenetic analyses. In particular, they are used during the calculation of likelihood of a tree (in Bayesian and maximum likelihood approaches to tree estimation) and they are used to estimate the evolutionary distance between sequences from the observed differences between the sequences.",https://en.wikipedia.org/wiki/Models_of_DNA_evolution
Yooreeka,"Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book ""Algorithms of the Intelligent Web"". Although the term ""Web"" prevailed in the title, in essence, the algorithms are valuable in any software application.",https://en.wikipedia.org/wiki/Yooreeka
Learning to rank,"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is ""similar"" to rankings in the training data in some sense.",https://en.wikipedia.org/wiki/Learning_to_rank
CMA-ES,"CMA-ES stands for covariance matrix adaptation evolution strategy. Evolution strategies (ES) are stochastic, derivative-free methods for numerical optimization  of non-linear or non-convex continuous optimization problems. They belong to the class of evolutionary algorithms and evolutionary computation.  An evolutionary algorithm is broadly based on the principle of biological evolution, namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as x) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or objective function value f(x). Like this, over the generation sequence, individuals with better and better f-values are generated.",https://en.wikipedia.org/wiki/CMA-ES
Forward–backward algorithm,"The forward–backward algorithm is an  inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions o1:T:=o1,…,oT,\dots ,o_{T}}, i.e. it computes, for all hidden state variables Xt∈{X1,…,XT},\dots ,X_{T}\}}, the distribution P(Xt | o1:T). This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.",https://en.wikipedia.org/wiki/Forward-backward_algorithm
Sequence assembly,"In bioinformatics, sequence assembly refers to aligning and merging fragments from a longer DNA sequence in order to reconstruct the original sequence. This is needed as DNA sequencing technology cannot read whole genomes in one go, but rather reads small pieces of between 20 and 30,000 bases, depending on the technology used. Typically the short fragments, called reads, result from shotgun sequencing genomic DNA, or gene transcript (ESTs).",https://en.wikipedia.org/wiki/Sequence_assembly
Constrained conditional model,A constrained conditional model (CCM) is a machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive[clarification needed] prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference.,https://en.wikipedia.org/wiki/Constrained_conditional_model
Google Brain,"Google Brain is a deep learning artificial intelligence research team at Google. Formed in the early 2010s, Google Brain combines open-ended machine learning research with systems engineering and Google-scale computing resources.",https://en.wikipedia.org/wiki/Google_Brain
Cryptographic hash function,"A cryptographic hash function (CHF) is a hash function that is suitable for use in cryptography. It is a mathematical algorithm that maps data of arbitrary size (often called the ""message"") to a bit string of a fixed size (the ""hash value"", ""hash"", or ""message digest"") and is a one-way function, that is, a function which is practically infeasible to invert. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography.",https://en.wikipedia.org/wiki/Cryptographic_hash_function
Detailed balance,"The principle of detailed balance can be used in kinetic systems which are decomposed into elementary processes (collisions, or steps, or elementary reactions). It states that at equilibrium, each elementary process is in equilibrium with its reverse process.",https://en.wikipedia.org/wiki/Detailed_balance
Runge–Kutta methods,"In numerical analysis, the Runge–Kutta methods  are a family of implicit and explicit iterative methods, which include the well-known routine called the Euler Method, used in temporal discretization for the approximate solutions of ordinary differential equations. These methods were developed around 1900 by the German mathematicians Carl Runge and Wilhelm Kutta.",https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods
SipHash,"SipHash is an add–rotate–xor (ARX) based family of pseudorandom functions created by Jean-Philippe Aumasson and Daniel J. Bernstein in 2012,:165 in response to a spate of ""hash flooding"" denial-of-service attacks in late 2011.",https://en.wikipedia.org/wiki/SipHash
Online algorithm,"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.",https://en.wikipedia.org/wiki/Online_algorithm
SNNS,"SNNS (Stuttgart Neural Network Simulator) is a neural network simulator originally developed at the University of Stuttgart. While it was originally built for X11 under Unix, there are Windows ports[citation needed]. Its successor JavaNNS never reached the same popularity.",https://en.wikipedia.org/wiki/SNNS
Intersection algorithm,"The intersection algorithm is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources, it forms part of the modern Network Time Protocol. It is a modified form of Marzullo's algorithm.",https://en.wikipedia.org/wiki/Intersection_algorithm
Geometric hashing,"In computer science, geometric hashing is originally a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation (example below is based on similarity transformation), though extensions exist to some other object representations and transformations. In an off-line step, the objects are encoded by treating each pair of points as a geometric basis. The remaining points can be represented in an invariant fashion with respect to this basis using two parameters. For each point, its quantized transformed coordinates are stored in the hash table as a key, and indices of the basis points as a value. Then a new pair of basis points is selected, and the process is repeated. In the on-line (recognition) step, randomly selected pairs of data points are considered as candidate bases. For each candidate basis, the remaining data points are encoded according to the basis and possible correspondences from the object are found in the previously constructed table. The candidate basis is accepted if a sufficiently large number of the data points index a consistent object basis.",https://en.wikipedia.org/wiki/Geometric_hashing
Nuisance variable,"In the theory of stochastic processes in probability theory and statistics, a nuisance variable is a random variable that is fundamental to the probabilistic model, but that is of no particular interest in itself or is no longer of interest: one such usage arises for the Chapman–Kolmogorov equation. For example, a model for a stochastic process may be defined conceptually using intermediate variables that are not observed in practice. If the problem is to derive the theoretical properties, such as the mean, variance and covariances of quantities that would be observed, then the intermediate variables are nuisance variables.",https://en.wikipedia.org/wiki/Nuisance_variable
Highway network,"In machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (""information highways"").",https://en.wikipedia.org/wiki/Highway_network
Poly1305,Poly1305 is a cryptographic message authentication code (MAC) created by Daniel J. Bernstein. It can be used to verify the data integrity and the authenticity of a message. A variant of Bernstein's Poly1305 that does not require AES has been standardized by the Internet Engineering Task Force in RFC 8439.,https://en.wikipedia.org/wiki/Poly1305
General number field sieve,"In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 10100. Heuristically, its complexity for factoring an integer n (consisting of ⌊log2 n⌋ + 1 bits) is of the form",https://en.wikipedia.org/wiki/General_number_field_sieve
Vasant Honavar,"Vasant G. Honavar is an Indian born American computer scientist, and artificial intelligence, machine learning, big data, data science, causality, knowledge representation, bioinformatics and health informatics researcher and educator.",https://en.wikipedia.org/wiki/Vasant_Honavar
Mountain car problem,,https://en.wikipedia.org/wiki/Mountain_car_problem
Instance-based learning,"In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.",https://en.wikipedia.org/wiki/Instance-based_learning
Vector optimization,"Vector optimization is a subarea of mathematical optimization where optimization problems with a vector-valued objective functions are optimized with respect to a given partial ordering and subject to certain constraints.  A multi-objective optimization problem is a special case of a vector optimization problem: The objective space is the finite dimensional Euclidean space partially ordered by the component-wise ""less than or equal to"" ordering. ",https://en.wikipedia.org/wiki/Vector_optimization
Linear predictive coding,"Linear predictive coding (LPC) is a method used mostly in audio signal processing and speech processing for representing the spectral envelope of a digital signal of speech in compressed form, using the information of a linear predictive model. It is one of the most powerful speech analysis techniques, and one of the most useful methods for encoding good quality speech at a low bit rate and provides highly accurate estimates of speech parameters. LPC is the most widely used method in speech coding and speech synthesis.",https://en.wikipedia.org/wiki/Linear_predictive_coding
Feature (machine learning),"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.",https://en.wikipedia.org/wiki/Feature_space
Iterative deepening A*,"Iterative deepening A* (IDA*) is a graph traversal and path search algorithm that can find the shortest path between a designated start node and any member of a set of goal nodes in a weighted graph. It is a variant of iterative deepening depth-first search that borrows the idea to use a heuristic function to evaluate the remaining cost to get to the goal from the A* search algorithm. Since it is a depth-first search algorithm, its memory usage is lower than in A*, but unlike ordinary iterative deepening search, it concentrates on exploring the most promising nodes and thus does not go to the same depth everywhere in the search tree. Unlike A*, IDA* does not utilize dynamic programming and therefore often ends up exploring the same nodes many times.",https://en.wikipedia.org/wiki/Iterative_deepening_A*
Wang and Landau algorithm,"The Wang and Landau algorithm, proposed by  Fugao Wang and David P. Landau, is a Monte Carlo method designed to estimate the density of states of a system. The method performs a non-Markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.",https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm
Strand sort,Strand sort is a recursive sorting algorithm that sorts items of a list into increasing order. It has O(n2) worst time complexity which occurs when the input list is reverse sorted. It has a best case time complexity of O(n) which occurs when the input is a list that is already sorted. Strand sort is not in-place as its space complexity is O(n).,https://en.wikipedia.org/wiki/Strand_sort
Pattern language (formal languages),"In theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning.",https://en.wikipedia.org/wiki/Pattern_language_(formal_languages)
Bead sort,"Bead sort, also called gravity sort, is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science.  Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers.  Also, it would seem that even in the best case, the algorithm requires O(n2) space.",https://en.wikipedia.org/wiki/Bead_sort
Tensor processing unit,A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning.,https://en.wikipedia.org/wiki/Tensor_processing_unit
Computer vision,"Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.",https://en.wikipedia.org/wiki/Image_recognition
Stochastic block model,"The stochastic block model is a generative model for random graphs. This model tends to produce graphs containing communities, subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data.",https://en.wikipedia.org/wiki/Stochastic_block_model
Normal mapping,"In 3D computer graphics, normal mapping, or Dot3 bump mapping, is a technique used for faking the lighting of bumps and dents – an implementation of bump mapping. It is used to add details without using more polygons. A common use of this technique is to greatly enhance the appearance and details of a low polygon model by generating a normal map from a high polygon model or height map.",https://en.wikipedia.org/wiki/Normal_mapping
AlchemyAPI,"AlchemyAPI is an IBM-owned company that uses machine learning (specifically, deep learning) to do natural language processing (specifically, semantic text analysis, including sentiment analysis) and computer vision (specifically, face detection and recognition) for its clients both over the cloud and on-premises.",https://en.wikipedia.org/wiki/AlchemyAPI
Artificial intelligence,,https://en.wikipedia.org/wiki/Artificial_intelligence
Longest common subsequence problem,"The longest common subsequence (LCS) problem is the problem of finding the longest subsequence common to all sequences in a set of sequences (often just two sequences). It differs from the longest common substring problem: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences. The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics. It is also widely used by revision control systems such as Git for reconciling multiple changes made to a revision-controlled collection of files. ",https://en.wikipedia.org/wiki/Longest_common_subsequence_problem
Movidius,"Movidius is a company based in San Mateo, California that designs specialised low-power processor chips for computer vision. The company was acquired by Intel in September 2016.",https://en.wikipedia.org/wiki/Movidius
Natural evolution strategy,"Natural evolution strategies (NES) are a family of numerical optimization algorithms for black box problems. Similar in spirit to evolution strategies, they iteratively update the (continuous) parameters of a search distribution by following the natural gradient towards higher expected fitness.",https://en.wikipedia.org/wiki/Natural_evolution_strategy
wildmat,,https://en.wikipedia.org/wiki/Wildmat
LL parser,"In computer science, an LL parser (Left-to-right, Leftmost derivation) is a top-down parser for a subset of context-free languages. It parses the input from Left to right, performing Leftmost derivation of the sentence.",https://en.wikipedia.org/wiki/LL_parser
Berlekamp–Massey algorithm,The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence. The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field.  The field requirement means that the Berlekamp–Massey algorithm requires all non-zero elements to have a multiplicative inverse.  Reeds and Sloane offer an extension to handle a ring.,https://en.wikipedia.org/wiki/Berlekamp%E2%80%93Massey_algorithm
"Training, validation, and test sets","In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.",https://en.wikipedia.org/wiki/Validation_set
Cubic Hermite spline,"In numerical analysis, a cubic Hermite spline or cubic Hermite interpolator is a spline where each piece is a third-degree polynomial specified in Hermite form: i.e., by its values and first derivatives at the end points of the corresponding domain interval.",https://en.wikipedia.org/wiki/Cubic_interpolation
Eric Xing,"Eric Xing is a professor at Carnegie Mellon University and researcher in machine learning, computational biology, and statistical methodology.",https://en.wikipedia.org/wiki/Eric_Xing
Longest common substring problem,"In computer science, the longest common substring problem is to find the longest string (or strings) that is a substring (or are substrings) of two or more strings.",https://en.wikipedia.org/wiki/Longest_common_substring_problem
Maximum parsimony (phylogenetics),"In phylogenetics, maximum parsimony is an optimality criterion under which the phylogenetic tree that minimizes the total number of character-state changes is to be preferred.  Under the maximum-parsimony criterion, the optimal tree will minimize the amount of homoplasy (i.e., convergent evolution, parallel evolution, and evolutionary reversals).  In other words, under this criterion, the shortest possible tree that explains the data is considered best.  The principle is akin to Occam's razor, which states that—all else being equal—the simplest hypothesis that explains the data should be selected. Some of the basic ideas behind maximum parsimony were presented by James S. Farris  in 1970 and Walter M. Fitch in 1971.",https://en.wikipedia.org/wiki/Maximum_parsimony_(phylogenetics)
Apache Giraph,Apache Giraph is an Apache project to perform graph processing on big data. Giraph utilizes Apache Hadoop's MapReduce implementation to process graphs. Facebook used Giraph with some performance improvements to analyze one trillion edges using 200 machines in 4 minutes. Giraph is based on a paper published by Google about its own graph processing system called Pregel. It can be compared to other Big Graph processing libraries such as Cassovary.,https://en.wikipedia.org/wiki/Apache_Giraph
Nonlinear programming,"In mathematics, nonlinear programming (NLP) is the process of solving an optimization problem where some of the constraints or the objective function are nonlinear. An optimization problem is one of calculation of the extrema (maxima, minima or stationary points) of an objective function over a set of unknown real variables and conditional to the satisfaction of a system of equalities and inequalities, collectively termed constraints. It is the sub-field of mathematical optimization that deals with problems that are not linear.",https://en.wikipedia.org/wiki/Nonlinear_optimization
Gradient boosting,"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.",https://en.wikipedia.org/wiki/Gradient_boosting
Uniform binary search,"Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; therefore, it is optimized for architectures (such as Knuth's MIX) on which",https://en.wikipedia.org/wiki/Uniform_binary_search
Gesture Description Language,"Gesture Description Language (GDL or GDL Technology) is a method of describing and automatic (computer) syntactic classification of gestures and movements createdby doctor Tomasz Hachaj (PhD) and professor Marek R. Ogiela(PhD, DSc).GDL uses context-free formal grammar named GDLs (Gesture Description Language script). With GDLs it is possible to define rules that describe set of gestures. Those rules play similar role as rules in classic expert systems. With rules it is possible to define static body positions (so called key frames) and sequences of key frames that create together definitions of gestures or movements. The recognition is done by forward chaining inference engine. The latest GDL implementations utilize Microsoft Kinect controller and enable real time classification. The license for GDL-based software allows using those programs for educational and scientific purposes for free.",https://en.wikipedia.org/wiki/Gesture_Description_Language
Kriging,"In statistics, originally in geostatistics, kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process governed by prior covariances.  Under suitable assumptions on the priors, kriging gives the best linear unbiased prediction of the intermediate values.  Interpolating methods based on other criteria such as smoothness (e.g., smoothing spline) may not yield the most likely intermediate values.  The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener–Kolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov.",https://en.wikipedia.org/wiki/Gaussian_process_regression
Single-photon emission computed tomography,"Single-photon emission computed tomography (SPECT, or less commonly, SPET) is a nuclear medicine tomographic imaging technique using gamma rays. It is very similar to conventional nuclear medicine planar imaging using a gamma camera (that is, scintigraphy), but is able to provide true 3D information. This information is typically presented as cross-sectional slices through the patient, but can be freely reformatted or manipulated as required.",https://en.wikipedia.org/wiki/Single_photon_emission_computed_tomography
Tucker decomposition,"In mathematics, Tucker decomposition decomposes a tensor into a set of matrices and one small core tensor.  It is named after Ledyard R. Tuckeralthough it goes back to Hitchcock in 1927.Initially described as a three-mode extension of factor analysis and principal component analysis it may actually be generalized to higher mode analysis, which is also called Higher Order Singular Value Decomposition (HOSVD).",https://en.wikipedia.org/wiki/Tucker_decomposition
Deep learning,"Deep learning  (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.",https://en.wikipedia.org/wiki/Deep_Boltzmann_Machine
Hyper basis function network,"In machine learning, a Hyper basis function network, or HyperBF network, is a generalization of radial basis function (RBF) networks concept, where the Mahalanobis-like distance is used instead of Euclidean distance measure. Hyper basis function networks were first introduced by Poggio and Girosi in the 1990 paper “Networks for Approximation and Learning”.",https://en.wikipedia.org/wiki/Hyper_basis_function_network
Noisy text analytics,"Noisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today's knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.",https://en.wikipedia.org/wiki/Noisy_text_analytics
Entropy encoding,In information theory an  entropy encoding is a lossless data compression scheme that is independent of the specific characteristics of the medium.,https://en.wikipedia.org/wiki/Entropy_encoding
Vapnik–Chervonenkis theory,"Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.",https://en.wikipedia.org/wiki/VC_theory
Sorting algorithm,"In computer science, a sorting algorithm is an algorithm that puts elements of a list in a certain order. The most frequently used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output.",https://en.wikipedia.org/wiki/Sorted_list
Recommender system,"A recommender system, or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine), is a subclass of information filtering system that seeks to predict the ""rating"" or ""preference"" a user would give to an item. They are primarily used in commercial applications.",https://en.wikipedia.org/wiki/Content-based_filtering
mlpack,,https://en.wikipedia.org/wiki/Mlpack
Tarski–Kuratowski algorithm,In computability theory and mathematical logic the Tarski–Kuratowski algorithm is a non-deterministic algorithm which produces an upper bound for the complexity of a given formula in the arithmetical hierarchy and analytical hierarchy.,https://en.wikipedia.org/wiki/Tarski%E2%80%93Kuratowski_algorithm
Automated machine learning,Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring to become an expert in this field first. ,https://en.wikipedia.org/wiki/Automated_machine_learning
Hidden Markov random field,"In statistics, a hidden Markov random field is a generalization of a hidden Markov model.  Instead of having an underlying Markov chain, hidden Markov random fields have an  underlying Markov random field.",https://en.wikipedia.org/wiki/Hidden_Markov_random_field
Discrete phase-type distribution,"The discrete phase-type distribution is a probability distribution that results from a system of one or more inter-related geometric distributions occurring in sequence, or phases. The sequence in which each of the phases occur may itself be a stochastic process. The distribution can be represented by a random variable describing the time until absorption of an absorbing Markov chain with one absorbing state. Each of the states of the Markov chain represents one of the phases.",https://en.wikipedia.org/wiki/Discrete_phase-type_distribution
Quadratic residue,"In number theory, an integer q is called a quadratic residue modulo n if it is congruent to a perfect square modulo n; i.e.",https://en.wikipedia.org/wiki/Modular_square_root
Dehaene–Changeux model,"The Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's ""global workspace model"" for consciousness.",https://en.wikipedia.org/wiki/Dehaene%E2%80%93Changeux_model
Neural Lab,"Neural Lab is a free neural network simulator that designs and trains artificial neural networks for use in engineering, business, computer science and technology. It integrates with Microsoft Visual Studio using C (Win32 - Wintempla) to incorporate artificial neural networks into custom applications, research simulations or end user interfaces. ",https://en.wikipedia.org/wiki/Neural_Lab
Advanced Encryption Standard,"The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: ), is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.",https://en.wikipedia.org/wiki/Rijndael
Peltarion Synapse,"Synapse is a component-based development environment for neural networks and adaptive systems. Created by Peltarion, Synapse allows data mining, statistical analysis, visualization, preprocessing, design and training of neural networks and adaptive systems and the deployment of them. It utilizes a plug-in based architecture making it a general platform for signal processing. The first version of the product was released in May 2006.",https://en.wikipedia.org/wiki/Peltarion_Synapse
Dekker's algorithm,"Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication.",https://en.wikipedia.org/wiki/Dekker%27s_algorithm
Transduction (machine learning),"In logic, statistical inference, and supervised learning,transduction or transductive inference is reasoning fromobserved, specific (training) cases to specific (test) cases. In contrast,induction is reasoning from observed training casesto general rules, which are then applied to the test cases. The distinction ismost interesting in cases where the predictions of the transductive model arenot achievable by any inductive model. Note that this is caused by transductiveinference on different test sets producing mutually inconsistent predictions.",https://en.wikipedia.org/wiki/Transduction_(machine_learning)
Graph kernel,"In structure mining, a domain of learning on structured data objects in machine learning, a graph kernel is a kernel function that computes an inner product on graphs. Graph kernels can be intuitively understood as functions measuring the similarity of pairs of graphs. They allow kernelized learning algorithms such as support vector machines to work directly on graphs, without having to do feature extraction to transform them to fixed-length, real-valued feature vectors. They find applications in bioinformatics, in chemoinformatics (as a type of molecule kernels), and in social network analysis.",https://en.wikipedia.org/wiki/Graph_kernel
Binary search algorithm,,https://en.wikipedia.org/wiki/Binary_search_algorithm
Qloo,"Qloo (pronounced ""clue"") is a company that uses artificial intelligence (AI) to understand taste and cultural correlations. It provides companies with an application programming interface (API). It received funding from Leonardo DiCaprio, Elton John, Barry Sternlicht, Pierre Lagrange and others.",https://en.wikipedia.org/wiki/Qloo
Dynamic topic model,Dynamic topic models are generative models that can be used to analyze the evolution of (unobserved) topics of a collection of documents over time. This family of models was proposed by David Blei and John Lafferty and is an extension to Latent Dirichlet Allocation (LDA) that can handle sequential documents.,https://en.wikipedia.org/wiki/Dynamic_topic_model
Elias omega coding,"Elias ω coding or Elias omega coding is a universal code encoding the positive integers developed by Peter Elias. Like Elias gamma coding and Elias delta coding, it works by prefixing the integer with a representation of its order of magnitude in a universal code. Unlike those other two codes, however, Elias omega recursively encodes that prefix; thus, they are sometimes known as recursive Elias codes.",https://en.wikipedia.org/wiki/Elias_omega_coding
Graph cuts in computer vision,"As applied in the field of computer vision, graph cut optimization can be employed to efficiently solve a wide variety of low-level computer vision problems (early vision), such as image smoothing, the stereo correspondence problem, image segmentation, and many other computer vision problems that can be formulated in terms of energy minimization.  Many of these energy minimization problems can be approximated by solving a maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph).  Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution.  Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term ""graph cuts"" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms).",https://en.wikipedia.org/wiki/Graph_cuts_in_computer_vision
Occam learning,"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.",https://en.wikipedia.org/wiki/Occam_learning
Sparse dictionary learning,"Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation. ",https://en.wikipedia.org/wiki/Sparse_dictionary_learning
Activity recognition,"Activity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.",https://en.wikipedia.org/wiki/Activity_recognition
Image segmentation,"In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images.  More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.",https://en.wikipedia.org/wiki/Segmentation_(image_processing)
Tanagra (machine learning),"Tanagra is a free suite of machine learning software for research and academic purposesdeveloped by Ricco Rakotomalala at the Lumière University Lyon 2, France.Tanagra supports several standard data mining tasks such as: Visualization, Descriptive statistics, Instance selection, feature selection, feature construction, regression, factor analysis, clustering, classification and association rule learning.",https://en.wikipedia.org/wiki/Tanagra_(machine_learning)
Linear-feedback shift register,,https://en.wikipedia.org/wiki/Linear-feedback_shift_register
Knuth–Morris–Pratt algorithm,"In computer science, the Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a ""word"" W within a main ""text string"" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.",https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm
Timsort,,https://en.wikipedia.org/wiki/Timsort
Simulated annealing,"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (e.g., the traveling salesman problem). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as gradient descent.",https://en.wikipedia.org/wiki/Simulated_annealing
AlphaGo Zero,"AlphaGo Zero is a version of DeepMind's Go software AlphaGo. AlphaGo's team published an article in the journal Nature on 19 October 2017, introducing AlphaGo Zero, a version created without using data from human games, and stronger than any previous version. By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.",https://en.wikipedia.org/wiki/AlphaGo_Zero
Connect (computer system),Connect is a new social network analysis software data mining computer system developed by HMRC (UK) that cross-references business's and people's tax records with other databases to establish fraudulent or undisclosed (misdirected) activity.,https://en.wikipedia.org/wiki/Connect_(computer_system)
Multiple discriminant analysis,Multiple Discriminant Analysis (MDA) is a multivariate dimensionality reduction technique. It has been used to predict signals as diverse as neural memory traces and corporate failure.,https://en.wikipedia.org/wiki/Multiple_discriminant_analysis
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/0-19-853864-2
ECML PKDD,"ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading academic conferences on machine learning and knowledge discovery, held in Europe every year.",https://en.wikipedia.org/wiki/ECML_PKDD
Synchronous context-free grammar,"Synchronous context-free grammars (SynCFG or SCFG; not to be confused with stochastic CFGs) are a type of formal grammar designed for use in transfer-based machine translation. Rules in these grammars apply to two languages at the same time, capturing grammatical structures that are each other's translations.",https://en.wikipedia.org/wiki/Synchronous_context-free_grammar
Iterative Viterbi decoding,"Iterative Viterbi decoding is an algorithm that spots the subsequence S of an observation O = {o1, ..., on} having the highest average probability (i.e., probability scaled by the length of S) of being generated by a given hidden Markov model M with m states.  The algorithm uses a modified Viterbi algorithm as an internal step.",https://en.wikipedia.org/wiki/Iterative_Viterbi_decoding
Cuckoo search,"In operations research, cuckoo search is an optimization algorithm developed by Xin-she Yang and Suash Debin 2009. It was inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds (of other species). Some host birds can engage direct conflict with the intruding cuckoos. For example, if a host bird discovers the eggs are not their own, it will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colors and pattern of the eggs of a few chosen host species. Cuckoo search idealized such breeding behavior, and thus can be applied for various optimization problems.",https://en.wikipedia.org/wiki/Cuckoo_search
Sparse matrix,"In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix). Using those definitions, a matrix will be sparse when its sparsity is greater than 0.5.",https://en.wikipedia.org/wiki/Sparse_matrix
Cluster analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.",https://en.wikipedia.org/wiki/Cluster_analysis
Branch and cut,"Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations.  Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch.",https://en.wikipedia.org/wiki/Branch_and_cut
Luca Maria Gambardella,,https://en.wikipedia.org/wiki/Luca_Maria_Gambardella
Lenstra–Lenstra–Lovász lattice basis reduction algorithm,"The Lenstra–Lenstra–Lovász (LLL) lattice basis reduction algorithm is a polynomial time lattice reduction algorithm invented by Arjen Lenstra, Hendrik Lenstra and László Lovász in 1982. Given a basis B={b1,b2,…,bd},\mathbf {b} _{2},\dots ,\mathbf {b} _{d}\}} with n-dimensional integer coordinates, for a lattice L (a discrete subgroup of  Rn) with  d≤n, the LLL algorithm calculates an LLL-reduced (short, nearly orthogonal) lattice basis in time",https://en.wikipedia.org/wiki/Lenstra%E2%80%93Lenstra%E2%80%93Lov%C3%A1sz_lattice_basis_reduction_algorithm
Textual case-based reasoning,"Textual case-based reasoning is a subtopic of case-based reasoning, in short CBR, a popular area in artificial intelligence. CBR suggests the ways to use past experiences to solve future similar problems, requiring that past experiences be structured in a form similar to attribute-value pairs. This leads to the investigation of textual descriptions for knowledge exploration whose output will be, in turn, used to solve similar problems.",https://en.wikipedia.org/wiki/Textual_case-based_reasoning
Latent Dirichlet allocation,"In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox.",https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation
Lempel–Ziv–Oberhumer,Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.,https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer
Biogeography-based optimization,"Biogeography-based optimization (BBO) is an evolutionary algorithm (EA) that optimizes a function by stochastically and iteratively improving candidate solutions with regard to a given measure of quality, or fitness function. BBO belongs to the class of metaheuristics since it includes many variations, and since it does not make any assumptions about the problem and can therefore be applied to a wide class of problems.",https://en.wikipedia.org/wiki/Biogeography-based_optimization
SolveIT Software,,https://en.wikipedia.org/wiki/SolveIT_Software
Constrained clustering,"In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. ",https://en.wikipedia.org/wiki/Constrained_clustering
Witness set,"In computational learning theory, let C be a concept class over a domain X and c be a concept in C. A subset S of X is a witness set for c in C if c(S) verifies c (i.e., c is the only consistent concept with respect to c(S)). The minimum size of a witness set for c is called the witness size or specification number and is denoted by wC(c). The value max{wC(c):c∈C} is called the teaching dimension of C.",https://en.wikipedia.org/wiki/Witness_set
Digital signature,"A digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents. A valid digital signature, where the prerequisites are satisfied, gives a recipient very strong reason to believe that the message was created by a known sender (authentication), and that the message was not altered in transit (integrity).",https://en.wikipedia.org/wiki/Digital_signature
Computer-automated design,"Design Automation usually refers to electronic design automation, or Design Automation which is a Product Configurator.  Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD) are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification and optimization, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.",https://en.wikipedia.org/wiki/Computer-automated_design
Cortica,"Headquartered in Tel Aviv with R&D and executive offices in Israel and New York City, Cortica utilizes unsupervised learning methods to recognize and analyze digital images and video. The technology developed by the Cortica team is based on research of the function of the human brain.",https://en.wikipedia.org/wiki/Cortica
Search game,"A search game is a two-person zero-sum game which takes place in a set called the search space. The searcher can choose any continuous trajectory subject to a maximal velocity constraint. It is always assumed that neither the searcher nor the hider has any knowledge about the movement of the other player until their distance apart is less than or equal to the discovery radius and at this very moment capture occurs. As mathematical models, search games can be applied to areas such as hide-and-seek games that children play or representations of some tactical military situations. The area of search games was introduced in the last chapter of Rufus Isaacs' classic book ""Differential Games"" and has been developed further by Shmuel Gal and Steve Alpern. The princess and monster game deals with a moving target.",https://en.wikipedia.org/wiki/Search_game
Library sort,"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions.",https://en.wikipedia.org/wiki/Library_sort
Latent variable,"In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables) are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, demography, economics, engineering, medicine, physics, machine learning/artificial intelligence, bioinformatics, chemometrics, natural language processing, econometrics, management and the social sciences.",https://en.wikipedia.org/wiki/Latent_variable
Formal grammar,"In formal language theory, a grammar (when the context is not given, often called a formal grammar for clarity) describes how to form strings from a language's alphabet that are valid according to the language's syntax. A grammar does not describe the meaning of the strings or what can be done with them in whatever context—only their form. A formal grammar is defined as a set of production  rules for strings in a formal language.",https://en.wikipedia.org/wiki/Start_symbol_(formal_languages)
Fowlkes–Mallows index,Fowlkes–Mallows index  is an external evaluation method that is used to determine the similarity between two clusterings (clusters obtained after a clustering algorithm). This measure of similarity could be either between two hierarchical clusterings or a clustering and a benchmark classification. A higher value for the Fowlkes–Mallows index indicates a greater similarity between the clusters and the benchmark classifications.,https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index
Greedy randomized adaptive search procedure,"The greedy randomized adaptive search procedure (also known as GRASP) is a metaheuristic algorithm commonly applied to combinatorial optimization problems.  GRASP typically consists of iterations made up from successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search. The greedy randomized solutions are generated by adding elements to the problem's solution set from a list of elements ranked by a greedy function according to the quality of the solution they will achieve. To obtain variability in the candidate set of greedy solutions, well-ranked candidate elements are often placed in a restricted candidate list (RCL), and chosen at random when building up the solution. This kind of greedy randomized construction method is also known as a semi-greedy heuristic, first described in Hart and Shogan (1987).",https://en.wikipedia.org/wiki/Greedy_randomized_adaptive_search_procedure
Basis (linear algebra),"In mathematics, a set B of elements (vectors) in a vector space V is called a basis, if every element of V may be written in a unique way as a (finite) linear combination of elements of B. The coefficients of this linear combination are referred to as components or coordinates on B of the vector. The elements of a basis are called basis vectors.",https://en.wikipedia.org/wiki/Basis_(linear_algebra)
Special Interest Group on Knowledge Discovery and Data Mining,SIGKDD is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining. SIGKDD hosts an influential annual conference.,https://en.wikipedia.org/wiki/Conference_on_Knowledge_Discovery_and_Data_Mining
Radial basis function network,"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.",https://en.wikipedia.org/wiki/Radial_basis_function_network
Rate-monotonic scheduling,"In computer science, rate-monotonic scheduling (RMS) is a priority assignment algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class. The static priorities are assigned according to the cycle duration of the job, so a shorter cycle duration results in a higher job priority.",https://en.wikipedia.org/wiki/Rate-monotonic_scheduling
Latent variable model,A latent variable model is a statistical model that relates a set of observable variables (so-called manifest variables) to a set of latent variables.,https://en.wikipedia.org/wiki/Latent_variable_model
Tridiagonal matrix algorithm,"In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations.  A tridiagonal system for n unknowns may be written as",https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm
Causality,,https://en.wikipedia.org/wiki/Causality
Neural modeling fields,"Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields,  modeling fields theory (MFT),  Maximum likelihood artificial neural networks (MLANS).This framework has been developed by Leonid Perlovsky at the AFRL.  NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding.  NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.",https://en.wikipedia.org/wiki/Neural_modeling_fields
Fitness approximation,"In function optimization, fitness approximation is a method for decreasing the number of fitness function evaluations to reach a target solution. It belongs to the general class of evolutionary computation or artificial evolution methodologies.",https://en.wikipedia.org/wiki/Fitness_approximation
Luhn algorithm,"The Luhn algorithm or Luhn formula, also known as the ""modulus 10"" or ""mod 10"" algorithm, named after its creator, IBM scientist Hans Peter Luhn, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, Canadian Social Insurance Numbers, Israel ID Numbers, South African ID Numbers, Greek Social Security Numbers (ΑΜΚΑ), and  survey codes appearing on McDonald's, Taco Bell, and Tractor Supply Co. receipts. It is described in U.S. Patent No. 2,950,048, filed on January 6, 1954, and granted on August 23, 1960.",https://en.wikipedia.org/wiki/Luhn_algorithm
Farthest-first traversal,"In computational geometry, the farthest-first traversal of a bounded metric space is a sequence of points in the space, where the first point is selected arbitrarily and each successive point is as far as possible from the set of previously-selected points. The same concept can also be applied to a finite set of geometric points, by restricting the selected points to belong to the set or equivalently by considering the finite metric space generated by these points. For a finite metric space or finite set of geometric points, the resulting sequence forms a permutation of the points, known as the greedy permutation.",https://en.wikipedia.org/wiki/Farthest-first_traversal
Tarjan's off-line lowest common ancestors algorithm,"In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. It is named after Robert Tarjan, who discovered the technique in 1979. Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. The simplest version of the algorithm uses the union-find data structure, which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time.",https://en.wikipedia.org/wiki/Tarjan%27s_off-line_lowest_common_ancestors_algorithm
AIXI,AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence.It combines Solomonoff induction with sequential decision theory.AIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutter's 2005 book Universal Artificial Intelligence.,https://en.wikipedia.org/wiki/AIXI
Warnock algorithm,"The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics. It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.",https://en.wikipedia.org/wiki/Warnock_algorithm
Andrei Knyazev (mathematician),"Andrei (Andrew) Knyazev (Russian: Андрей Владимирович Князев) is a Russian-American mathematician. He graduated from the Faculty of Computational Mathematics and Cybernetics of Moscow State University under the supervision of Evgenii Georgievich D'yakonov (Russian: Евгений Георгиевич Дьяконов) in 1981 and obtained his PhD in Numerical Mathematics at the Russian Academy of Sciences under the supervision of Vyacheslav Ivanovich Lebedev (Russian: Вячеслав Иванович Лебедев) in 1985. He worked at the Kurchatov Institute in 1981-1983, and then to 1992 at the Marchuk Institute of Numerical Mathematics (Russian: ru:Институт вычислительной математики имени Г. И. Марчука РАН) of the Russian Academy of Sciences, headed by Gury Marchuk (Russian: Гурий Иванович Марчук).",https://en.wikipedia.org/wiki/Andrei_Knyazev_(mathematician)
Maekawa's algorithm,Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites.,https://en.wikipedia.org/wiki/Maekawa%27s_Algorithm
Simple matching coefficient,The simple matching coefficient (SMC) or Rand similarity coefficient is a statistic used for comparing the similarity and diversity of sample sets.,https://en.wikipedia.org/wiki/Simple_matching_coefficient
k-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-Means minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. Better Euclidean solutions can for example be found using k-medians and k-medoids.",https://en.wikipedia.org/wiki/K-means_clustering
Random forest,"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.:587–588",https://en.wikipedia.org/wiki/Random_forest
Dijkstra's algorithm,,https://en.wikipedia.org/wiki/Uniform-cost_search
Least slack time scheduling,"Least slack time (LST) scheduling is a scheduling algorithm. It assigns priority based on the slack time of a process. Slack time is the amount of time left after a job if the job was started now. This algorithm is also known as least laxity first. Its most common use is in embedded systems, especially those with multiple processors. It imposes the simple constraint that each process on each available processor possesses the same run time, and that individual processes do not have an affinity to a certain processor. This is what lends it a suitability to embedded systems.",https://en.wikipedia.org/wiki/Least_slack_time_scheduling
BCJR algorithm,"The BCJR algorithm is an algorithm for maximum a posteriori decoding of error correcting codes defined on trellises (principally convolutional codes).  The algorithm is named after its inventors: Bahl, Cocke, Jelinek and Raviv.  This algorithm is critical to modern iteratively-decoded error-correcting codes including turbo codes and low-density parity-check codes.",https://en.wikipedia.org/wiki/BCJR_algorithm
Margin-infused relaxed algorithm,"Margin-infused relaxed algorithm (MIRA) is a machine learning algorithm, an online algorithm for multiclass classification problems. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss. The change of the parameters is kept as small as possible.",https://en.wikipedia.org/wiki/Margin-infused_relaxed_algorithm
Level-set method,"Level-set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. The advantage of the level-set model is that one can perform numerical computations involving curves and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach). Also, the level-set method makes it very easy to follow shapes  that change topology, for example, when a shape splits in two, develops holes, or the reverse of these operations.  All these make the level-set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water.",https://en.wikipedia.org/wiki/Level_set_method
Jaro–Winkler distance,"In computer science and statistics, the Jaro–Winkler distance is a string metric measuring an edit distance between two sequences. It is a variant proposed in 1990 by William E. Winkler of the Jaro distance metric (1989, Matthew A. Jaro).",https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance
Random forest,"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.:587–588",https://en.wikipedia.org/wiki/Kernel_random_forest
Least-angle regression,"In statistics, least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani.",https://en.wikipedia.org/wiki/Least-angle_regression
Reasoning system,In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.,https://en.wikipedia.org/wiki/Reasoning_system
Ofer Dekel (researcher),Ofer Dekel is a computer science researcher in the Machine Learning Department of Microsoft Research. He obtained his PhD in Computer Science from the Hebrew University of Jerusalem and is an affiliate faculty at the Computer Science & Engineering department at the University of Washington.,https://en.wikipedia.org/wiki/Ofer_Dekel_(researcher)
Antipodal point,"In mathematics, the antipodal point of a point on the surface of a sphere is the point which is diametrically opposite to it – so situated that a line drawn from the one to the other passes through the center of the sphere and forms a true diameter.",https://en.wikipedia.org/wiki/Antipodal_point
Banker's algorithm,"The Banker algorithm, sometimes referred to as the detection algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an ""s-state"" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.",https://en.wikipedia.org/wiki/Banker%27s_algorithm
Canonical correlation,"In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other. T. R. Knapp notes that ""virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables."" The method was first introduced by Harold Hotelling in 1936, although in the context of angles between flats the mathematical concept was published by Jordan in 1875.",https://en.wikipedia.org/wiki/Canonical_correlation_analysis
Sequence alignment,"In bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences. Aligned sequences of nucleotide or amino acid residues are typically represented as rows within a matrix. Gaps are inserted between the residues so that identical or similar characters are aligned in successive columns.Sequence alignments are also used for non-biological sequences, such as calculating the distance cost between strings in a natural language or in financial data.",https://en.wikipedia.org/wiki/Sequence_alignment
Prime-factor FFT algorithm,,https://en.wikipedia.org/wiki/Prime-factor_FFT_algorithm
Template:Graph search algorithm,,https://en.wikipedia.org/wiki/Template:Graph_search_algorithm
Orange (software),"Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative data analysis and interactive data visualization.",https://en.wikipedia.org/wiki/Orange_(software)
Odlyzko–Schönhage algorithm,"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by  (Odlyzko &  Schönhage 1988).  The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). The Riemann–Siegel formula  used for calculating the Riemann zeta function  with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2.  This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps  to about T1+ε steps. ",https://en.wikipedia.org/wiki/Odlyzko%E2%80%93Sch%C3%B6nhage_algorithm
Ray Solomonoff,"Ray Solomonoff (July 25, 1926 – December 7, 2009) was the inventor of algorithmic probability, his General Theory of Inductive Inference (also known as Universal Inductive Inference),  and was a founder of algorithmic information theory. He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.",https://en.wikipedia.org/wiki/Ray_Solomonoff
Peter E. Hart,"Peter E. Hart (born c. 1940s) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967–75 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.",https://en.wikipedia.org/wiki/Peter_E._Hart
Hash tree (persistent data structure),"In computer science, a hash tree (or hash trie) is a persistent data structure that can be used to implement sets and maps, intended to replace hash tables in purely functional programming. In its basic form, a hash tree stores the hashes of its keys, regarded as strings of bits, in a trie, with the actual keys and (optional) values stored at the trie's ""final"" nodes.",https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)
Logic learning machine,"Logic Learning Machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, Senior Researcher at the Italian National Research Council CNR-IEIIT in Genoa.Logic Learning Machine is implemented in the Rulex suite.",https://en.wikipedia.org/wiki/Logic_learning_machine
Baillie–PSW primality test,"The Baillie–PSW primality test is a probabilistic primality testing algorithm that determines whether a number is composite or is a probable prime. It is named after Robert Baillie, Carl Pomerance, John Selfridge, and Samuel Wagstaff.",https://en.wikipedia.org/wiki/Baillie-PSW_primality_test
Anomaly detection,"In data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.",https://en.wikipedia.org/wiki/Anomaly_detection
Triangulation (geometry),"In geometry, a triangulation is a subdivision of a planar object into triangles, and by extension the subdivision of a higher-dimension geometric object into simplices. Triangulations of a three-dimensional volume would involve subdividing it into tetrahedra (""pyramids"" of various shapes and sizes) packed together. ",https://en.wikipedia.org/wiki/Triangulation_(geometry)
Sample complexity,The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.,https://en.wikipedia.org/wiki/Sample_complexity
Dendrogram,A dendrogram is a diagram representing a tree.,https://en.wikipedia.org/wiki/Dendrogram
Lamport's bakery algorithm,"Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.",https://en.wikipedia.org/wiki/Lamport%27s_Bakery_algorithm
Multispectral pattern recognition,"Multispectral remote sensing is the collection and analysis of reflected, emitted, or back-scattered energy from an object or an area of interest in multiple bands of regions of the electromagnetic spectrum (Jensen, 2005). Subcategories of multispectral remote sensing include hyperspectral, in which hundreds of bands are collected and analyzed, and ultraspectral remote sensing where many hundreds of bands are used (Logicon, 1997). The main purpose of multispectral imaging is the potential to classify the image using multispectral classification.  This is a much faster method of image analysis than is possible by human interpretation.",https://en.wikipedia.org/wiki/Multispectral_pattern_recognition
Analogical modeling,"Analogical modeling (AM) is a formal theory of exemplar based analogical reasoning, proposed by Royal Skousen, professor of Linguistics and English language at Brigham Young University in Provo, Utah. It is applicable to language modeling and other categorization tasks. Analogical modeling is related to connectionism and nearest neighbor approaches, in that it is data-based rather than abstraction-based; but it is distinguished by its ability to cope with imperfect datasets (such as caused by simulated short term memory limits) and to base predictions on all relevant segments of the dataset, whether near or far. In language modeling, AM has successfully predicted empirically valid forms for which no theoretical explanation was known (see the discussion of Finnish morphology in Skousen et al. 2002). ",https://en.wikipedia.org/wiki/Analogical_modeling
Dartmouth Conferences (peace),"The Dartmouth Conference is the longest continuous bilateral dialogue between American and Soviet (now Russian) representatives. The first Dartmouth Conference took place at Dartmouth College in 1961. Subsequent conferences were held through 1990. They were revived in 2014 and continue today. Task forces begun under the auspices of the main conference continued to work after the main conference stopped. The Regional Conflicts Task Force extended the sustained dialogue model, based on the Dartmouth experience, to conflicts in Tajikistan and Nagorno-Karabakh. Dartmouth inspired a number of other dialogues in the former Soviet Union and elsewhere, many of them under the auspices of the Sustained Dialogue Institute and the Kettering Foundation.",https://en.wikipedia.org/wiki/Dartmouth_Conferences
Constraint (computational chemistry),"In computational chemistry, a constraint algorithm is a method for satisfying the  Newtonian motion of a rigid body which consists of mass points. A restraint algorithm is used to ensure that the distance between mass points is maintained. The general steps involved are; (i) choose novel unconstrained coordinates (internal coordinates), (ii) introduce explicit constraint forces, (iii) minimize constraint forces implicitly by the technique of Lagrange multipliers or projection methods.",https://en.wikipedia.org/wiki/Constraint_algorithm
Email filtering,"Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of incoming messages with anti-spam techniques - to outgoing emails as well as those being received.",https://en.wikipedia.org/wiki/Email_filtering
Diffbot,"Diffbot is a developer of machine learning and computer vision algorithms and public APIs for extracting data from web pages / web scraping. The company was founded in 2008 at Stanford University and was the first company funded by StartX (then Stanford Student Enterprises), Stanford's on-campus venture capital fund.",https://en.wikipedia.org/wiki/Diffbot
Predictive state representation,"In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities.  This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.",https://en.wikipedia.org/wiki/Predictive_state_representation
Lise Getoor,"Lise Getoor is a professor in the Computer Science Department, at the University of California, Santa Cruz, and an adjunct professor in the Computer Science Department at the University of Maryland, College Park. Her primary research interests are in machine learning and reasoning with uncertainty, applied to graphs and structured data. She also works in data integration, social network analysis and visual analytics. She has edited a book on Statistical relational learning that is a main reference in this domain.She has published many highly cited papers in academic journals and conference proceedings. She has also served as action editor for the Machine Learning Journal, JAIR associate editor, and TKDD associate editor.  She is a board member of the International Machine Learning Society, has been a member of AAAI Executive council, was PC co-chair of ICML 2011, and has served as senior PC member for conferences including AAAI, ICML, IJCAI, ISWC, KDD, SIGMOD, UAI, VLDB, WSDM and WWW.",https://en.wikipedia.org/wiki/Lise_Getoor
Viterbi algorithm,"The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).",https://en.wikipedia.org/wiki/Viterbi_algorithm
Queueing theory,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.",https://en.wikipedia.org/wiki/Queuing_theory
Recommender system,"A recommender system, or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine), is a subclass of information filtering system that seeks to predict the ""rating"" or ""preference"" a user would give to an item. They are primarily used in commercial applications.",https://en.wikipedia.org/wiki/Recommendation_system
Bayesian optimization,Bayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.,https://en.wikipedia.org/wiki/Bayesian_optimization
Canny edge detector,The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works.,https://en.wikipedia.org/wiki/Canny_edge_detector
Thurstonian model,"A Thurstonian model is a stochastic transitivity model with latent variables for describing the mapping of some continuous scale onto discrete, possibly ordered categories of response. In the model, each of these categories of response corresponds to a latent variable whose value is drawn from a normal distribution, independently of the other response variables and with constant variance. Developments over the last two decades, however, have led to Thurstonian models that allow unequal variance and non zero covariance terms.  Thurstonian models have been used as an alternative to generalized linear models in analysis of sensory discrimination tasks. They have also been used to model long-term memory in ranking tasks of ordered alternatives, such as the order of the amendments to the US Constitution. Their main advantage over other models ranking tasks is that they account for non-independence of alternatives.    Ennis  provides a comprehensive account of the derivation of Thurstonian models for a wide variety of behavioral tasks including preferential choice, ratings, triads, tetrads, dual pair, same-different and degree of difference, ranks, first-last choice, and applicability scoring.  In Chapter 7 of this book, a closed form expression, derived in 1988, is given for a Euclidean-Gaussian similarity model that provides a solution to the well-known problem that many Thurstonian models are computationally complex often involving multiple integration.  In Chapter 10, a simple form for ranking tasks is presented that only involves the product of univariate normal distribution functions and includes rank-induced dependency parameters.  A theorem is proven that shows that the particular form of the dependency parameters provides the only way that this simplification is possible.  Chapter 6 links discrimination, identification and preferential choice through a common multivariate model in the form of weighted sums of central F distribution functions and allows a general variance-covariance matrix for the items.      ",https://en.wikipedia.org/wiki/Thurstonian_model
Featherstone's algorithm,"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an ""open kinematic chain"") such as a skeleton used in ragdoll physics.",https://en.wikipedia.org/wiki/Featherstone%27s_algorithm
Syntactic pattern recognition,"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.",https://en.wikipedia.org/wiki/Syntactic_pattern_recognition
Grammar checker,"A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness. Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text.",https://en.wikipedia.org/wiki/Grammar_checker
Trigonometric interpolation,"In mathematics, trigonometric interpolation is interpolation with trigonometric polynomials. Interpolation is the process of finding a function which goes through some given data points. For trigonometric interpolation, this function has to be a trigonometric polynomial, that is, a sum of sines and cosines of given periods. This form is especially suited for interpolation of periodic functions.",https://en.wikipedia.org/wiki/Trigonometric_interpolation
Convex hull,"In geometry, the convex hull or convex envelope or convex closure of a shape is the smallest convex set that contains it. For a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around the subset.",https://en.wikipedia.org/wiki/Convex_hull
Masaru Tomita,"Masaru Tomita (Japanese: 冨田 勝, Hepburn: Tomita Masaru, born December 28, 1957) is a Japanese molecular biologist and computer scientist, best known as the director of the E-Cell simulation environment software and/or the inventor of GLR parser algorithm. He is a professor of Keio University, president of the Institute for Advanced Biosciences, and the founder and board member of Human Metabolome Technologies, Inc. He is also the co-founder and on the board of directors of The Metabolomics Society.[citation needed] His father is composer Isao Tomita.[citation needed]",https://en.wikipedia.org/wiki/Masaru_Tomita
Learning with errors,"Learning with errors (LWE) is the computational problem of inferring a linear n some of which may be erroneous.The LWE problem is conjectured to be hard to solve, and thus be useful in cryptography.",https://en.wikipedia.org/wiki/Learning_with_errors
Hamming weight,"The Hamming weight of a string is the number of symbols that are different from the zero-symbol of the alphabet used. It is thus equivalent to the Hamming distance from the all-zero string of the same length. For the most typical case, a string of bits, this is the number of 1's in the string, or the digit sum of the binary representation of a given number and the ℓ₁ norm of a bit vector. In this binary case, it is also called the population count, popcount, sideways sum, or bit summation.",https://en.wikipedia.org/wiki/Hamming_weight
Chaff algorithm,"Chaff is an algorithm for solving instances of the Boolean satisfiability problem in programming. It was designed by researchers at Princeton University, United States. The algorithm is an instance of the DPLL algorithm with a number of enhancements for efficient implementation.",https://en.wikipedia.org/wiki/Chaff_algorithm
k-medoids,"The k-medoids or partitioning around medoids (PAM) algorithm is a clustering algorithm reminiscent of the k-means algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses data points as centers (medoids or exemplars) and can be used with arbitrary distances, while in k-means the centre of a cluster is not necessarily one of the input data points (it is the average between the points in the cluster). The PAM method was proposed in 1987 for the work with l1 norm and other distances.",https://en.wikipedia.org/wiki/K-medoids
Watershed (image processing),"In the study of image processing, a watershed is a transformation defined on a grayscale image. The name refers metaphorically to a geological watershed, or drainage divide, which separates adjacent drainage basins. The watershed transformation treats the image it operates upon like a topographic map, with the brightness of each point representing its height, and finds the lines that run along the tops of ridges.",https://en.wikipedia.org/wiki/Watershed_(algorithm)
Binary splitting,"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. In particular, it can be used to evaluate hypergeometric series at rational points.",https://en.wikipedia.org/wiki/Binary_splitting
UPGMA,UPGMA (unweighted pair group method with arithmetic mean) is a simple agglomerative (bottom-up) hierarchical clustering method. The method is generally attributed to Sokal and Michener.,https://en.wikipedia.org/wiki/UPGMA
Computer vision,"Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.",https://en.wikipedia.org/wiki/Computer_vision
Subclass reachability,"In computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).",https://en.wikipedia.org/wiki/Subclass_reachability
Locality-sensitive hashing,"In computer science, locality-sensitive hashing (LSH) is an algorithmic technique that hashes similar input items into the same ""buckets"" with high probability. (The number of buckets are much smaller than the universe of possible input items.) Since similar items end up in the same buckets, this technique can be used for data clustering and nearest neighbor search. It differs from conventional hashing techniques in that hash collisions are maximized, not minimized. Alternatively, the technique can be seen as a way to reduce the dimensionality of high-dimensional data; high-dimensional input items can be reduced to low-dimensional versions while preserving relative distances between items.",https://en.wikipedia.org/wiki/Locality-sensitive_hashing
Damerau–Levenshtein distance,"In information theory and computer science, the Damerau–Levenshtein distance (named after Frederick J. Damerau and Vladimir I. Levenshtein) is a string metric for measuring the edit distance between two sequences. Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations (consisting of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters) required to change one word into the other.",https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance
Fast folding algorithm,"In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data.  It computes superpositions of the signal modulo various window sizes simultaneously.",https://en.wikipedia.org/wiki/Fast_folding_algorithm
Krauss wildcard-matching algorithm,"In computer science, the Krauss wildcard-matching algorithm is a pattern matching algorithm. Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, the algorithm provides a non-recursive mechanism for matching patterns in software applications, based on syntax simpler than that typically offered by regular expressions.",https://en.wikipedia.org/wiki/Krauss_matching_wildcards_algorithm
Eager learning,"In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.",https://en.wikipedia.org/wiki/Eager_learning
Léon Bottou,"Léon Bottou (born 1965) is a researcher best known for his work in machine learning and data compression. His work presents stochastic gradient descent as a fundamental learning algorithm.[clarification needed] He is also one of the main creators of the DjVu image compression technology (together with Yann LeCun and Patrick Haffner), and the maintainer of DjVuLibre, the open source implementation of DjVu. He is the original developer of the Lush programming language.",https://en.wikipedia.org/wiki/L%C3%A9on_Bottou
Bidirectional search,"Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b, and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.",https://en.wikipedia.org/wiki/Bidirectional_search
Latent semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.",https://en.wikipedia.org/wiki/Latent_semantic_analysis
Basis function,"In mathematics,  a basis function is an element of a particular basis for a function space. Every continuous function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.",https://en.wikipedia.org/wiki/Basis_function
Calculus of variations,"The calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functionsand functionals, to find maxima and minima of functionals: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations.",https://en.wikipedia.org/wiki/Variational_method
Collaborative filtering,"Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.",https://en.wikipedia.org/wiki/Collaborative_filtering
Interior-point method,Interior-point methods (also referred to as barrier methods or IPMs) are a certain class of algorithms that solve linear and nonlinear convex optimization problems.,https://en.wikipedia.org/wiki/Interior_point_method
Montgomery modular multiplication,"In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication.  It was introduced in 1985 by the American mathematician Peter L. Montgomery.",https://en.wikipedia.org/wiki/Montgomery_reduction
Bruun's FFT algorithm,"Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary Cooley–Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision  (Storn, 1993).",https://en.wikipedia.org/wiki/Bruun%27s_FFT_algorithm
Pi,,https://en.wikipedia.org/wiki/Pi
Bayesian structural time series,"Bayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data.",https://en.wikipedia.org/wiki/Bayesian_structural_time_series
Queueing theory,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.",https://en.wikipedia.org/wiki/Queueing_theory
Naive Bayes classifier,"In machine learning, naïve Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.",https://en.wikipedia.org/wiki/Naive_Bayes_classifier
Influence diagram,"An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.",https://en.wikipedia.org/wiki/Influence_diagram
Tarjan's strongly connected components algorithm,"Tarjan's algorithm  is an algorithm in graph theory for finding the strongly connected components of a directed graph. It runs in linear time, matching the time bound for alternative methods including Kosaraju's algorithm and the path-based strong component algorithm. Tarjan's algorithm is named for its inventor, Robert Tarjan.",https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm
Fair-share scheduling,"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes.  ",https://en.wikipedia.org/wiki/Fair-share_scheduling
Advanced Encryption Standard,"The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: ), is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.",https://en.wikipedia.org/wiki/Advanced_Encryption_Standard
MD5,,https://en.wikipedia.org/wiki/MD5
Dither,,https://en.wikipedia.org/wiki/Dithering
k-medians clustering,"In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the squared 2-norm distance metric (which k-means does.)",https://en.wikipedia.org/wiki/K-medians_clustering
Version space learning,"Version space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction",https://en.wikipedia.org/wiki/Version_space_learning
Liang–Barsky algorithm,"In computer graphics, the Liang–Barsky algorithm (named after You-Dong Liang and Brian A. Barsky) is a line clipping algorithm. The Liang–Barsky algorithm uses the parametric equation of a line and inequalities describing the range of the clipping window to determine the intersections between the line and the clip window. With these intersections it knows which portion of the line should be drawn. This algorithm is significantly more efficient than Cohen–Sutherland. The idea of the Liang–Barsky clipping algorithm is to do as much testing as possible before computing line intersections.",https://en.wikipedia.org/wiki/Liang%E2%80%93Barsky
Lempel–Ziv–Storer–Szymanski,"Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski. LZSS was described in article ""Data compression via textual substitution"" published in Journal of the ACM (1982, pp. 928–951).",https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Storer%E2%80%93Szymanski
Bioinformatics,,https://en.wikipedia.org/wiki/Bioinformatics
Computus,"The computus (Latin for 'computation') is a calculation that determines the calendar date of Easter.:xviii  Easter is traditionally celebrated on the first Sunday after the Paschal full moon, which is the first full moon on or after 21 March (an approximation of the March equinox).  Determining this date in advance requires a correlation between the lunar months and the solar year, while also accounting for the month, date, and weekday of the calendar.:xviii-xx The calculations produce different results depending on whether the Julian calendar or the Gregorian calendar is used. ",https://en.wikipedia.org/wiki/Computus
Region growing,Region growing is a simple region-based image segmentation method. It is also classified as a pixel-based image segmentation method since it involves the selection of initial seed points.,https://en.wikipedia.org/wiki/Region_growing
Feature learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.",https://en.wikipedia.org/wiki/Feature_learning
Semidefinite embedding,Semidefinite embedding (SDE) or maximum variance unfolding (MVU) is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. MVU can be viewed as a non-linear generalization of Principal component analysis.,https://en.wikipedia.org/wiki/Semidefinite_embedding
Fowler–Noll–Vo hash function,"Fowler–Noll–Vo is a non-cryptographic hash function created by Glenn Fowler, Landon Curt Noll, and Kiem-Phong Vo.",https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function
Generalized multidimensional scaling,"Generalized multidimensional scaling (GMDS) is an extension of metric multidimensional scaling, in which the target space is non-Euclidean. When the dissimilarities are distances on a surface and the target space is another surface, GMDS allows finding the minimum-distortion embedding of one surface into another.",https://en.wikipedia.org/wiki/Generalized_multidimensional_scaling
Optical character recognition,,https://en.wikipedia.org/wiki/Optical_character_recognition
Calculus,,https://en.wikipedia.org/wiki/Calculus
Stefano Soatto,"Stefano Soatto is Professor of Computer Science at the University of California, Los Angeles (UCLA), in Los Angeles, CA, where he is also Professor of Electrical Engineering and Founding Director of the UCLA Vision Lab. He was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2013 for contributions to dynamic visual processes. He received the David Marr Prize in Computer Vision in 1999.",https://en.wikipedia.org/wiki/Stefano_Soatto
Naive Bayes classifier,"In machine learning, naïve Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.",https://en.wikipedia.org/wiki/Naive_Bayes
System of linear equations,"In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same set of variables. For example,",https://en.wikipedia.org/wiki/System_of_linear_equations
GLIMMER,"In bioinformatics, GLIMMER (Gene Locator and Interpolated Markov ModelER) is used to find genes in prokaryotic DNA. ""It is effective at finding genes in bacteria, archea, viruses, typically finding 98-99% of all relatively long protein coding genes"". GLIMMER was the first system that used the interpolated Markov model  to identify coding regions. The GLIMMER software is open source and is maintained by Steven Salzberg, Art Delcher, and their colleagues at the Center for Computational Biology at Johns Hopkins University. The original GLIMMER algorithms and software were designed by Art Delcher, Simon Kasif and Steven Salzberg and applied to bacterial genome annotation in collaboration with Owen White. ",https://en.wikipedia.org/wiki/GLIMMER
Genetic algorithm,,https://en.wikipedia.org/wiki/Genetic_algorithm
Spectral clustering,"In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.",https://en.wikipedia.org/wiki/Spectral_clustering
Arbitrary-precision arithmetic,"In computer science, arbitrary-precision arithmetic, also called bignum arithmetic, multiple-precision arithmetic, or sometimes infinite-precision arithmetic, indicates that calculations are performed on numbers whose digits of precision are limited only by the available memory of the host system.  This contrasts with the faster fixed-precision arithmetic found in most arithmetic logic unit (ALU) hardware, which typically offers between 8 and 64 bits of precision.",https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic
Boyer–Moore–Horspool algorithm,"In computer science, the Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. It was published by Nigel Horspool in 1980 as SBM.",https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm
Multiplicative weight update method,"The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise.  It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.",https://en.wikipedia.org/wiki/Multiplicative_weight_update_method
Santa Fe Trail problem,The Santa Fe Trail problem is a genetic programming exercise in which artificial ants search for food pellets according to a programmed set of instructions. The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.,https://en.wikipedia.org/wiki/Santa_Fe_Trail_problem
Shifting nth root algorithm,"The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.",https://en.wikipedia.org/wiki/Shifting_nth-root_algorithm
Difference-map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.",https://en.wikipedia.org/wiki/Difference_map_algorithm
NetOwl,"NetOwl is a suite of multilingual text and identity analytics products that analyze big data in the form of text data – reports, web, social media, etc. – as well as structured entity data about people, organizations, places, and things.",https://en.wikipedia.org/wiki/NetOwl
Query-level feature,A query-level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.,https://en.wikipedia.org/wiki/Query-level_feature
Erkki Oja,"Erkki Oja (born 22 March 1948 in Helsinki) is a Finnish computer scientist and Aalto Distinguished Professor in the Department of Information and Computer Science at Aalto University School of Science. He is recognized for developing Oja's rule, which is a model of how neurons in the brain or in artificial neural networks learn over time. He is a Fellow of the International Association for Pattern Recognition and the IEEE, and a member of the Finnish Academy of Sciences. He served as chairman of the European Neural Network Society between 2000 and 2005, and as the chairman of the Academy of Finland’s Research Council for Natural Sciences and Engineering between 2007 and 2012.",https://en.wikipedia.org/wiki/Erkki_Oja
Polygon triangulation,"In computational geometry, polygon triangulation is the decomposition of a polygonal area (simple polygon) P into a set of triangles, i.e., finding a set of triangles with pairwise non-intersecting interiors whose union is P.",https://en.wikipedia.org/wiki/Polygon_triangulation
Point cloud,"A point cloud is a set of data points in space. Point clouds are generally produced by 3D scanners, which measure many points on the external surfaces of objects around them. As the output of 3D scanning processes, point clouds are used for many purposes, including to create 3D CAD models for manufactured parts, for metrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications.",https://en.wikipedia.org/wiki/Point_cloud
Nondeterministic algorithm,"In computer science, a nondeterministic algorithm is an algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm. There are several ways an algorithm may behave differently from run to run. A concurrent algorithm can perform differently on different runs due to a race condition. A probabilistic algorithm's behaviors depends on a random number generator. An algorithm that solves a problem in nondeterministic polynomial time can run in polynomial time or exponential time depending on the choices it makes during execution. The nondeterministic algorithms are often used to find an approximation to a solution, when the exact solution would be too costly to obtain using a deterministic one.",https://en.wikipedia.org/wiki/Nondeterministic_algorithm
Fibonacci number,"In mathematics, the Fibonacci numbers, commonly denoted Fn, form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is,",https://en.wikipedia.org/wiki/Fibonacci_numbers
Matching (graph theory),"In the mathematical discipline of graph theory, a matching or independent edge set in a graph is a set of edges without common vertices. Finding a matching in a bipartite graph can be treated as a network flow problem.",https://en.wikipedia.org/wiki/Perfect_matching
scikit-learn,,https://en.wikipedia.org/wiki/Scikit-learn
Microsoft Cognitive Toolkit,"Microsoft Cognitive Toolkit, previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.",https://en.wikipedia.org/wiki/Microsoft_Cognitive_Toolkit
Congruence of squares,"In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms.",https://en.wikipedia.org/wiki/Congruence_of_squares
International Semantic Web Conference,"The International Semantic Web Conference (ISWC) is a series of academic conferences and the premier international forum, for the Semantic Web, Linked Data and Knowledge Graph Community. Here, scientists, industry specialists, and practitioners meet to discuss the future of practical, scalable, user-friendly, and game changing solutions. Its proceedings are published in the Lecture Notes in Computer Science by Springer-Verlag.",https://en.wikipedia.org/wiki/International_Semantic_Web_Conference
MysteryVibe,,https://en.wikipedia.org/wiki/MysteryVibe
Argon2,"Argon2 is a key derivation function that was selected as the winner of the Password Hashing Competition in July 2015. It was designed by Alex Biryukov, Daniel Dinu, and Dmitry Khovratovich from the University of Luxembourg. The reference implementation of Argon2 is released under a Creative Commons CC0 license (i.e. public domain) or the Apache License 2.",https://en.wikipedia.org/wiki/Argon2
GraphLab,"Turi is a graph-based, high performance, distributed computation framework written in C++. The GraphLab project was started by Prof. Carlos Guestrin of Carnegie Mellon University in 2009. It is an open source project using an Apache License. While GraphLab was originally developed for Machine Learning tasks, it has found great success at a broad range of other data-mining tasks; out-performing other abstractions by orders of magnitude.",https://en.wikipedia.org/wiki/GraphLab
Elias gamma coding,"Elias γ code or Elias gamma code is a universal code encoding positive integers developed by Peter Elias.:197, 199 It is used most commonly when coding integers whose upper-bound cannot be determined beforehand.",https://en.wikipedia.org/wiki/Elias_gamma_coding
DeepArt,"DeepArt or DeepArt.io is a website that allows users to create unique artistic images by using an algorithm to redraw one image using the stylistic elements of another image. This uses ""A Neural Algorithm of Artistic Style"" a Neural Style Transfer algorithm that was developed by several of its creators to separate style elements from a piece of art. The tool allows users to create imitation works of art using the style of famous artists. The neural algorithm is used by the Deep Art website to create a representation of an image provided by the user by using the 'style' of another image provided by the user. A similar program, Prisma, is an iOS and Android app that was based on the open source programming that underlies DeepArt.",https://en.wikipedia.org/wiki/DeepArt
Kinect,,https://en.wikipedia.org/wiki/Kinect
Lifelong Planning A*,LPA* or Lifelong Planning A* is an incremental heuristic search algorithm based on A*. It was first described by Sven Koenig and Maxim Likhachev in 2001.,https://en.wikipedia.org/wiki/Lifelong_Planning_A*
Bayesian hierarchical modeling,"Bayesian hierarchical modelling is a statistical model written in multiple levels (hierarchical form) that estimates the parameters of the posterior distribution using the Bayesian method. The sub-models combine to form the hierarchical model, and Bayes' theorem is used to integrate them with the observed data and account for all the uncertainty that is present. The result of this integration is the posterior distribution, also known as the updated probability estimate, as additional evidence on the prior distribution is acquired.",https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling
Apache SINGA,"Apache SINGA is an Apache top-level project for developing an open source machine learning library. It provides a flexible architecture for scalable distributed training, is extensible to run over a wide range of hardware, and has a focus on health-care applications.",https://en.wikipedia.org/wiki/Apache_Singa
Karmarkar's algorithm,Karmarkar's algorithm is an algorithm introduced by Narendra Karmarkar in 1984 for solving linear programming problems. It was the first reasonably efficient algorithm that solves these problems in polynomial time. The ellipsoid method is also polynomial time but proved to be inefficient in practice.,https://en.wikipedia.org/wiki/Karmarkar%27s_algorithm
Dijkstra–Scholten algorithm,The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.,https://en.wikipedia.org/wiki/Dijkstra-Scholten_algorithm
Mean squared prediction error,"In statistics the mean squared prediction error or mean squared error of the predictions  of a smoothing or curve fitting procedure is the expected value of the squared difference between the fitted values implied by the predictive function g^ and the values of the (unobservable) function g. It is an inverse measure of the explanatory power of g^,,} and can be used in the process of cross-validation of an estimated model.",https://en.wikipedia.org/wiki/Mean_squared_prediction_error
Edmonds' algorithm,"In graph theory, Edmonds' algorithm or Chu–Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching).It is the directed analog of the minimum spanning tree problem.The algorithm was proposed independently first by Yoeng-Jin Chu and Tseng-Hong Liu (1965) and then by Jack Edmonds (1967).",https://en.wikipedia.org/wiki/Edmonds%27_algorithm
Bucket sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort,  a generalization of pigeonhole sort, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.",https://en.wikipedia.org/wiki/Bucket_sort
Schema (genetic algorithms),A schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets; and so form a topological space.,https://en.wikipedia.org/wiki/Schema_(genetic_algorithms)
Apache Spark,"Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.",https://en.wikipedia.org/wiki/Apache_Spark
Search-based software engineering,,https://en.wikipedia.org/wiki/Search-based_software_engineering
Steinhaus–Johnson–Trotter algorithm,"The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. Equivalently, this algorithm finds a Hamiltonian cycle in the permutohedron.",https://en.wikipedia.org/wiki/Steinhaus%E2%80%93Johnson%E2%80%93Trotter_algorithm
Quantum Artificial Intelligence Lab,"The Quantum Artificial Intelligence Lab (also called the Quantum AI Lab or QuAIL) is a joint initiative of NASA, Universities Space Research Association, and Google (specifically, Google Research) whose goal is to pioneer research on how quantum computing might help with machine learning and other difficult computer science problems. The lab is hosted at NASA's Ames Research Center.",https://en.wikipedia.org/wiki/Quantum_Artificial_Intelligence_Lab
Apriori algorithm,Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.,https://en.wikipedia.org/wiki/Apriori_algorithm
Euclidean minimum spanning tree,"The Euclidean minimum spanning tree or EMST is a minimum spanning tree of a set of n points in the plane (or more generally in ℝd), where the weight of the edge between each pair of points is the Euclidean distance between those two points. In simpler terms, an EMST connects a set of dots using lines such that the total length of all the lines is minimized and any dot can be reached from any other by following the lines.",https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree
Flashsort,Flashsort is a distribution sorting algorithm showing linear computational complexity O(n) for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.,https://en.wikipedia.org/wiki/Flashsort
Euclidean shortest path,"The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.",https://en.wikipedia.org/wiki/Euclidean_shortest_path_problem
Evolved antenna,"In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution. This procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual radiation patterns, for which none of the many existing antenna types are adequate.",https://en.wikipedia.org/wiki/Evolved_antenna
Hierarchical temporal memory,,https://en.wikipedia.org/wiki/Hierarchical_temporal_memory
Gaussian process emulator,"In statistics, Gaussian process emulator is one name for a general type of statistical model that has been used in contexts where the problem is to make maximum use of the outputs of a complicated (often non-random) computer-based simulation model. Each run of the simulation model is computationally expensive and each run is based on many different controlling inputs. The variation of the outputs of the simulation model is expected to vary reasonably smoothly with the inputs, but in an unknown way.",https://en.wikipedia.org/wiki/Gaussian_process_emulator
Kneser–Ney smoothing,"Kneser–Ney smoothing is a method primarily used to calculate the probability distribution of n-grams in a document based on their histories. It is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability's lower order terms to omit n-grams with lower frequencies. This approach has been considered equally effective for both higher and lower order n-grams. The method was proposed in a 1994 paper by Reinhard Kneser, Ute Essen and Hermann Ney .",https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing
Savi Technology,"Savi Technology was founded in 1989 and is based in Alexandria, Virginia.",https://en.wikipedia.org/wiki/Savi_Technology
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/0-471-03003-1
Proof of work,"A proof-of-work (PoW) system (or protocol, or function) is a consensus mechanism. It deters denial-of-service attacks and other service abuses such as spam on a network by requiring some work from the service requester, usually meaning processing time by a computer. The concept was invented by Cynthia Dwork and Moni Naor as presented in a 1993 journal article. The term ""proof of work"" was first coined and formalized in a 1999 paper by Markus Jakobsson and Ari Juels.",https://en.wikipedia.org/wiki/Proof-of-work_system
Nested loop join,A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important for database management.,https://en.wikipedia.org/wiki/Nested_loop_join
Elliptic-curve Diffie–Hellman,"Elliptic-curve Diffie–Hellman (ECDH) is a key agreement protocol that allows two parties, each having an elliptic-curve public–private key pair, to establish a shared secret over an insecure channel. This shared secret may be directly used as a key, or to derive another key.  The key, or the derived key, can then be used to encrypt subsequent communications using a symmetric-key cipher. It is a variant of the Diffie–Hellman protocol using elliptic-curve cryptography.",https://en.wikipedia.org/wiki/Elliptic-curve_Diffie-Hellman
Glottochronology,"Glottochronology (from Attic Greek γλῶττα tongue, language and χρóνος time) is the part of lexicostatistics dealing with the chronological relationship between languages.:131",https://en.wikipedia.org/wiki/Glottochronology
Marching triangles,"In computer graphics, the problem of transforming a cloud of points on the surface of a three-dimensional object into a polygon mesh for the object can be solved by a technique called marching triangles. This provides a faster alternative to other methods for the same problem of surface reconstruction, based on Delaunay triangulation.",https://en.wikipedia.org/wiki/Marching_triangles
Delaunay triangulation,"In mathematics and computational geometry,  a Delaunay triangulation (also known as a Delone triangulation) for a given set P of discrete points in a plane is a triangulation DT(P) such that no point in P is inside the circumcircle of any triangle in DT(P). Delaunay triangulations maximize the minimum angle of all the angles of the triangles in the triangulation; they tend to avoid sliver triangles. The triangulation is named after Boris Delaunay for his work on this topic from 1934.",https://en.wikipedia.org/wiki/Delaunay_triangulation
Local independence,Local independence is the underlying assumption of latent variable models.The observed items are conditionally independent of each other given an individual score on the latent variable(s). This means that the latent variable explains why the observed items are related to another. This can be explained by the following example.,https://en.wikipedia.org/wiki/Local_independence
Backtracking,"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (""backtracks"") as soon as it determines that the candidate cannot possibly be completed to a valid solution.",https://en.wikipedia.org/wiki/Backtracking
AlphaGo,,https://en.wikipedia.org/wiki/AlphaGo
Category:Graph algorithms,Graph algorithms solve problems related to graph theory.,https://en.wikipedia.org/wiki/Category:Graph_algorithms
Shortest path problem,"In graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.",https://en.wikipedia.org/wiki/Shortest_path_problem
Stone method,"In numerical analysis, Stone's method, also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations. The method uses an incomplete LU decomposition, which approximates the exact LU decomposition, to get an iterative solution of the problem. The method is named after Herbert L. Stone, who proposed it in 1968.",https://en.wikipedia.org/wiki/Stone%27s_method
Shortest seek first,Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.,https://en.wikipedia.org/wiki/Shortest_seek_first
Multidimensional analysis,"In statistics, econometrics, and related fields, multidimensional analysis (MDA) is a data analysis process that groups data into two categories: data dimensions and measurements. For example, a data set consisting of the number of wins for a single football team at each of several years is a single-dimensional (in this case, longitudinal) data set. A data set consisting of the number of wins for several football teams in a single year is also a single-dimensional (in this case, cross-sectional) data set. A data set consisting of the number of wins for several football teams over several years is a two-dimensional data set.",https://en.wikipedia.org/wiki/Multidimensional_analysis
Dynamical system,"In mathematics, a dynamical system is a system in which a function describes the time dependence of a point in a geometrical space.  Examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, and the number of fish each springtime in a lake.",https://en.wikipedia.org/wiki/Dynamical_system
CYK algorithm,"In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs  bottom-up parsing and dynamic programming.",https://en.wikipedia.org/wiki/CYK_algorithm
Hirschberg's algorithm,"In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other.  Hirschberg's algorithm is simply described as a more space efficient version of the Needleman–Wunsch algorithm that uses divide and conquer.  Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.",https://en.wikipedia.org/wiki/Hirschberg%27s_algorithm
Hill climbing,"In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.",https://en.wikipedia.org/wiki/Hill_climbing
Line segment intersection,"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect (cross).",https://en.wikipedia.org/wiki/Line_segment_intersection
Decision boundary,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.",https://en.wikipedia.org/wiki/Decision_boundary
Chew's second algorithm,"In mesh generation, Chew's second algorithm is a Delaunay refinement algorithm for creating quality constrained Delaunay triangulations.  The algorithm takes a piecewise linear system (PLS) and returns a constrained Delaunay triangulation of only quality triangles where quality is defined by the minimum angle in a triangle.  Developed by L. Paul Chew for meshing surfaces embedded in three-dimensional space, Chew's second algorithm has been adopted as a two-dimensional mesh generator due to practical advantages over Ruppert's algorithm in certain cases and is the default quality mesh generator implemented in the freely available Triangle package. Chew's second algorithm is guaranteed to terminate and produce a local feature size-graded meshes with minimum angle up to about 28.6 degrees.",https://en.wikipedia.org/wiki/Chew%27s_second_algorithm
Knowledge Graph,,https://en.wikipedia.org/wiki/Knowledge_Vault
Neuroph,Neuroph is an object-oriented artificial neural network framework written in Java. It can be used to create and train neural networks in Java programs. Neuroph provides Java class library as well as GUI tool easyNeurons for creating and training neural networks.,https://en.wikipedia.org/wiki/Neuroph
Maximum cardinality matching,Maximum cardinality matching is a fundamental problem in graph theory.,https://en.wikipedia.org/wiki/Maximum_cardinality_matching
Lanczos resampling,"Lanczos filtering and Lanczos resampling are two applications of a mathematical formula. It can be used as a low-pass filter or used to smoothly interpolate the value of a digital signal between its samples.  In the latter case it maps each sample of the given signal to a translated and scaled copy of the Lanczos kernel, which is a sinc function windowed by the central lobe of a second, longer, sinc function.  The sum of these translated and scaled kernels is then evaluated at the desired points.",https://en.wikipedia.org/wiki/Lanczos_resampling
WACA clustering algorithm,WACA is a clustering algorithm for dynamic networks. WACA (Weighted Application-aware Clustering Algorithm) uses a heuristic weight function for self-organized cluster creation. The election of clusterheads is based on local network information only.,https://en.wikipedia.org/wiki/WACA_clustering_algorithm
Diffusion map,"Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by  Coifman and Lafon  which computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the ""diffusion distance"" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps give a global description of the data-set. Compared with other methods, the diffusion map algorithm is robust to noise perturbation and computationally inexpensive.",https://en.wikipedia.org/wiki/Diffusion_map
Selection sort,"In computer science, selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.",https://en.wikipedia.org/wiki/Selection_sort
Canonical LR parser,"In computer science, a canonical LR parser or LR(1) parser is an LR(k) parser for k=1, i.e. with a single lookahead terminal. The special attribute of this parser is that any LR(k) grammar with k>1 can be transformed into an LR(1) grammar. However, back-substitutions are required to reduce k and as back-substitutions increase, the grammar can quickly become large, repetitive and hard to understand.  LR(k) can handle all deterministic context-free languages..  In the past this LR(k) parser has been avoided because of its huge memory requirements in favor of less powerful alternatives such as the LALR and the LL(1) parser.  Recently, however, a ""minimal LR(1) parser"" whose space requirements are close to LALR parsers, is being offered by several parser generators.  ",https://en.wikipedia.org/wiki/Canonical_LR_parser
IBM Machine Learning Hub,"The IBM Machine Learning Hub hosts businesses wanting to collaborate with IBM’s machine learning experts. Its mission is to close the gap between available open-source tools and the knowledge required to use them. During three-day workshops, the machine learning experts work with companies to implement initial prototypes. Within the workshops, data scientists use tools like Data Science Experience (DSX) to collaborate and find similar solutions to their use cases. The machine learning experts have completed cases in the travel, energy and utilities, healthcare, financial services, manufacturing, and retail industries. Together, they walk through the stages of the machine learning process to get the concrete results.",https://en.wikipedia.org/wiki/IBM_Machine_Learning_Hub
Recurrent neural network,"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.",https://en.wikipedia.org/wiki/Recurrent_neural_network
Unsupervised learning,"Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs. It forms one of the three main categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning, a related variant, makes use of supervised and unsupervised techniques.",https://en.wikipedia.org/wiki/Unsupervised_learning
Hopfield network,"A Hopfield network is a form of recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable (""associative"") memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum and, therefore, may converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum)[citation needed]. Hopfield networks also provide a model for understanding human memory .",https://en.wikipedia.org/wiki/Hopfield_net
Convex hull algorithms,Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.,https://en.wikipedia.org/wiki/Convex_hull_algorithms
Bing (search engine),,https://en.wikipedia.org/wiki/Bing_Predicts
Strassen algorithm,"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.",https://en.wikipedia.org/wiki/Strassen_algorithm
BLAST (biotechnology),"In bioinformatics, BLAST (basic local alignment search tool) is an algorithm and program for comparing primary biological sequence information, such as the amino-acid sequences of proteins or the nucleotides of DNA and/or RNA sequences. A BLAST search enables a researcher to compare a subject protein or nucleotide sequence (called a query) with a library or database of sequences, and identify library sequences that resemble the query sequence above a certain threshold.",https://en.wikipedia.org/wiki/Basic_Local_Alignment_Search_Tool
Association rule learning,Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.,https://en.wikipedia.org/wiki/Association_rule_learning
Berlekamp's root finding algorithm,"In number theory, Berlekamp's root finding algorithm, also called the Berlekamp–Rabin algorithm, is the probabilistic method of finding roots of polynomials over a field Zp. The method was discovered by Elwyn Berlekamp in 1970 as an auxiliary to the algorithm for polynomial factorization over finite fields. The algorithm was later modified by Rabin for arbitrary finite fields in 1979. The method was also independently discovered before Berlekamp by other researchers.",https://en.wikipedia.org/wiki/Berlekamp%27s_root_finding_algorithm
Crank–Nicolson method,"In numerical analysis, the Crank–Nicolson method is a finite difference method used for numerically solving the heat equation and similar partial differential equations. It is a second-order method in time. It is implicit in time and can be written as an implicit Runge–Kutta method, and it is numerically stable. The method was developed by John Crank and Phyllis Nicolson in the mid 20th century.",https://en.wikipedia.org/wiki/Crank%E2%80%93Nicolson_method
Ford–Fulkerson algorithm,,https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm
Garbage collection (computer science),,https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)
Similarity measure,"In statistics and related fields, a similarity measure or similarity function is a real-valued function that quantifies the similarity between two objects. Although no single definition of a similarity measure exists, usually such measures are in some sense the inverse of distance metrics: they take on large values for similar objects and either zero or a negative value for very dissimilar objects.",https://en.wikipedia.org/wiki/Similarity_measure
QR algorithm,"In numerical linear algebra, the QR algorithm is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. The QR algorithm was developed in the late 1950s by John G. F. Francis and by Vera N. Kublanovskaya, working independently.  The basic idea is to perform a QR decomposition, writing the matrix as a product of an orthogonal matrix and an upper triangular matrix, multiply the factors in the reverse order, and iterate.",https://en.wikipedia.org/wiki/QR_algorithm
Finite difference method,"In numerical analysis, finite-difference methods (FDM) are discretizations used for solving differential equations by approximating them with difference equations that finite differences approximate the derivatives.",https://en.wikipedia.org/wiki/Finite_difference_method
Code-excited linear prediction,"Code-excited linear prediction (CELP) is a linear predictive speech coding algorithm originally proposed by Manfred R. Schroeder and Bishnu S. Atal in 1985. At the time, it provided significantly better quality than existing low bit-rate algorithms, such as residual-excited linear prediction (RELP) and linear predictive coding (LPC) vocoders (e.g., FS-1015). Along with its variants, such as algebraic CELP, relaxed CELP, low-delay CELP and vector sum excited linear prediction, it is currently the most widely used speech coding algorithm[citation needed]. It is also used in MPEG-4 Audio speech coding. CELP is commonly used as a generic term for a class of algorithms and not for a particular codec.",https://en.wikipedia.org/wiki/Code-excited_linear_prediction
Simon's problem,"In the computational complexity theory and quantum computing, Simon's problem is a computational problem that can be solved exponentially faster on a quantum computer than on a classical (or traditional) computer.  Although the problem itself is of little practical value, it can be proved that a quantum algorithm can solve this problem exponentially faster than any known classical algorithm.",https://en.wikipedia.org/wiki/Simon%27s_algorithm
Combinatorial optimization,"In operations research, applied mathematics and theoretical computer science, combinatorial optimization  is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not tractable. It operates on the domain of those optimization problems in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution.  Some common problems involving combinatorial optimization are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem.",https://en.wikipedia.org/wiki/Combinatorial_optimization
Random forest,"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.:587–588",https://en.wikipedia.org/wiki/Random_Forest
Demon algorithm,"The demon algorithm is a Monte Carlo method for efficiently sampling members of a microcanonical ensemble with a given energy. An additional degree of freedom, called 'the demon', is added to the system and is able to store and provide energy. If a drawn microscopic state has lower energy than the original state, the excess energy is transferred to the demon. For a sampled state that has higher energy than desired, the demon provides the missing energy if it is available. The demon can not have negative energy and it does not interact with the particles beyond exchanging energy. Note that the additional degree of freedom of the demon does not alter a system with many particles significantly on a macroscopic level.",https://en.wikipedia.org/wiki/Demon_algorithm
Matrix multiplication,"In mathematics, matrix multiplication is a binary operation that produces a matrix from two matrices. For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The result matrix, known as the matrix product, has the number of rows of the first and the number of columns of the second matrix.",https://en.wikipedia.org/wiki/Matrix_multiplication
Kruskal's algorithm," Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized.  If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component).",https://en.wikipedia.org/wiki/Kruskal%27s_algorithm
Sum of absolute differences,"In digital image processing, the sum of absolute differences (SAD) is a measure of the similarity between image blocks.  It is calculated by taking the absolute difference between each pixel in the original block and the corresponding pixel in the block being used for comparison.  These differences are summed to create a simple metric of block similarity, the L1 norm of the difference image or Manhattan distance between two image blocks.",https://en.wikipedia.org/wiki/Sum_of_absolute_differences
Espresso heuristic logic minimizer,"The Espresso logic minimizer is a computer program using heuristic and specific algorithms for efficiently reducing the complexity of digital logic gate circuits. Espresso was developed at IBM by Robert K. Brayton. Richard Rudell later published the variant Espresso-MV in 1986 under the title ""Multiple-Valued Logic Minimization for PLA Synthesis"". Espresso has inspired many derivatives.",https://en.wikipedia.org/wiki/Espresso_heuristic_logic_minimizer
Blind deconvolution,"In electrical engineering and applied mathematics, blind deconvolution is deconvolution without explicit knowledge of the impulse response function used in the convolution. This is usually achieved by making appropriate assumptions of the input to estimate the impulse response by analyzing the output. Blind deconvolution is not solvable without making assumptions on input and impulse response. Most of the algorithms to solve this problem  are based on assumption that both input and impulse response live in respective  known subspaces. However,  blind deconvolution remains a very challenging non-convex optimization problem even with this assumption.",https://en.wikipedia.org/wiki/Blind_deconvolution
K-SVD,"In applied mathematics, K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. K-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. K-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.",https://en.wikipedia.org/wiki/K-SVD
Book sources,This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter.,https://en.wikipedia.org/wiki/Special:BookSources/978-1-118-63817-0
Levinson recursion,"Levinson recursion or Levinson–Durbin recursion is a procedure in linear algebra to recursively calculate the solution to an equation involving a Toeplitz matrix. The algorithm runs in Θ(n2) time, which is a strong improvement over Gauss–Jordan elimination, which runs in Θ(n3).",https://en.wikipedia.org/wiki/Levinson_recursion
Balanced clustering,"Balanced clustering is a special case of clustering where, in the strictest sense, cluster sizes are constrained to ⌊nk⌋, where n is the number of clusters. A typical algorithm is balanced k-means, which minimizes mean square error (MSE). Another type of balanced clustering called balance-driven clustering has a two-objective cost function that minimizes both the imbalance and the MSE. Typical cost functions are ratio cut and Ncut. Balanced clustering can be used for example in scenarios where freight has to be delivered to n cars. It is then preferred that each car delivers to an equal number of locations.",https://en.wikipedia.org/wiki/Balanced_clustering
Marching tetrahedra,Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations. It was originally introduced in 1991.,https://en.wikipedia.org/wiki/Marching_tetrahedrons
Clipping (computer graphics),"Clipping, in the context of computer graphics, is a method to selectively enable or disable rendering operations within a defined region of interest.  Mathematically, clipping can be described using the terminology of constructive geometry.  A rendering algorithm only draws pixels in the intersection between the clip region and the scene model.  Lines and surfaces outside the view volume (aka. frustum) are removed.",https://en.wikipedia.org/wiki/Clipping_(computer_graphics)
Monotone cubic interpolation,"In the mathematical field of numerical analysis, monotone cubic interpolation is a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.",https://en.wikipedia.org/wiki/Monotone_cubic_interpolation
PVLV,The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.,https://en.wikipedia.org/wiki/PVLV
Richard Zemel,"Richard Stanley Zemel (born 1963) is a computer scientist and professor at University of Toronto, Department of Computer Science, and a leading figure in the field of Machine Learning and Computer Vision.",https://en.wikipedia.org/wiki/Richard_Zemel
Phonetic algorithm,"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language[citation needed]; consequently, applying the rules to words in other languages might not give a meaningful result.",https://en.wikipedia.org/wiki/Phonetic_algorithm
Metric space,"In mathematics, a metric space is a set together with a metric on the set.  The metric is a function that defines a concept of distance between any two members of the set, which are usually called points.  The metric satisfies a few simple properties.",https://en.wikipedia.org/wiki/Metric_space
State–action–reward–state–action,"State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name ""Modified Connectionist Q-Learning"" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.",https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action
Island algorithm,"The island algorithm is an algorithm for performing inference on hidden Markov models, or their generalization, dynamic Bayesian networks.It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. ",https://en.wikipedia.org/wiki/Island_algorithm
Structured sparsity regularization,"Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable Y (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space X (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in X.",https://en.wikipedia.org/wiki/Structured_sparsity_regularization
Whitening transformation,"A whitening transformation or sphering transformation is a linear transformation that transforms a vector of random variables with a known covariance matrix into a set of new variables whose covariance is the identity matrix, meaning that they are uncorrelated and each have variance 1. The transformation is called ""whitening"" because it changes the input vector into a white noise vector.",https://en.wikipedia.org/wiki/Whitening_transformation
Weiler–Atherton clipping algorithm,The Weiler–Atherton is a polygon-clipping algorithm. It is used in areas like computer graphics and games development where clipping of polygons is needed. It allows clipping of a subject or candidate polygon by an arbitrarily shaped clipping polygon/area/region. ,https://en.wikipedia.org/wiki/Weiler%E2%80%93Atherton
Error diffusion,"Error diffusion is a type of halftoning in which the quantization residual is distributed to neighboring pixels that have not yet been processed. Its main use is to convert a multi-level image into a binary image, though it has other applications.",https://en.wikipedia.org/wiki/Error_diffusion
Secret sharing,"Secret sharing (also called secret splitting) refers to methods for distributing a secret amongst a group of participants, each of whom is allocated a share of the secret. The secret can be reconstructed only when a sufficient number, of possibly different types, of shares are combined together; individual shares are of no use on their own.",https://en.wikipedia.org/wiki/Secret_sharing
GeneRec,"GeneRec is a generalization of the recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. It is used as part of the Leabra algorithm for error-driven learning.",https://en.wikipedia.org/wiki/GeneRec
Mark V. Shaney,"Mark V. Shaney is a synthetic Usenet user whose postings in the net.singles newsgroups were generated by Markov chain techniques, based on text from other postings.  The username is a play on the words ""Markov chain"".  Many readers were fooled into thinking that the quirky, sometimes uncannily topical posts were written by a real person.",https://en.wikipedia.org/wiki/Mark_V._Shaney
Rademacher complexity,"In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution.",https://en.wikipedia.org/wiki/Rademacher_complexity
Anti-unification (computer science),"Anti-unification is the process of constructing a generalization common to two given symbolic expressions. As in unification, several frameworks are distinguished depending on which expressions (also called terms) are allowed, and which expressions are considered equal. If variables representing functions are allowed in an expression, the process is called ""higher-order anti-unification"", otherwise ""first-order anti-unification"". If the generalization is required to have an instance literally equal to each input expression, the process is called ""syntactical anti-unification"", otherwise ""E-anti-unification"", or ""anti-unification modulo theory"".",https://en.wikipedia.org/wiki/Anti-unification_(computer_science)
International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics,,https://en.wikipedia.org/wiki/International_Meeting_on_Computational_Intelligence_Methods_for_Bioinformatics_and_Biostatistics
Quality control and genetic algorithms,"The combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems. Quality control is a process by which entities review the quality of all factors involved in production. Quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated, general implied or obligatory. Genetic algorithms are search algorithms, based on the mechanics of natural selection and natural genetics.",https://en.wikipedia.org/wiki/Quality_control_and_genetic_algorithms
Context-free grammar,"In formal language theory, a context-free grammar (CFG) is a certain type of formal grammar:  a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule",https://en.wikipedia.org/wiki/Context-free_grammar
Bowyer–Watson algorithm,"In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. The algorithm can be also used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation.",https://en.wikipedia.org/wiki/Bowyer%E2%80%93Watson_algorithm
Data Encryption Standard,"The Data Encryption Standard (DES /ˌdiːˌiːˈɛs, dɛz/) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits—criticized from the beginning—makes it too insecure for most current applications, it has been highly influential in the advancement of modern cryptography.",https://en.wikipedia.org/wiki/Data_Encryption_Standard
Numenta,"Numenta is a machine intelligence company that has developed a cohesive theory, core software, technology and applications based on the principles of the neocortex. The company was founded on February 4, 2005 by Palm founder Jeff Hawkins with his longtime business partner Donna Dubinsky and Stanford graduate student Dileep George. Numenta is headquartered in Redwood City, California and is privately funded.",https://en.wikipedia.org/wiki/Numenta
Linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.",https://en.wikipedia.org/wiki/Fisher%27s_linear_discriminant
Proactive learning,Proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.,https://en.wikipedia.org/wiki/Proactive_learning
Nicholl–Lee–Nicholl algorithm,"The Nicholl–Lee–Nicholl algorithm is a fast line clipping algorithm that reduces the chances of clipping a single line segment multiple times, as may happen in the Cohen–Sutherland algorithm.",https://en.wikipedia.org/wiki/Nicholl%E2%80%93Lee%E2%80%93Nicholl
Graphics processing unit,"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In certain CPUs, they are embedded on the CPU die.",https://en.wikipedia.org/wiki/Graphics_processing_unit
Heap's algorithm,"Heap's algorithm generates all possible permutations of n  objects. It was first proposed by B. R. Heap in 1963. The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other n−2 elements are not disturbed. In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer.",https://en.wikipedia.org/wiki/Heap%27s_algorithm
Linear multistep method,"Linear multistep methods are used for the numerical solution of ordinary differential equations. Conceptually, a numerical method starts from an initial point and then takes a short step forward in time to find the next solution point. The process continues with subsequent steps to map out the solution. Single-step methods (such as Euler's method) refer to only one previous point and its derivative to determine the current value. Methods such as Runge–Kutta take some intermediate steps (for example, a half-step) to obtain a higher order method, but then discard all previous information before taking a second step. Multistep methods attempt to gain efficiency by keeping and using the information from previous steps rather than discarding it. Consequently, multistep methods refer to several previous points and derivative values. In the case of linear multistep methods, a linear combination of the previous points and derivative values is used.",https://en.wikipedia.org/wiki/Linear_multistep_method
False nearest neighbor algorithm,"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension.  The concept was proposed by Kennel et al.. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension.  In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real.  With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.",https://en.wikipedia.org/wiki/False_nearest_neighbor_algorithm
Solomonoff's theory of inductive inference,"Solomonoff's theory of inductive inference is Ray Solomonoff's mathematical formalization of Occam's razor.  It explains observations of the world by the smallest computer program that outputs those observations.  Solomonoff proved that this explanation is the most likely one, by assuming the world is generated by an unknown computer program.  That is to say the probability distribution of all computer programs that output the observations favors the shortest one.",https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference
Cellular evolutionary algorithm,"A cellular evolutionary algorithm (cEA) is a kind of evolutionary algorithm (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement).",https://en.wikipedia.org/wiki/Cellular_evolutionary_algorithm
Tournament selection,"Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm. Tournament selection involves running several ""tournaments"" among a few individuals (or ""chromosomes"") chosen at random from the population.  The winner of each tournament (the one with the best fitness) is selected for crossover.  Selection pressure, a probabilistic measure of a chromosome's likelihood of participation in the tournament based on the participant selection pool size, is easily adjusted by changing the tournament size[why?]. If the tournament size is larger, weak individuals have a smaller chance to be selected, because, if a weak individual is selected to be in a tournament, there is a higher probability that a stronger individual is also in that tournament.",https://en.wikipedia.org/wiki/Tournament_selection
Cutting-plane method,"In mathematical optimization, the cutting-plane method is any of a variety of optimization methods that iteratively refine a feasible set or objective function by means of linear inequalities, termed cuts.  Such procedures are commonly used to find integer solutions to mixed integer linear programming (MILP) problems, as well as to solve general, not necessarily differentiable convex optimization problems.  The use of cutting planes to solve MILP was introduced by Ralph E. Gomory.",https://en.wikipedia.org/wiki/Cutting-plane_method
Interacting particle system,"In probability theory, an interacting particle system (IPS) is a stochastic process (X(t))t∈R+ given by a site space, a countable-infinite graph G and a local state space, a compact metric space S. More precisely IPS are continuous-time Markov jump processes describing the collective behavior of stochastically interacting components. IPS are the continuous-time analogue of stochastic cellular automata.",https://en.wikipedia.org/wiki/Interacting_particle_system
Neural Designer,"Neural Designer is a software tool for data analytics based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.",https://en.wikipedia.org/wiki/Neural_Designer
John Platt (computer scientist),"John Carlton Platt (born 1963) is an American computer scientist. He is currently a Principal Scientist at Google. Formerly he was a Deputy Managing Director at Microsoft Research Redmond Labs. Platt worked for Microsoft from 1997 to 2015. Before that, he served as Director of Research at Synaptics.",https://en.wikipedia.org/wiki/John_Platt_(computer_scientist)
Multigrid method,"Multigrid (MG) methods in numerical analysis are algorithms for solving differential equations using a hierarchy of discretizations. They are an example of a class of techniques called multiresolution methods, very useful in problems exhibiting multiple scales of behavior. For example, many basic relaxation methods exhibit different rates of convergence for short- and long-wavelength components, suggesting these different scales be treated differently, as in a Fourier analysis approach to multigrid. MG methods can be used as solvers as well as preconditioners.",https://en.wikipedia.org/wiki/Multigrid_method
Hammersley–Clifford theorem,"The Hammersley–Clifford theorem is a result in probability theory, mathematical statistics and statistical mechanics, that gives necessary and sufficient conditions under which a strictly positive probability distribution[clarification needed] can be represented as a Markov network (also known as a Markov random field). It is the fundamental theorem of random fields. It states that a probability distribution that has a strictly positive mass or density satisfies one of the Markov properties with respect to an undirected graph G if and only if it is a Gibbs random field, that is, its density can be factorized over the cliques (or complete subgraphs) of the graph.",https://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem
Kabsch algorithm,"The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. It is useful in graphics, cheminformatics to compare molecular structures, and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).",https://en.wikipedia.org/wiki/Kabsch_algorithm
Hinge loss,"In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for ""maximum-margin"" classification, most notably for support vector machines (SVMs).",https://en.wikipedia.org/wiki/Hinge_loss
Gerchberg–Saxton algorithm,"The Gerchberg–Saxton (GS) algorithm is an iterative algorithm for retrieving the phase of a pair of light distributions (or any other mathematically valid distribution) related via a propagating function, such as the Fourier transform, if their intensities at their respective optical planes are known.",https://en.wikipedia.org/wiki/Gerchberg%E2%80%93Saxton_algorithm
Naive Bayes classifier,"In machine learning, naïve Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.",https://en.wikipedia.org/wiki/Multinomial_Naive_Bayes
Minimum spanning tree,,https://en.wikipedia.org/wiki/Minimum_spanning_tree
CN2 algorithm,The CN2 induction algorithm is a learning algorithm for rule induction. It is designed to work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3.,https://en.wikipedia.org/wiki/CN2_algorithm
Stable marriage problem,"In mathematics, economics, and computer science, the stable marriage problem (also stable matching problem or SMP) is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element.  A matching is a bijection from the elements of one set to the elements of the other set.",https://en.wikipedia.org/wiki/Stable_marriage_problem
Graph coloring,"In graph theory, graph coloring is a special case of graph labeling; it is an assignment of labels traditionally called ""colors"" to elements of a graph subject to certain constraints. In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices are of the same color; this is called a vertex coloring. Similarly, an edge coloring assigns a color to each edge so that no two adjacent edges are of the same color, and a face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color.",https://en.wikipedia.org/wiki/Coloring_algorithm
First-difference estimator,The first-difference (FD) estimator is an approach used to address the problem of omitted variables in econometrics and statistics with panel data. The estimator is obtained by running a pooled OLS estimation for a regression of Δyit.[clarification needed],https://en.wikipedia.org/wiki/First-difference_estimator
Fisher kernel,"In statistical classification, the Fisher kernel, named after Ronald Fisher, is a function that measures the similarity of two objects on the basis of sets of measurements for each object and a statistical model. In a classification procedure, the class for a new object (whose real class is unknown) can be estimated by minimising, across classes, an average of the Fisher kernel distance from the new object to each known member of the given class.",https://en.wikipedia.org/wiki/Fisher_kernel
OPTICS algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.",https://en.wikipedia.org/wiki/OPTICS_algorithm
Ray Kurzweil,"Raymond Kurzweil (/ˈkɜːrzwaɪl/ KURZ-wyle; born February 12, 1948) is an American inventor and futurist. He is involved in fields such as optical character recognition (OCR), text-to-speech synthesis, speech recognition technology, and electronic keyboard instruments. He has written books on health, artificial intelligence (AI), transhumanism, the technological singularity, and futurism. Kurzweil is a public advocate for the futurist and transhumanist movements, and gives public talks to share his optimistic outlook on life extension technologies and the future of nanotechnology, robotics, and biotechnology.",https://en.wikipedia.org/wiki/Ray_Kurzweil
Factor analysis of mixed data,"In statistics, factor analysis of mixed data (FAMD), or factorial analysis of mixed data, is the factorial method devoted to data tables in which a group of individuals is described both by quantitative and qualitative variables. It belongs to the exploratory methods developed by the French school called Analyse des données founded by Jean-Paul Benzécri.",https://en.wikipedia.org/wiki/Factor_analysis_of_mixed_data
3Dc,"3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI. It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia.",https://en.wikipedia.org/wiki/3Dc
Faugère's F4 and F5 algorithms,"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring.  The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.",https://en.wikipedia.org/wiki/Faug%C3%A8re_F4_algorithm
Glossary of artificial intelligence,,https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence
Johnson's algorithm,"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.",https://en.wikipedia.org/wiki/Johnson%27s_algorithm
Intelligent agent,"In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).[citation needed] Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.",https://en.wikipedia.org/wiki/Intelligent_agent
Ordination (statistics),"Ordination or gradient analysis, in multivariate analysis, is a method complementary to data clustering, and used mainly in exploratory data analysis (rather than in hypothesis testing). Ordination orders objects that are characterized by values on multiple variables (multivariate objects) so that similar objects are near each other and dissimilar objects are farther from each other. Such relationships between the objects, on each of several axes (one for each variable), are then characterized numerically and/or graphically. Many ordination techniques exist, including principal components analysis (PCA), non-metric multidimensional scaling (NMDS), correspondence analysis (CA) and its derivatives (detrended CA (DCA), canonical CA (CCA)), Bray–Curtis ordination, and redundancy analysis (RDA), among others.",https://en.wikipedia.org/wiki/Ordination_(statistics)
Co-training,Co-training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. It was introduced by Avrim Blum and Tom Mitchell in 1998.,https://en.wikipedia.org/wiki/Co-training
Line clipping,"In computer graphics, line clipping is the process of removing lines or portions of lines outside an area of interest. Typically, any line or part thereof which is outside of the viewing area is removed.",https://en.wikipedia.org/wiki/Line_clipping
Similarity learning,"Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems,  visual identity tracking, face verification, and speaker verification.",https://en.wikipedia.org/wiki/Similarity_learning
